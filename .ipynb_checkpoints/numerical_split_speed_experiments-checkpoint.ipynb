{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Numerical Splitter Bake-off: Louppe (Sklearn) vs. Wright (Ranger)\n",
    "*Â© Copyright James Dellinger, 2022-2025. All rights reserved.*\n",
    "\n",
    "The two most widely-used modern Random Forests implementations are those from [Sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier) and [Ranger](https://github.com/imbs-hl/ranger). [Gilles Louppe](https://glouppe.github.io/) designed and wrote the core parts of Sklearn's module, which has emerged as the go-to choice for data scientists hailing from the Python ecosystem. Ranger's creator [Marvin Wright](https://mnwright.github.io/), on the other hand, has geared his Random Forests implementation toward practitioners more comfortable living in the R language ecosystem.\n",
    "\n",
    "The two libraries' differences run deeper than the choice of API language, and it's arguable that their respective under-the-hood approaches to implementing decision trees are far more disparate, and more interesting, than any of their surface-level dissimilarities. In particular, when it comes to implementing numerical split finding, the central step of the decision tree fitting process, these libraries' approaches could not be more different.\n",
    "\n",
    "In this notebook I prototype in pure Python and then write Cython re-implementations of both Sklearn's and Ranger's numerical split finding algorithms, highlighting along the way the tradeoffs inherent in each library's design decisions. I also integrate each split-finder into its own full-fledged decision tree classifier and use it to fit a tree model to Kaggle's [Titanic dataset](https://www.kaggle.com/c/titanic). \n",
    "\n",
    "Because dataset size might influence the relative speeds of the two candidate numerical split finders, I run a further sanity check of fitting each of my Cython decision tree implementations to the Kaggle Santander Customer Satisfaction competition [dataset](https://www.kaggle.com/competitions/santander-customer-satisfaction/data), which is substantially larger (about 60K rows) than the Titanic dataset (about 800 rows). \n",
    "\n",
    "### Spoilers: Louppe is best for most implementations and practitioners\n",
    "For those who prefer to read the ending first, here are the times that it took for each of my Cython implementations to fit a decision tree classifier model to the Santander data:\n",
    "\n",
    "|Splitting Algorithm|Pre-sorting Required|Speed on Santander Dataset|\n",
    "|---|---|---|\n",
    "|Louppe|No|137 ms|\n",
    "|Wright SmallQ|No|874 ms|\n",
    "|Wright LargeQ|Yes|138 ms|\n",
    "|Wright SmallQ/LargeQ|Yes|139 ms|\n",
    "|Louppe SmallQ/Wright LargeQ|Yes|129 ms| \n",
    "\n",
    "The best results were attained by a hybrid I made that used Louppe's splitter to split nodes where the ratio of `num samples in node`/`num unique training set values for splitting feature` was less than 0.02, and used Wright's LargeQ splitter otherwise.\n",
    "\n",
    "It's crucial to keep in mind that in order to use Wright's LargeQ splitter during training, the practitioner must pre-sort the unique values of each numerical feature in the training set, and then create a lookup table of shape `(# training rows, # numerical features)` where rows' raw numerical feature values are replaced with the indices of where the original values can be found in the pre-sorted unique values table.\n",
    "\n",
    "Creating these two tables takes time and extra memory, and my judgement is that the extra effort is not worth the modest ~5% speed-up gained over using Louppe splitting alone. \n",
    "\n",
    "An interesting question that's beyond the scope of these experiments is whether Wright's LargeQ sorting could ever be worth the pre-sorting costs of time and memory. Perhaps in situations where tree nodes contain millions or even billions of rows, while numerical features possess a small number of unique values (say, four to ten), Louppe's strategy of relocating each node's rows' values into a contiguous memory buffer to facilitate just-in-time in-place sorting might become prohibitively expensive. \n",
    "\n",
    "### In a nutshell, Louppe's Numerical Split Finder: Sort at every split\n",
    "[Gilles Louppe](https://glouppe.github.io/) outlines the design of Sklearn's numerical split finder on pages 107 to 110 of his doctoral [dissertation](https://arxiv.org/abs/1407.7502):\n",
    "1. For each training sample (row) in the current node, copy the value held by that row for a given candidate numerical feature into a static, contiguous buffer (in other words, into a new 1-d array).\n",
    "2. Use introsort to sort these feature values in ascending order (duplicates are kept) and simultaneously do an in-place argsort of the row indices of the node's samples so that each row index is in the same position (relative to the node's start index) as its corresponding feature value (relative to the beginning of the temporary 1-d array of feature values).\n",
    "3. Iterate through each unique feature value, from smallest to largest, calculating the split criterion score (usually Gini) for the left and right child nodes for each split-point.\n",
    "\n",
    "### Breiman and Cutler's Alternate Approach: Pre-sort\n",
    "In his PhD thesis, Louppe acknowledges that an alternate split finding strategy is used by Breiman and Cutler in their [Fortran implementation](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm) of Random Forests. Before training begins, they create lists of argsorted row indices for each numerical feature.\n",
    "\n",
    "As Louppe explains on page 110, having numerical features pre-sorted reduces the complexity of best split searches from $O(KNlogN)$ to $O(KN)$ (where $N$ is the number of samples in the parent node and $K$ is the number of candidate features randomly selected for that node). \n",
    "\n",
    "It turns out, though, that there is no free lunch here as pre-sorting introduces new complications that can raise complexity in other areas. Specifically, since pre-sorting requires us to store a sorted list of row indices for *each* numerical feature, everytime we split a parent node we'd have to re-organize each numerical feature's argsorted row indices such that the rows belonging to the left child are contiguous and those residing in the right child are next to each other.\n",
    "\n",
    "This is no doubt a hassle, but the beauty of this process is that as the tree is grown, each numerical feature's argsorted rows will remain properly sorted in all child nodes that have yet to be split. The prospect of not having to run a sorting function on the samples from each candidate numerical feature, each time a row is split, for each tree in a random forest has an undeniable appeal.\n",
    "\n",
    "### Is Breiman and Cutler's numerical split finding method provably faster?\n",
    "Louppe goes on to demonstrate that the complexity of ensuring that each feature's row indices remain organized in a manner that's consistent with the decision tree's structure is $O(\\rho N)$ (where $\\rho$ is the number of numerical features in the training set). The ratio of the complexities of pre-sorting once to doing per-split sorts is $O(\\frac{\\rho}{KlogN})$, with the takeaway being that depending on the number of features in the training set ($\\rho$) as well as the number of features randomly selected for each node's split search ($K$), it is by no means an ironclad certainty that pre-sorting will result in faster performance. \n",
    "\n",
    "For this reason, Louppe didn't elect to implement pre-sorting for sklearn's random forests, and one could hardly fault him for not following in Breiman and Cutler's footsteps in this regard. Nevertheless, Louppe's complexity derivation makes it clear that neither approach has an unassailable advantage over the other.\n",
    "\n",
    "### What about parallel training?\n",
    "It seems to me, however, that even if there situations where Breiman & Cutler's method is faster than Louppe's, their approach wouldn't be compatible with a Random Forest fitting algorithm that can fit several decision trees in parallel. Each tree would have to have its own copy of the dataset's sorted raw feature values so that these values could be partitioned according to the node structure of each decision tree in the random forest. This would be profoundly expensive in terms of memory use and downright unfeasible when training on a massive dataset.\n",
    "\n",
    "### In a nutshell, Ranger's Numerical Split Finder: A hybrid approach\n",
    "One of the things that attracted me to Ranger was the fact that tries to take advantage of the benefits of both Louppe's sort-every-split strategy and Breiman and Cutler's pre-sort method. Furthermore, unlike Breiman & Cutler's presorting, Ranger's pre-sorting strategy is quite compatible with parallel training.\n",
    "\n",
    "On page three of his [2015 paper](https://arxiv.org/abs/1508.04409) that provides a high level overview of the philosophy behind Ranger's implementation, [Marvin Wright](https://mnwright.github.io/) explains that users can choose to train a model using either \"runtime-optimized\" mode or \"memory efficient\" mode.\n",
    "1. On the surface, \"memory efficient\" mode looks a lot like Sklearn. No presorting takes place and raw feature values are sorted for each candidate numerical feature for each node that gets split. Under the hood, Ranger has a different way of bypassing identical feature values than Sklearn. As we'll see later on below, Ranger's code to do this looks a little simpler, but Sklearn runs a bit faster.\n",
    "2. \"Runtime-optimized\" mode is where things get really cool. Here, pre-sorting does take place before training (argsorts for each numerical feature), but the pre-argsorted row indices are only used for splitting large nodes. Wright [defines](https://github.com/imbs-hl/ranger/blob/ce497711884c783e133fb36750b60de4c140773f/src/TreeClassification.cpp#L172) a node as \"large\" if the ratio, `Q`, of the number of its samples to the number of unique values held by *all training samples* for the curent numerical feature is [greater than or equal to 0.02](https://github.com/imbs-hl/ranger/blob/ce497711884c783e133fb36750b60de4c140773f/src/globals.h#L106). \n",
    "\n",
    "   What's interesting about this definition is that it doesn't just look at the raw number of samples sitting inside a given node, but that it then adjusts this quantity by the total number of unique values in the entire training set that are held by the numerical feature that is currently being investigated (to see if it offers the best split of the node's samples). If a node had only 5 samples, but across the entire training set there were only 4 unique values for the current candidate feature, then the node would be considered to be \"large Q.\" On the other hand, if the node had 1,000 samples, but the current given feature had two million unique training values, the node would be considered to be \"small Q.\"\n",
    "\n",
    "  Under this standard, if a node is deemed to be \"small Q,\" it will get split with the same method as \"memory efficient\" mode, where the raw feature values are drawn and sorted real-time.\n",
    "3. Finally, note that once it's arranged, the table of sorted row indices is never subsequently adjusted and could thus be used when fitting a random forest's trees in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Louppe's Numerical Split Finder\n",
    "Without futher ado, let's code up a Pythonic representation of Louppe's numerical splitting algorithm and use it to fit a decision tree to the Titanic dataset. A few implementation notes:\n",
    "* The Sklearn library's decision tree architecture is *very* object-oriented. Far beyond having a simple `DecisionTree` class, Sklearn instantiates a splitter object to conduct split churches, and even uses a criterion object calculate the gini score of each split point. \n",
    "* My personal opinion: nailing down the purpose and function of each of these objects (by tabbing from `.pyx` file to `.pyx` file) made it hard for me to quickly attain high-level understanding of how Sklearn does what it does.\n",
    "* Thus, I prefer a more functional approach for my own implementation. \n",
    "* What's more, I prefer to use an on-line algorithm to update my gini score calculation after each subsequent split point is examined (this is what Breiman and Cutler do in their Fortran code). Sklearn happens to eschew this practice -- it's `Criterion` class [calculates the node purity score from scratch](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_criterion.pyx#L656) at each split point.\n",
    "* I also employ the following [trick](https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/tree/_criterion.pyx#L430) that I learned from Sklearn:\n",
    "\n",
    "    * If several samples in the right child are identical, it may be faster to update left and right child stats by temporarily presuming that left child contains all the parent node's samples and then move the last few of these over to the right child. \n",
    "\n",
    "      I'll do this when the number of samples that'd go from left to right is less than the number of identically-valued samples that'd otherwise have to go from right to left. \n",
    "\n",
    "    ```python\n",
    "    if pos-prev_pos > n_parent-pos-1:\n",
    "        l_num, l_den = parent_num, parent_den\n",
    "        r_num, r_den = 0., 0.\n",
    "        l_wcc, r_wcc = parent_wcc.copy(), [0.]*n_class\n",
    "        for row in reversed(rows[pos+node_start: n_parent+node_start]):\n",
    "            label = labels[row]; w = c_wts[label]\n",
    "            r_num += w*( 2*r_wcc[label] + w); r_den += w\n",
    "            l_num += w*(-2*l_wcc[label] + w); l_den -= w \n",
    "            r_wcc[label] += w; l_wcc[label] -= w\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cython dual sort/argsort function\n",
    "Even with my pure Python prototype of Louppe's decision tree algorithm that follows below, I'd still prefer to do my sorting as rapidly as possible.\n",
    "\n",
    "To that end, the following functions comprise my own home-grown Cython introsort implementation that simultaneously sorts a 1-d array containing floats, and at the same time correspondingly argsorts the portion of the 1-d array containing the row indices of all training samples that contains the row indices of the samples in the current node. \n",
    "\n",
    "In designing this implementation, I borrowed ideas that I liked from the introsort implementations found in [Sklearn](https://github.com/scikit-learn/scikit-learn/blob/4c6fc05b2a1f11bedef5784c46b9f5d3e52489c2/sklearn/tree/_splitter.pyx#L440), [Numpy](https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211), and [libstdc++](https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t\n",
    "\n",
    "from libc.math cimport log as ln\n",
    "cimport cython\n",
    "\n",
    "cdef inline void dual_swap(DTYPE_t* items, SIZE_t* rows, SIZE_t i, SIZE_t j) nogil:\n",
    "    items[i], items[j] = items[j], items[i]\n",
    "    rows[i], rows[j] = rows[j], rows[i]\n",
    "\n",
    "# Quicksort helpers\n",
    "\n",
    "cdef inline void dual_med_three(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find the median-of-three pivot point of the second through final \n",
    "    items of a list of numbers. Once identified, the pivot is moved to \n",
    "    the front of the list. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t middle = <int>(first + (last - first)/2)\n",
    "    cdef SIZE_t second = first + 1\n",
    "    last -= 1\n",
    "    if items[second] < items[middle]:\n",
    "        if items[middle] < items[last]:\n",
    "            dual_swap(items, rows, first, middle)    \n",
    "        elif items[second] < items[last]:\n",
    "            dual_swap(items, rows, first, last)         \n",
    "        else:                        \n",
    "            dual_swap(items, rows, first, second)\n",
    "    elif items[second] < items[last]:\n",
    "        dual_swap(items, rows, first, second)\n",
    "    elif items[middle] < items[last]:\n",
    "        dual_swap(items, rows, first, last)\n",
    "    else:\n",
    "        dual_swap(items, rows, first, middle)\n",
    "\n",
    "cdef inline SIZE_t dual_partition(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, SIZE_t pivot) nogil:\n",
    "    \"\"\"Group numbers less than the pivot value together on the left and\n",
    "    those that are greater on the right. Find the index that separates\n",
    "    these two groups, which will belong to the first item that is greater\n",
    "    than or equal to the pivot. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1885\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        pivot      : Index holding the median pivot value.\n",
    "        \n",
    "    Returns:\n",
    "        Index of cut point used to partition the items into two smaller sequences.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while first < last and items[first] < items[pivot]:\n",
    "            first += 1                      # Get index of first item greater than or equal to median-of-three pivot. \n",
    "        last -= 1\n",
    "        while items[pivot] < items[last]:\n",
    "            last -= 1                       # Get index of last item less than or equal to the pivot.\n",
    "        if not (first < last): \n",
    "            return first                    # After swaps are done, return index of first item in right partition.\n",
    "        \n",
    "        dual_swap(items, rows, first, last) # Swap the first item greater than or equal to the pivot with the\n",
    "                                            # last item less than or equal to the pivot. \n",
    "        first += 1\n",
    "\n",
    "cdef inline void dual_insertion_sort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Follows the spirit of the Numpy implementation at: \n",
    "        https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t i\n",
    "    cdef SIZE_t j\n",
    "    cdef SIZE_t k\n",
    "    cdef DTYPE_t val\n",
    "    for i in range(first+1, last):\n",
    "        j = i\n",
    "        k = i - 1\n",
    "        val = items[i]\n",
    "        row = rows[i]\n",
    "        while (j > first) and val < items[k]:\n",
    "            items[j] = items[k]\n",
    "            rows[j] = rows[k]\n",
    "            j-=1\n",
    "            k-=1\n",
    "        items[j] = val\n",
    "        rows[j] = row\n",
    "\n",
    "# Heapsort\n",
    "\n",
    "cdef inline void dual_sift_down(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n, \n",
    "                                SIZE_t p, SIZE_t c, DTYPE_t val, SIZE_t row) nogil:\n",
    "    \"\"\"Swap a heap item with one of its children if that child's value is \n",
    "    greater than or equal to that parent's value. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L61\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "        p    : Index of the parent.\n",
    "        c    : Index of the parent's first (left) child.\n",
    "        val  : The parent's value.\n",
    "        row  : The parent's training row index.\n",
    "    \"\"\"\n",
    "    while c < n:    # Look at the descendents of current parent, `p`.\n",
    "        if c < n-1 and items[start + c] < items[start + c + 1]: # Find larger of the first and second children.\n",
    "            c += 1\n",
    "        if val < items[start + c]: # If child greater than parent, swap child and parent.\n",
    "            items[start + p] = items[start + c]\n",
    "            rows[start + p] = rows[start + c]\n",
    "            p = c   # Current greater child becomes the parent.\n",
    "            c += c  # Look at this child's child, if it exists.\n",
    "        else:\n",
    "            break \n",
    "    items[start + p] = val\n",
    "    rows[start + p] = row\n",
    "\n",
    "cdef inline void dual_sort_heap(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Sort a binary max heap of numbers. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L77\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t row\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        val = items[start + n]\n",
    "        row = rows[start + n]\n",
    "        items[start + n] = items[start]\n",
    "        rows[start + n] = rows[start]\n",
    "        dual_sift_down(items, rows, start, n, 0, 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapify(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Turn a list of items into a binary max heap. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L59\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t p\n",
    "    cdef SIZE_t last_p = (n-2)//2\n",
    "    for p in range(last_p, -1, -1):\n",
    "        val = items[start + p] # value of last parent\n",
    "        row = rows[start + p]\n",
    "        dual_sift_down(items, rows, start, n, p, 2*p + 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapsort(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Applies the heapsort algorithm to sort a list of items from least to greatest. \n",
    "    From Williams, 1964.\n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    dual_heapify(items, rows, start, n)\n",
    "    dual_sort_heap(items, rows, start, n)\n",
    "    \n",
    "# Introsort \n",
    "\n",
    "cdef void dual_introsort_loop(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, int depth) nogil:\n",
    "    \"\"\"The recursive heart of the introsort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        depth      : Current recursion depth.\n",
    "    \"\"\"\n",
    "    cdef int MIN_SIZE_THRESH = 16\n",
    "    cdef SIZE_t cut\n",
    "    while last-first > MIN_SIZE_THRESH:\n",
    "        if depth == 0:\n",
    "            dual_heapsort(items, rows, first, last-first)\n",
    "        depth -= 1\n",
    "        dual_med_three(items, rows, first, last)\n",
    "        cut = dual_partition(items, rows, first+1, last, first)\n",
    "        dual_introsort_loop(items, rows, cut, last, depth)\n",
    "        last = cut\n",
    "\n",
    "# Log base-2 helper function. From Sklearn's implementation at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_utils.pyx#L7\n",
    "cdef inline DTYPE_t log2(DTYPE_t x) nogil:\n",
    "    return ln(x) / ln(2.0)\n",
    "\n",
    "cdef void dual_introsort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Implementation as described in Musser, 1997. Switches to heapsort\n",
    "    when max recursion depth exceeded. Otherwise uses median-of-three \n",
    "    quicksort (Bentley & McIlroy, 1993) with all the usual optimizations:\n",
    "        - Swap equal elements.\n",
    "        - Only process partitions longer than the minimum size threshold.\n",
    "        - When a new partition is made, recurse on the smaller half and \n",
    "          iterate over the larger half.\n",
    "        - Make a final pass with insertion sort over the entire list.\n",
    "\n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef int max_depth = 2 * <int>log2(last-first)\n",
    "    dual_introsort_loop(items, rows, first, last, max_depth)\n",
    "    dual_insertion_sort(items, rows, first, last)\n",
    "    \n",
    "# Python wrapper\n",
    "def dual_sort(np.ndarray[np.float64_t, ndim=1, mode=\"c\"] items, np.ndarray[np.intp_t, ndim=1, mode=\"c\"] rows,\n",
    "              SIZE_t first, SIZE_t last):\n",
    "    \"\"\"Wrapper function for my Cython implementation of `introsort()`. \n",
    "    Followed Laura Mendoza's guide on how to have Cython access\n",
    "    a Numpy array using the C pointer:\n",
    "        https://members.loria.fr/LMendoza/link/Cython_speedup_notes.html#Working-with-numpy-arrays-in-I/O\n",
    "    \n",
    "    Arguments:\n",
    "        items (Numpy array, float64): The numbers to be sorted.\n",
    "        rows      (Numpy array, int): Training set row indices of all samples used to \n",
    "                                      fit the decision tree.\n",
    "        first, last            (int): The range to be sorted. \n",
    "    \"\"\"\n",
    "    cdef DTYPE_t* items_ptr = <DTYPE_t *> items.data\n",
    "    cdef SIZE_t* rows_ptr = <SIZE_t *> rows.data\n",
    "    dual_introsort(items_ptr, rows_ptr, first, last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louppe numerical split finder\n",
    "Once the Sklearn numerical splitter has sorted the node's raw feature values and argsorted the nodes indices to match, the rest is pretty straightforward:\n",
    "1. At the beginning of the split search, all of the parent node's samples sit in right child and so its weighted class counts are the same as those of the parent. The left child is empty.\n",
    "2. For each child, the numerator and denominator used to calculate its gini impurity score are updated, on-line, as more samples move from the right child to the left at each unique split point.\n",
    "3. We start our search by moving all samples that have the lowest raw value for the given feature from the right child over to the left child. We repeat this, value by value, as we proceed through all unique values up to and including the second-to-highest unique value. \n",
    "4. The split-point that gets recorded each time we shift a batch from the right to left child is the mid-point of: the value held by the group of samples that just got moved to the left side, and the next-lowest unique value (whose samples still sit in the right child).\n",
    "5. Sometimes this mid-point will be equal to that next-lowest value. When this happens, we don't use the mid-point but instead record the lower of the two values as the split point.\n",
    "6. We take advantage of situations where we can more quickly examine the rest of the split-points by reversing direction and begin moving samples from the left child back over to the right child. This can happen when the right child has many identical samples.\n",
    "7. Finally, for each split point, we keep track of the index (in the `rows` list) that would mark the start of the right child if that split point eventually gets selected as the best split point. This saves us from having to work this out all over again once the best split point is discovered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_split(rows, items, labels, node_start, n_parent, n_class, \n",
    "                   min_samples_leaf, min_weight_leaf, c_wts, l_wcc, r_wcc,\n",
    "                   parent_wcc, best_split, current_feat, parent_num, parent_den):\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "    \n",
    "    Uses Gilles Louppe's split-finding algorithm.\n",
    "    \n",
    "    Saves a split's feature idx, threshold, position, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows           (ndarray of int): Indices of all rows in the training set. \n",
    "                                         Shape: (n train samples,).\n",
    "        items      (ndarray of float64): The sorted feature values of the samples in the parent\n",
    "                                         node (beginning at `node_start`). Shape: (n train samples,).\n",
    "        labels         (ndarray of int): All training labels. Shape: (n training samples,).\n",
    "        node_start                (int): Index of the beginning of the parent node in `rows`.\n",
    "        n_parent                  (int): Number of samples in the parent node.\n",
    "        n_class                   (int): Number of unique classes in the training set.\n",
    "        min_samples_leaf          (int): Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf       (float64): Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts      (ndarray of float64): Class weights. Shape: (`n_class`,).\n",
    "        l_wcc      (ndarray of float64): Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc      (ndarray of float64): Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc (ndarray of float64): Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split              (Split): Holds the feature, threshold, position, and impurity\n",
    "                                         score of the parent node's current best split.\n",
    "        current_feat              (int): Column index of feature under investigation.\n",
    "        parent_num            (float64): Numerator of parent node's impurity score.\n",
    "        parent_den            (float64): Denominator of parent node's impurity score.\n",
    "              \n",
    "    Returns: \n",
    "        int: 1 if feature is constant for eligible split-points. 0, otherwise.\n",
    "    \"\"\"\n",
    "    # So that we can iterate across all feature values in the node.\n",
    "    prev_pos, pos = node_start, node_start\n",
    "    node_end = node_start + n_parent\n",
    "    lowest = items[pos]\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    l_num, l_den  = 0., 0.\n",
    "    r_num, r_den, = parent_num, parent_den\n",
    "    \n",
    "    # Whether or not feat is constant within search range permitted\n",
    "    # by min_samples_leaf and min_weight_leaf (0 if no, 1 if yes).\n",
    "    current_feat_const = 1\n",
    "    \n",
    "    # Find the best split and store its score, threshold, position,\n",
    "    # as well as it's children's weighted class counts.\n",
    "    while pos < node_end:\n",
    "        while items[pos] == lowest: # When consecutive items have the same value.\n",
    "            if pos == node_end - 1: # When the final few samples all have the same value.\n",
    "                return current_feat_const\n",
    "            pos+=1\n",
    "        next_lowest = items[pos]\n",
    "        mid = lowest/2. + next_lowest/2. # Split threshold is always the mid-point between two consecutive values.\n",
    "        if mid == next_lowest: mid = lowest\n",
    "            \n",
    "        # Move samples from the left to right child when it's quicker to do so.\n",
    "        if pos-prev_pos > node_end-pos-1:\n",
    "            l_num, l_den = parent_num, parent_den\n",
    "            r_num, r_den = 0., 0.\n",
    "            l_wcc[:] = parent_wcc\n",
    "            r_wcc[:] = 0.\n",
    "            for r in reversed(range(pos, node_end)):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label]\n",
    "                r_num += w*( 2*r_wcc[label] + w); r_den += w\n",
    "                l_num += w*(-2*l_wcc[label] + w); l_den -= w \n",
    "                r_wcc[label] += w; l_wcc[label] -= w\n",
    "        else:\n",
    "            for r in range(prev_pos, pos):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label] \n",
    "                l_num += w*( 2.*l_wcc[label] + w); l_den += w\n",
    "                r_num += w*(-2.*r_wcc[label] + w); r_den -= w\n",
    "                l_wcc[label] += w; r_wcc[label] -= w  \n",
    "                \n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf.\n",
    "        if pos - node_start < min_samples_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif node_end - pos < min_samples_leaf:\n",
    "            return current_feat_const\n",
    "        # l_den and r_den are left and right children's weighted sample sums.\n",
    "        elif l_den < min_weight_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif r_den < min_weight_leaf:\n",
    "            return current_feat_const\n",
    "\n",
    "        current_feat_const = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Only update best split stats if current score beats all\n",
    "            # other best found among all other features already explored \n",
    "            # at the current node.\n",
    "            best_split.score, best_split.thresh = score, mid\n",
    "            best_split.pos, best_split.feat = pos, current_feat\n",
    "        lowest = next_lowest\n",
    "        prev_pos = pos; pos+=1\n",
    "    return current_feat_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split-making Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_split(rows, X, node_info, best_split):\n",
    "    \"\"\"Split a decision tree node using a given ordered numerical feature and threshold. \n",
    "    \n",
    "    Uses the similar logic as Gilles Louppe's Cython implementation at: \n",
    "        https://github.com/scikit-learn/scikit-learn/blob/47e3358712d483a8e8dcb84d87386eb4f3d49070/sklearn/tree/_splitter.pyx#L605\n",
    "    \n",
    "    Arguments: \n",
    "        rows  (ndarray of int): Indices of all rows in the training set. \n",
    "                                Shape: (n train samples,).\n",
    "        X (ndarray of float64): The training data. Shape: (n train samples, n features).\n",
    "        node_info (StackEntry): Stats of the node to be split.\n",
    "        best_split     (Split): Stats of a node split.\n",
    "    \"\"\"\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[rows[p]][best_split.feat] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louppe Decision Tree Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_generator(seed=None):\n",
    "    \"\"\"Make a new Numpy random generator or use a previous one, if it exists.\n",
    "    \n",
    "    Inspired by sklearn's `check_random_state()` function:\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/62fc8bb94dcd65e72878c0599ff91391d9983424/sklearn/utils/validation.py#L852\n",
    "    \"\"\"\n",
    "    if isinstance(seed, np.random.Generator): \n",
    "        return seed\n",
    "    else:\n",
    "        return np.random.default_rng(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StackEntry():\n",
    "    \"\"\"Pertinent stats needed to push a decision tree node onto a LIFO priority stack.\n",
    "    \n",
    "    Attributes:\n",
    "        start         (int): Index of node's first sample.\n",
    "        end           (int): Range of all samples in node.\n",
    "        node_id       (int): Location of node in a decision tree.\n",
    "        parent_id     (int): Location of node's parent.\n",
    "        n_const_feats (int): Num features constant for samples in node.\n",
    "    \"\"\"\n",
    "    def __init__(self, start, end, node_id, parent_id, n_const_feats):\n",
    "        self.start, self.end = start, end\n",
    "        self.node_id, self.parent_id = node_id, parent_id\n",
    "        self.n_const_feats = n_const_feats\n",
    "        \n",
    "class Split():\n",
    "    \"\"\"Pertinent stats needed to compare node splits.\n",
    "    \n",
    "    Attributes:\n",
    "        feat       (int): Column index of splitting feature.\n",
    "        thresh (float64): Split threshold.\n",
    "        pos        (int): Index of first sample in split's right child.\n",
    "        score  (float64): Impurity score.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat, thresh, pos, score):\n",
    "        self.feat, self.thresh, self.pos, self.score = feat, thresh, pos, score\n",
    "        \n",
    "class Node():\n",
    "    \"\"\"A decision tree node.\n",
    "    \n",
    "    Attributes:\n",
    "        l_child    (int): Location of node's left child (-1 if leaf).\n",
    "        r_child    (int): Location of node's right child (-1 if leaf).\n",
    "        feat       (int): Column index of best split's feature (-1 if leaf).\n",
    "        thresh (float64): Threshold of best split (-np.inf if leaf).\n",
    "        label      (int): Class label if node is a leaf (-1, otherwise).\n",
    "    \"\"\"\n",
    "    def __init__(self, l_child, r_child, feat, thresh, label):\n",
    "        self.l_child, self.r_child = l_child, r_child\n",
    "        self.feat, self.thresh = feat, thresh\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLouppe():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first algorithm.\n",
    "    \n",
    "    Based on page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    Keeps track of and avoids features that are constant for a given node's samples.\n",
    "\n",
    "    Attributes:\n",
    "            m                                    (int): Number of candidate features randomly selected to try \n",
    "                                                        to split each node.\n",
    "            min_samples_leaf                     (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf         (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                        of the sum of weights of *all* training samples used to fit \n",
    "                                                        the tree.\n",
    "            class_weights         (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                                 (int): Seed of the random number generator used for tree growing.\n",
    "            rows                      (ndarray of int): Row indices of all training samples. Shape: (`n_samples`,).\n",
    "            features                  (ndarray of int): Column indices of all training features. Shape: (`n_features`,).\n",
    "            n_class                              (int): Number of unique classes in the training set.\n",
    "            n_samples                            (int): Number of samples in the training set.\n",
    "            n_features                           (int): Number of features used to train.\n",
    "            mem_capacity                         (int): Max number of tree nodes that can be stored in `self.nodes`\n",
    "                                                        and `self.weighted_class_counts`.\n",
    "            min_weight_leaf                  (float64): Total weight of any leaf's samples will be at least this much.\n",
    "            n_nodes                              (int): Number of nodes in the tree.\n",
    "            \n",
    "            Decision Tree data structure\n",
    "            ----------------------------\n",
    "            nodes                    (ndarray of Node): All nodes in the decision tree. Shape: (`n_nodes`,).\n",
    "            weighted_class_counts (ndarray of float64): Weighted class counts of training samples in each\n",
    "                                                        node. Shape: (`n_nodes` x `n_class`,).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights=[], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                         (int): Use when reproducibility desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C')\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = np.empty(0, dtype=Node, order='C')\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = np.empty(0, dtype=np.float64, order='C')\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self.n_nodes\n",
    "    \n",
    "    @property \n",
    "    def left_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].l_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def right_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].r_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_features(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].feat\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): \n",
    "        out = np.empty(self.n_nodes, dtype='float64')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].thresh\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def weighted_cc(self):\n",
    "        out_size = self.n_nodes*self.n_class\n",
    "        out = np.empty(out_size, dtype='float64')\n",
    "        for i in range(out_size):\n",
    "            out[i] = self.weighted_class_counts[i]\n",
    "        out.resize(self.n_nodes, self.n_class)\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def labels(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].label\n",
    "        return out\n",
    "    \n",
    "    def _increase_mem_capacity(self, new_capacity):\n",
    "        \"\"\"Resize ndarrays that hold tree's nodes and weighted class counts.\n",
    "        \n",
    "        Arguments:\n",
    "            new_capacity (int): Amount of nodes that resized arrays will be able to hold.\n",
    "        \"\"\"\n",
    "        self.nodes.resize(new_capacity, refcheck=False)\n",
    "        self.weighted_class_counts.resize(new_capacity*self.n_class, refcheck=False)\n",
    "    \n",
    "    def _make_leaf(self, node_id, wcc, n_classes_node):\n",
    "        \"\"\"Set and store the class label of a leaf node.\n",
    "        \n",
    "        Break ties at random when multiple classes share the same max weight.\n",
    "        Doing this avoids a bias towards lower classes that would be a possible\n",
    "        consequence of using np.argmax (which is what Sklearn does).\n",
    "        \n",
    "        Arguments:\n",
    "            node_id            (int): Location of node in `self.nodes`.\n",
    "            wcc (ndarray of float64): Node's weighted class counts. Shape: (`self.n_class`,).\n",
    "            n_classes_node     (int): Number of unique class labels found among\n",
    "                                      node's training samples.\n",
    "        \"\"\"\n",
    "        if n_classes_node == 1: \n",
    "            label = max(enumerate(wcc), key=lambda f: f[1])[0]\n",
    "        else:              \n",
    "            label = self._rng.choice(np.argwhere(wcc==np.max(wcc)).flatten())\n",
    "        self.nodes[node_id] = Node(-1, -1, -1, np.nan, label)\n",
    "        \n",
    "    def _grow_tree(self, X, y):\n",
    "        \"\"\"Depth-first growth of a decision tree.\n",
    "        \n",
    "        Arguments:\n",
    "            X (ndarray of float64): Training samples. Shape: (n samples, n features).\n",
    "            y     (ndarray of int): Training labels. Shape: (n samples,).\n",
    "        \"\"\"\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        node_stack = []\n",
    "        \n",
    "        # Stores the weighted class counts of the current node.\n",
    "        node_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        # For finding the best split.\n",
    "        l_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        r_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        items = np.empty(self.n_samples, dtype=np.float64)\n",
    "\n",
    "        # Keeping track of nodes' constant features. Uses\n",
    "        # same strategy as Sklearn: \n",
    "        #     https://github.com/scikit-learn/scikit-learn/blob/4c6fc05b2a1f11bedef5784c46b9f5d3e52489c2/sklearn/tree/_splitter.pyx#L424\n",
    "        features = self.features.copy()\n",
    "        constant_features = np.empty(self.n_features, dtype=np.intp)\n",
    "        \n",
    "        # Push root node onto the LIFO stack.\n",
    "        node_stack.append(StackEntry(0, self.n_samples, 0, 0, 0))\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "        while len(node_stack) > 0:\n",
    "            node_info = node_stack.pop()\n",
    "            start, end = node_info.start, node_info.end\n",
    "            node_id, parent_id = node_info.node_id, node_info.parent_id\n",
    "            n_consts = node_info.n_const_feats\n",
    "            n_samples_node = end-start\n",
    "            \n",
    "            # Tabulate and store the current node's weighted class counts.\n",
    "            node_wcc[:] = 0.\n",
    "            for i in range(n_samples_node):\n",
    "                row = self.rows[start + i]\n",
    "                label = y[row]\n",
    "                wt = self.class_weights[label]\n",
    "                node_wcc[label] += wt \n",
    "            self.weighted_class_counts[node_id*self.n_class: (node_id + 1)* self.n_class] = node_wcc\n",
    "            \n",
    "            # Make a leaf if required to do so.\n",
    "            n_classes_node, sum_node_wcc, sum_node_wcc_sqr = 0, 0., 0.\n",
    "            for c in range(self.n_class):\n",
    "                wcc = node_wcc[c]\n",
    "                if wcc > 0: n_classes_node += 1\n",
    "                # Compute the current node's proxy gini numerator and denominator while we're at it.\n",
    "                sum_node_wcc_sqr += wcc**2 \n",
    "                sum_node_wcc += wcc \n",
    "            if n_classes_node == 1:                      \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            \n",
    "            # Or perform a split.\n",
    "            else:\n",
    "                # Initialize stats for best split of node.\n",
    "                best_split = Split(-1, 0., -1, -np.inf)\n",
    "                \n",
    "                # Ensure feats drawn w/out replacement.\n",
    "                n_drawn_feats = 0\n",
    "                n_new_consts = 0\n",
    "                n_total_consts = n_consts\n",
    "                lb = 0                      # Range in `features` array from which we \n",
    "                ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "               \n",
    "                while n_drawn_feats < self.m:\n",
    "                    n_drawn_feats += 1\n",
    "                    idx = self._rng.choice(range(lb, ub-n_new_consts+1))\n",
    "                    \n",
    "                    # So that we don't draw a known constant feature again this split-search.\n",
    "                    if idx < n_consts:\n",
    "                        features[idx], features[lb] = features[lb], features[idx]\n",
    "                        lb += 1 \n",
    "                        continue\n",
    "                        \n",
    "                    # So that no new const feats get drawn more than once per split-search.\n",
    "                    idx += n_new_consts\n",
    "\n",
    "                    feat_idx = features[idx]  \n",
    "                    # Prepare the rows' feature values for sorting.\n",
    "                    items[start:end] = X[:,feat_idx][self.rows[start:end]]\n",
    "                    \n",
    "                    # Sort feature values and corresponding sample row indices\n",
    "                    # to prepare for numerical split finding.\n",
    "                    dual_sort(items, self.rows, start, end)\n",
    "                    \n",
    "                    # Make sure the feature not constant for node's samples.\n",
    "                    if items[start] == items[end-1]:\n",
    "                        # Move the newly-discovered constant feat to the far right-end\n",
    "                        # of the left half of `features` list holding the known const\n",
    "                        # feats as well as any other const feats newly discovered \n",
    "                        # during this node's split-search.\n",
    "                        features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                        n_new_consts += 1\n",
    "                        n_total_consts += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Initialize weighted class counts of right and left children.\n",
    "                        # Right child's counts are initially the same as parent node's.\n",
    "                        r_wcc[:] = node_wcc\n",
    "                        l_wcc[:] = 0.\n",
    "                    \n",
    "                        # If the feature has an impurity score that's better than the best score \n",
    "                        # found among all other features visited thus far for this node, find_num_split()\n",
    "                        # updates the attributes of the struct containing the node's best split info. \n",
    "                        # \n",
    "                        # But even if a new best score isn't reached, if an impurity score can\n",
    "                        # be calculated at least once during the feature's split search, the\n",
    "                        # following indicator will be toggled off, to indicate that the feature\n",
    "                        # is not constant (1 = is constant; 0 = not constant).\n",
    "                        current_feat_const = find_num_split(self.rows, items, y, start, n_samples_node, self.n_class, \n",
    "                                                            self.min_samples_leaf, self.min_weight_leaf, self.class_weights, \n",
    "                                                            l_wcc, r_wcc, node_wcc, best_split, feat_idx, sum_node_wcc_sqr, \n",
    "                                                            sum_node_wcc)\n",
    "\n",
    "                        if current_feat_const:\n",
    "                            # The feature may be constant within the search range permitted\n",
    "                            # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                            # the feature is a newly discovered constant.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The feature is non-constant, so we ensure it's not drawn again\n",
    "                            # during this split-search.\n",
    "                            features[idx], features[ub] = features[ub], features[idx]\n",
    "                            ub -= 1 \n",
    "                            \n",
    "                # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                features[0:n_consts] = constant_features[0:n_consts]\n",
    "                constant_features[n_consts:n_consts+n_new_consts] = features[n_consts:n_consts+n_new_consts]\n",
    "                \n",
    "                # Make node a leaf if constant for all randomly drawn feats.\n",
    "                # (# drawn known constant feats + # drawn new constant feats)\n",
    "                if lb + n_new_consts == n_drawn_feats: \n",
    "                    self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "                else: \n",
    "                    make_num_split(self.rows, X, node_info, best_split) \n",
    "\n",
    "                    # Update info for node that's getting split.\n",
    "                    l_child_id = self.n_nodes\n",
    "                    r_child_id = l_child_id + 1\n",
    "                    self.nodes[node_id] = Node(l_child_id, r_child_id, best_split.feat, best_split.thresh, -1)\n",
    "\n",
    "                    # Prepare for the left and right child nodes\n",
    "                    # by increasing tree data memory capacity if\n",
    "                    # necessary.\n",
    "                    if self.n_nodes + 2 > self.mem_capacity:\n",
    "                        # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                        # part of WhozCraig's SO answer at: \n",
    "                        #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                        # Add one after squaring so that the new capacity can\n",
    "                        # contain not only a tree of greater depth, but also\n",
    "                        # the maximum # nodes that that depth could have.\n",
    "                        new_capacity = 2*self.mem_capacity + 1\n",
    "                        self._increase_mem_capacity(new_capacity)\n",
    "                        self.mem_capacity = new_capacity\n",
    "                    \n",
    "                    # Push right child info onto the LIFO stack.\n",
    "                    node_stack.append(StackEntry(best_split.pos, end, r_child_id, node_id, n_total_consts))\n",
    "                    # Push left child info onto queue.\n",
    "                    node_stack.append(StackEntry(start, best_split.pos, l_child_id, node_id, n_total_consts))\n",
    "\n",
    "                    # And update size of the tree.\n",
    "                    self.n_nodes += 2\n",
    "    \n",
    "    def fit(self, X, y, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X (Fortran-style ndarray of float64): Pre-processed training data. \n",
    "                                                  Shape: (num train samples, num train features).\n",
    "            y                   (ndarray of int): Training labels. Shape: (num train samples,).\n",
    "            rows                          (list): Indices of the rows to be used for training. \n",
    "                                                  All rows used if empty.\n",
    "            features                      (list): Column indices of training features that will be used.\n",
    "                                                  All features used if empty.                           \n",
    "        Returns:\n",
    "            DecisionTreeLouppe: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        # Determine num classes found among all training samples.\n",
    "        root_cc = np.unique(y, return_counts=True)[1] # Root node class counts.\n",
    "        self.n_class = root_cc.size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "\n",
    "        self.n_samples = len(self.rows)\n",
    "        self.n_features = len(self.features)\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        init_capacity = 15\n",
    "        \n",
    "         # Allocate tree memory.\n",
    "        self._increase_mem_capacity(init_capacity)\n",
    "        self.mem_capacity = init_capacity\n",
    "        \n",
    "        # And sum the class weights of all the root node's samples in\n",
    "        # order to know minimum total weight a leaf must have (which\n",
    "        # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "        root_wcc = root_cc*self.class_weights\n",
    "        self.min_weight_leaf = self.min_weight_fraction_leaf*root_wcc.sum()\n",
    "        \n",
    "        # Initialize the random number generator.\n",
    "        self._rng = get_random_generator(self.seed)\n",
    "        \n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X, y)\n",
    "        return self\n",
    "        \n",
    "    def _next_node(self, nxt): return self.nodes[nxt]\n",
    "       \n",
    "    def _get_leaf_idx(self, i, X):\n",
    "        root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[:,leaf.feat][i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else:\n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        n_preds = X.shape[0]\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        for i in range(n_preds):\n",
    "            preds[i] = self.nodes[self._get_leaf_idx(i, X)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        for i in range(n_probs):\n",
    "            idx = self._get_leaf_idx(i, X)\n",
    "            for j in range(self.n_class):\n",
    "                wcc[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Titanic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "titanic_dir = Path.home()/'data'/'titanic'\n",
    "\n",
    "def get_titanic_data():\n",
    "    \"\"\"Download and place the kaggle Titanic train and test csv files into Pandas dataframes.\"\"\"\n",
    "    titanic_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_csv_path, test_csv_path = titanic_dir/'train.csv', titanic_dir/'test.csv'\n",
    "    if not train_csv_path.exists():\n",
    "        # Visit https://github.com/Kaggle/kaggle-api for more info on \n",
    "        # how to install the kaggle API and generate a kaggle.json key.\n",
    "        !kaggle competitions download -c titanic --path \"$titanic_dir\"\n",
    "        with ZipFile(titanic_dir/'titanic.zip', 'r') as z: z.extractall(titanic_dir) \n",
    "    titanic_train_df = pd.read_csv(train_csv_path); titanic_train_df.drop('PassengerId', axis=1, inplace=True)\n",
    "    titanic_test_df = pd.read_csv(test_csv_path)\n",
    "    return titanic_train_df, titanic_test_df\n",
    "    \n",
    "titanic_train_df, titanic_test_df = get_titanic_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_train_df = titanic_train_df.iloc[:,[i for i in range(len(titanic_train_df.columns)) if i not in [2,7,9]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(df, label_col_idx, pct_val=20):\n",
    "    \"\"\"Randomly draw samples to form a validation set.\n",
    "    \n",
    "    Arguments:            \n",
    "        df (Pandas dataframe): Dataframe containing a dataset\n",
    "        label_col_idx   (int): Index of the column containing the labels\n",
    "        pct_val         (int): Percent of the dataset's samples to go into the validation set.\n",
    "    \n",
    "    Returns: \n",
    "        the train/val inputs and labels\n",
    "    \"\"\"\n",
    "    if pct_val < 0 or pct_val > 100: print('pct_val should be an int or float between 0 and 100'); return\n",
    "    n = len(df)\n",
    "    val_idx = np.random.choice(n, size=n*pct_val//100, replace=False)\n",
    "    val_idx = np.sort(val_idx)\n",
    "    train_idx = df.index.difference(val_idx)\n",
    "    labels = df.iloc[:,label_col_idx]\n",
    "    inputs = df.iloc[:,[i for i in range(len(df.columns)) if i != label_col_idx]]\n",
    "    return inputs.loc[train_idx], labels.loc[train_idx], inputs.loc[val_idx], labels.loc[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "xTrain_titanic, yTrain_titanic, xVal_titanic, yVal_titanic = train_val_split(titanic_train_df, 0, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features to be numerically encoded using \n",
    "# PCA rank encoding.\n",
    "cat_feats = [1,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wt_cov_matrix(P, w, n):\n",
    "    \"\"\"Calculate the unbiased weighted estimated covariance of class probability \n",
    "    matrix of a categorical feature's levels.\n",
    "    \n",
    "    Parameters: \n",
    "        P (Numpy array - floats): class probability matrix of the levels of a given categorical variable.\n",
    "        w           (int vector): sample weights\n",
    "        n                  (int): number of samples in the parent node\n",
    "                \n",
    "    Returns: \n",
    "        the unbiased weighted estimated covariance matrix of P\n",
    "    \"\"\"\n",
    "    p_avg = np.sum(w[:,None]*P, axis=0)/n\n",
    "    X = P.copy(); X = X.T   # Can't alter the original matrix P; it will be needed later\n",
    "    X -= p_avg[:,None]\n",
    "    return X.dot((X*w).T)/(n - np.sum(w**2)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode(x):\n",
    "    \"\"\"Perform a naive ordinal encoding of a categorical variable's samples. Leaves any NaNs alone.\n",
    "    \n",
    "    Arguments: \n",
    "        x (vector): All samples in the categorical variable's column.\n",
    "    \n",
    "    Returns: \n",
    "        1-d array containing the variable's encoded values, \n",
    "        list of the cat variable's unique original values\n",
    "    \"\"\"\n",
    "    levels = []; [levels.append(i) for i in x if i not in levels]\n",
    "    level_map = {l:i for i, l in enumerate(levels)}  # Integer used to encode a level corresponds to the \n",
    "    x_enc = [level_map[l] for l in x]                # index where that level appears in `levels` list.\n",
    "    return x_enc, levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prob_matrix(x, y, n, ncat, nclass):\n",
    "    \"\"\"Construct a class probability matrix for a categorical variable's unique levels.\n",
    "    Computes contingency table of per-level class counts using fuglede's *wonderful* algorithm: \n",
    "        https://stackoverflow.com/questions/51294382/python-contingency-table/51294568#51294568\n",
    "    \n",
    "    Arguments: \n",
    "        x (Numpy vector of int): Naive ordinally encoded values of categorical variable.\n",
    "        y (Numpy vector of int): Class labels corresponding to rows in <x>.\n",
    "        n                 (int): Number of rows/samples.\n",
    "        ncat              (int): Number of unique categories.\n",
    "        nclass            (int): Number of unique classes.\n",
    "        \n",
    "    Returns: \n",
    "        2-d array containing the class probability matrix,\n",
    "        list of categorical level weights (the number of samples belonging to each level)\n",
    "    \"\"\"\n",
    "    N = np.bincount(nclass * x + y, minlength=ncat*nclass).reshape((ncat, nclass)) # Cat levels' class counts\n",
    "    w = N.sum(axis=1) # Level weights are number of samples per level.\n",
    "    P = np.zeros((ncat, nclass))                        # First initialize P as all zeros to prevent the output \n",
    "    np.divide(N, w[:,None], where=w[:,None]!=0, out=P)  # of np.divide from being numerically unstable.\n",
    "    return P, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_rank_encode(x_int, y, levels, n, nclass):\n",
    "    \"\"\"PCA rank encode a categorical variable using technique from Coppersmith et. al. (1999):\n",
    "        https://link.springer.com/article/10.1023%2FA%3A1009869804967\n",
    "        \n",
    "    Arguments: \n",
    "        x (Numpy vector of int): Naive ordinally encoded values of categorical variable.\n",
    "        y (Numpy vector of int): Class labels corresponding to rows in <x>.\n",
    "        levels (list): All original unique categorical levels.\n",
    "        ncat (int): Number of unique categories.\n",
    "        nclass (int): Number of unique classes.\n",
    "                \n",
    "    Returns: 1-d array containing the categorical variable's PCA rank encoded values,\n",
    "             dict holding the level:PCArank mapping for each unique categorical value\n",
    "    \"\"\"\n",
    "    ncat = len(levels)                           \n",
    "    P, w = get_prob_matrix(x_int, y, n, ncat, nclass)      # Construct the levels' class prob. matrix.\n",
    "    Sigma = wt_cov_matrix(P, w, n)                         # Then build the weighted cov matrix.\n",
    "    v = np.linalg.svd(Sigma)[2][0]                         # Next, get direction of 1st principal component.\n",
    "    s = P.dot(v)                                           # And project levels' class prob coords. onto 1st PC.\n",
    "    zipped = list(zip(levels, list(range(ncat))))          # Pair levels with their ordinal encodings.\n",
    "    sorted = [zipped[i] for i in np.argsort(s)]            # Then sort these pairs according to levels' PC ranks.\n",
    "    int_pca_map = {z[1]:r for r, z in enumerate(sorted)}   # Map each ordinal encoding to the appropriate PC rank.\n",
    "    x_pca = [int_pca_map[i] for i in x_int]                # Exchange column's ordinal values with these ranks.\n",
    "    level_pca_map = {z[0]:r for r, z in enumerate(sorted)} # Finally, map each orig. level to its PC rank.\n",
    "    return x_pca, level_pca_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "def preprocess_train(x, y, cat_feats=[]):\n",
    "    \"\"\"Pre-process training data and labels for training via random forests.\n",
    "    \n",
    "    Uses these heuristics:\n",
    "        1. Categorical features by default are preprocessed using PCA encoding from:\n",
    "               Coppersmith et. al. (1999): https://link.springer.com/article/10.1023%2FA%3A1009869804967\n",
    "           This done only once at the beginning of training (and not at each split), from:\n",
    "               Wright and KÃ¶nig (2019): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6368971/pdf/peerj-07-6339.pdf\n",
    "           Avoids the absent levels problem described in:\n",
    "               Wu (2018): https://dl.acm.org/doi/abs/10.5555/3291125.3309607\n",
    "        2. Numerical feature NaNs replaced by median value.\n",
    "        3. Categorical feature NaNs replaced by mode level. Both these NaN strategies\n",
    "           are Breiman's \"current preferred method\" from:\n",
    "               Breiman (2002): https://www.stat.berkeley.edu/~breiman/Using_random_forests_V3.1.pdf\n",
    "    \n",
    "    Arguments: \n",
    "        x (Pandas or Numpy array): The original un-preprocessed training data.\n",
    "        y (Pandas or Numpy array): Numerically encoded training labels.\n",
    "        cat_feats          (list): Categorical features' column indices.\n",
    "        \n",
    "    Returns: \n",
    "        The processed training data and labels, a dictionary of values used to fill\n",
    "        each column's NaN values, and a dictionary containing categorical level-to-PCA maps.\n",
    "        The contents of both these dicts are stored under column index numbers.\n",
    "    \"\"\"\n",
    "    x, y = np.asfortranarray(x), np.ascontiguousarray(y)\n",
    "    n, nclass, nfeat, has_cat_feats = len(x), len(np.unique(y)), x.shape[1], len(cat_feats) > 0\n",
    "    num_feats = [i for i in range(nfeat) if i not in cat_feats]; has_num_feats = nfeat > len(cat_feats)\n",
    "    nan_fillers, pca_maps = {}, {} # NaN fill values and cat level-to-PCA mappings stored under feat col idxs.\n",
    "    if has_cat_feats:\n",
    "        for i in cat_feats:\n",
    "            values = x[:,i]\n",
    "            nans = pd.isna(values); has_nans = nans.sum() > 0\n",
    "            # Step 1: Naive ordinal encode all non-NaN categorical values.\n",
    "            values[~nans], levels = integer_encode(values[~nans])\n",
    "            # Step 2: Store modes of all categorical features.\n",
    "            mode = stats.mode(values.astype(float), nan_policy='omit').mode[0]\n",
    "            # Step 3: Replace any NaNs in cat cols with cols' modes.\n",
    "            if has_nans: values[nans] = mode\n",
    "            # Step 4: PCA rank-encode all categorical features.\n",
    "            x[:,i], pca_maps[i] = PCA_rank_encode(values.astype(int), y, levels, n, nclass)\n",
    "            nan_fillers[i] = pca_maps[i][levels[int(mode)]] # Store the PCA rank-encoded mode value for each cat feat.\n",
    "    if has_num_feats:                                       # The levels list stores orig. level strings in order of \n",
    "        values = x[:,num_feats].astype(float)               # their naive ordinal encodings.\n",
    "        nans = np.isnan(values); has_nans = nans.sum() > 0\n",
    "        # Step 5: Store medians of all numerical features.\n",
    "        medians = np.nanmedian(values, axis=0)\n",
    "        for i,m in enumerate(medians): nan_fillers[num_feats[i]] = m\n",
    "        # Step 6: Replace any NaNs in num cols with cols' medians.\n",
    "        if has_nans: x[:,num_feats] = np.where(nans, medians, x[:,num_feats])\n",
    "    return x.astype('float64'), y, nan_fillers, pca_maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_test(x, nan_fillers, cat_feats=[], pca_maps=None):\n",
    "    \"\"\"Pre-process test (or any unlabelled) data for inference via random forests.\n",
    "    \n",
    "    Fills NaNs and encodes categorical labels using values derived from a training set.\n",
    "    \n",
    "    Arguments: \n",
    "        x (Pandas or Numpy array): The original un-preprocessed test data.\n",
    "        nan_fillers        (dict): The values (stored under col idxs) used to fill each column's NaN rows.\n",
    "        cat_feats          (list): Categorical features' column indices.\n",
    "        pca_maps           (dict): Categorical level-to-PCA maps (each stored under a col's idx).\n",
    "         \n",
    "    Returns: \n",
    "        The processed test data.\n",
    "    \"\"\"\n",
    "    x, nfeat, has_cat_feats = np.asfortranarray(x), x.shape[1], len(cat_feats) > 0\n",
    "    if has_cat_feats:\n",
    "        for i in cat_feats:\n",
    "            # Step 1: Replace any new levels with NaN.\n",
    "            levels = list(pca_maps[i].keys())\n",
    "            new_levels = [l for l in x[:,i] if l not in levels]\n",
    "            if len(new_levels) > 0: \n",
    "                new_levels_rows = np.stack([x[:,i] == l for l in new_levels]).sum(axis=0) > 0\n",
    "                x[:,i] = np.where(new_levels_rows, np.nan, x[:,i])\n",
    "            # Step 2: Replace levels in each cat col's non-NaN rows with proper PCA ranks.\n",
    "            nans = pd.isna(x[:,i])\n",
    "            pca_enc = [pca_maps[i][l] for l in x[:,i][~nans]]\n",
    "            x[:,i][~nans] = pca_enc\n",
    "    # Step 3: Replace all columns' NaNs   \n",
    "    nan_filler_list = np.array([nan_fillers[i] for i in np.sort(list(nan_fillers.keys()))], dtype=object)\n",
    "    nans = pd.isna(x); has_nans = nans.sum() > 0\n",
    "    if has_nans: x = np.where(nans, nan_filler_list, x)\n",
    "    x = np.asfortranarray(x)\n",
    "    return x.astype('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the Titanic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_proc, yTrain_proc, nan_fillers, pca_maps = preprocess_train(xTrain_titanic, yTrain_titanic, cat_feats)\n",
    "xVal_proc = preprocess_test(xVal_titanic, nan_fillers, cat_feats, pca_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(preds, labels):\n",
    "    return np.sum(np.array(preds) != np.array(labels))/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds, labels):\n",
    "    return 1 - error(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Louppe Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4 # number of features randomly selected as candidates for each split.\n",
    "dt = DecisionTreeLouppe(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Number of nodes in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752808988764045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.1 ms Â± 967 Âµs per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504 Âµs Â± 10.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louppe Decision Tree Cython Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "# distutils: language = c++\n",
    "# distutils: extra_compile_args = -std=c++11\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t # Signed, same as ssize_t in C. See MSeifert's SO answer: https://stackoverflow.com/a/46416257/8628758\n",
    "cimport cython\n",
    "from libc.math cimport log as ln\n",
    "from libc.stdlib cimport realloc, free\n",
    "from libc.string cimport memcpy\n",
    "from libc.string cimport memset\n",
    "from libcpp.stack cimport stack\n",
    "\n",
    "# For C++ random number generation.\n",
    "from libc.stdint cimport uint_fast32_t \n",
    "\n",
    "# Swap helper func for sorting.\n",
    "cdef inline void dual_swap(DTYPE_t* items, SIZE_t* rows, SIZE_t i, SIZE_t j) nogil:\n",
    "    items[i], items[j] = items[j], items[i]\n",
    "    rows[i], rows[j] = rows[j], rows[i]\n",
    "\n",
    "# Quicksort helpers\n",
    "\n",
    "cdef inline void dual_med_three(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find the median-of-three pivot point of the second through final \n",
    "    items of a list of numbers. Once identified, the pivot is moved to \n",
    "    the front of the list. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t middle = <int>(first + (last - first)/2)\n",
    "    cdef SIZE_t second = first + 1\n",
    "    last -= 1\n",
    "    if items[second] < items[middle]:\n",
    "        if items[middle] < items[last]:\n",
    "            dual_swap(items, rows, first, middle)    \n",
    "        elif items[second] < items[last]:\n",
    "            dual_swap(items, rows, first, last)         \n",
    "        else:                        \n",
    "            dual_swap(items, rows, first, second)\n",
    "    elif items[second] < items[last]:\n",
    "        dual_swap(items, rows, first, second)\n",
    "    elif items[middle] < items[last]:\n",
    "        dual_swap(items, rows, first, last)\n",
    "    else:\n",
    "        dual_swap(items, rows, first, middle)\n",
    "\n",
    "cdef inline SIZE_t dual_partition(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, SIZE_t pivot) nogil:\n",
    "    \"\"\"Group numbers less than the pivot value together on the left and\n",
    "    those that are greater on the right. Find the index that separates\n",
    "    these two groups, which will belong to the first item that is greater\n",
    "    than or equal to the pivot. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1885\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        pivot      : Index holding the median pivot value.\n",
    "        \n",
    "    Returns:\n",
    "        Index of cut point used to partition the items into two smaller sequences.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while first < last and items[first] < items[pivot]:\n",
    "            first += 1                      # Get index of first item greater than or equal to median-of-three pivot. \n",
    "        last -= 1\n",
    "        while items[pivot] < items[last]:\n",
    "            last -= 1                       # Get index of last item less than or equal to the pivot.\n",
    "        if not (first < last): \n",
    "            return first                    # After swaps are done, return index of first item in right partition.\n",
    "        \n",
    "        dual_swap(items, rows, first, last) # Swap the first item greater than or equal to the pivot with the\n",
    "                                            # last item less than or equal to the pivot. \n",
    "        first += 1\n",
    "\n",
    "cdef inline void dual_insertion_sort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Follows the spirit of the Numpy implementation at: \n",
    "        https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t i\n",
    "    cdef SIZE_t j\n",
    "    cdef SIZE_t k\n",
    "    cdef DTYPE_t val\n",
    "    for i in range(first+1, last):\n",
    "        j = i\n",
    "        k = i - 1\n",
    "        val = items[i]\n",
    "        row = rows[i]\n",
    "        while (j > first) and val < items[k]:\n",
    "            items[j] = items[k]\n",
    "            rows[j] = rows[k]\n",
    "            j-=1\n",
    "            k-=1\n",
    "        items[j] = val\n",
    "        rows[j] = row\n",
    "\n",
    "# Heapsort\n",
    "\n",
    "cdef inline void dual_sift_down(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n, \n",
    "                                SIZE_t p, SIZE_t c, DTYPE_t val, SIZE_t row) nogil:\n",
    "    \"\"\"Swap a heap item with one of its children if that child's value is \n",
    "    greater than or equal to that parent's value. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L61\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "        p    : Index of the parent.\n",
    "        c    : Index of the parent's first (left) child.\n",
    "        val  : The parent's value.\n",
    "        row  : The parent's training row index.\n",
    "    \"\"\"\n",
    "    while c < n:    # Look at the descendents of current parent, `p`.\n",
    "        if c < n-1 and items[start + c] < items[start + c + 1]: # Find larger of the first and second children.\n",
    "            c += 1\n",
    "        if val < items[start + c]: # If child greater than parent, swap child and parent.\n",
    "            items[start + p] = items[start + c]\n",
    "            rows[start + p] = rows[start + c]\n",
    "            p = c   # Current greater child becomes the parent.\n",
    "            c += c  # Look at this child's child, if it exists.\n",
    "        else:\n",
    "            break \n",
    "    items[start + p] = val\n",
    "    rows[start + p] = row\n",
    "\n",
    "cdef inline void dual_sort_heap(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Sort a binary max heap of numbers. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L77\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t row\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        val = items[start + n]\n",
    "        row = rows[start + n]\n",
    "        items[start + n] = items[start]\n",
    "        rows[start + n] = rows[start]\n",
    "        dual_sift_down(items, rows, start, n, 0, 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapify(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Turn a list of items into a binary max heap. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L59\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t p\n",
    "    cdef SIZE_t last_p = (n-2)//2\n",
    "    for p in range(last_p, -1, -1):\n",
    "        val = items[start + p] # value of last parent\n",
    "        row = rows[start + p]\n",
    "        dual_sift_down(items, rows, start, n, p, 2*p + 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapsort(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Applies the heapsort algorithm to sort a list of items from least to greatest. \n",
    "    From Williams, 1964.\n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    dual_heapify(items, rows, start, n)\n",
    "    dual_sort_heap(items, rows, start, n)\n",
    "    \n",
    "# Introsort \n",
    "\n",
    "cdef void dual_introsort_loop(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, int depth) nogil:\n",
    "    \"\"\"The recursive heart of the introsort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        depth      : Current recursion depth.\n",
    "    \"\"\"\n",
    "    cdef int MIN_SIZE_THRESH = 16\n",
    "    cdef SIZE_t cut\n",
    "    while last-first > MIN_SIZE_THRESH:\n",
    "        if depth == 0:\n",
    "            dual_heapsort(items, rows, first, last-first)\n",
    "        depth -= 1\n",
    "        dual_med_three(items, rows, first, last)\n",
    "        cut = dual_partition(items, rows, first+1, last, first)\n",
    "        dual_introsort_loop(items, rows, cut, last, depth)\n",
    "        last = cut\n",
    "\n",
    "# Log base-2 helper function. From Sklearn's implementation at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_utils.pyx#L7\n",
    "cdef inline DTYPE_t log2(DTYPE_t x) nogil:\n",
    "    return ln(x) / ln(2.0)\n",
    "\n",
    "cdef void dual_introsort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Implementation as described in Musser, 1997. Switches to heapsort\n",
    "    when max recursion depth exceeded. Otherwise uses median-of-three \n",
    "    quicksort (Bentley & McIlroy, 1993) with all the usual optimizations:\n",
    "        - Swap equal elements.\n",
    "        - Only process partitions longer than the minimum size threshold.\n",
    "        - When a new partition is made, recurse on the smaller half and \n",
    "          iterate over the larger half.\n",
    "        - Make a final pass with insertion sort over the entire list.\n",
    "\n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef int max_depth = 2 * <int>log2(last-first)\n",
    "    dual_introsort_loop(items, rows, first, last, max_depth)\n",
    "    dual_insertion_sort(items, rows, first, last)\n",
    "    \n",
    "# For convenient memory reallocation.\n",
    "ctypedef fused realloc_t:\n",
    "    SIZE_t\n",
    "    DTYPE_t\n",
    "    Node\n",
    "\n",
    "cdef inline realloc_t* safe_realloc(realloc_t* ptr, SIZE_t n_items) nogil except *:\n",
    "    # Inspired by Sklearn's safe_realloc() func. However, thankfully\n",
    "    # Cython now no longer requires us to send a pointer to a pointer\n",
    "    # in order to prevent crashes.\n",
    "    cdef realloc_t elem = ptr[0]\n",
    "    cdef SIZE_t n_bytes = n_items * sizeof(elem)\n",
    "    # Make sure we're not trying to allocate too much memory.\n",
    "    if n_bytes/sizeof(elem) != n_items:\n",
    "        with gil:\n",
    "            raise MemoryError(f\"Overflow error: unable to allocate {n_bytes} bytes.\")       \n",
    "    cdef realloc_t* res_ptr = <realloc_t *> realloc(ptr, n_bytes)\n",
    "    with gil:\n",
    "        if not res_ptr: raise MemoryError()\n",
    "    return res_ptr\n",
    "\n",
    "# C++ random number generator. Not yet a part of a Cython release so\n",
    "# pasted in from: \n",
    "#     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/Cython/Includes/libcpp/random.pxd#L1\n",
    "cdef extern from \"<random>\" namespace \"std\" nogil:\n",
    "    cdef cppclass random_device:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        random_device() except +\n",
    "        result_type operator()() except +\n",
    "\n",
    "    cdef cppclass mt19937:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        mt19937() except +\n",
    "        mt19937(result_type seed) except +\n",
    "        result_type operator()() except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        void discard(size_t z) except +\n",
    "        void seed(result_type seed) except +\n",
    "\n",
    "    cdef cppclass uniform_int_distribution[T]:\n",
    "        ctypedef T result_type\n",
    "        uniform_int_distribution() except +\n",
    "        uniform_int_distribution(T, T) except +\n",
    "        result_type operator()[Generator](Generator&) except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        \n",
    "# Info for any node that will eventually be split or made into a leaf.\n",
    "# Similar to what Sklearn does at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/a2c4d8b1f4471f52a4fcf1026f495e637a472568/sklearn/tree/_tree.pyx#L126\n",
    "cdef struct StackEntry:\n",
    "    SIZE_t start\n",
    "    SIZE_t end\n",
    "    SIZE_t node_id\n",
    "    SIZE_t parent_id\n",
    "    SIZE_t n_const_feats\n",
    "\n",
    "# To compare node splits.\n",
    "cdef struct Split:\n",
    "    SIZE_t feat\n",
    "    DTYPE_t thresh\n",
    "    SIZE_t pos\n",
    "    DTYPE_t score  \n",
    "\n",
    "# Vital characteristics of a node. Set when it's added to the tree.\n",
    "cdef struct Node:\n",
    "    SIZE_t l_child # idx of left child, -1 if leaf\n",
    "    SIZE_t r_child # idx of right child, -1 if leaf\n",
    "    SIZE_t feat    # col idx of split feature, -1 if leaf\n",
    "    DTYPE_t thresh # double split threshold, NAN if leaf\n",
    "    SIZE_t label   # class label if leaf, -1 if non-leaf.\n",
    "\n",
    "cdef inline void find_num_split(SIZE_t* rows, DTYPE_t* items, SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, \n",
    "                                SIZE_t n_class, SIZE_t min_samples_leaf, DTYPE_t min_weight_leaf, DTYPE_t* c_wts,\n",
    "                                DTYPE_t* l_wcc, DTYPE_t* r_wcc, DTYPE_t* parent_wcc, Split* best_split, \n",
    "                                SIZE_t current_feat, DTYPE_t parent_num, DTYPE_t parent_den, \n",
    "                                bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "    \n",
    "    Uses Gilles Louppe's split-finding algorithm:\n",
    "        Page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    \n",
    "    Saves a split's feature idx, threshold, position, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows              : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        items             : The sorted feature values of the samples in the parent\n",
    "                            node (beginning at `node_start`). Shape: (n train samples,).\n",
    "        labels            : All training labels. Shape: (n training samples,).\n",
    "        node_start        : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent          : Number of samples in the parent node.\n",
    "        n_class           : Number of unique classes in the training set.\n",
    "        min_samples_leaf  : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf   : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts             : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc             : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc             : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc        : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split        : Holds the feature, threshold, position, and impurity\n",
    "                            score of the parent node's current best split.\n",
    "        current_feat      : Column index of feature under investigation.\n",
    "        parent_num        : Numerator of parent node's impurity score.\n",
    "        parent_den        : Denominator of parent node's impurity score.\n",
    "        current_feat_const: Whether current splitting feature is constant for all eligible split \n",
    "                            thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, w, score\n",
    "    cdef SIZE_t row, label, r\n",
    "    \n",
    "    # To iterate across all the node's samples' feature values.\n",
    "    cdef SIZE_t prev_pos, pos, node_end\n",
    "    cdef DTYPE_t lowest, next_lowest, mid\n",
    "    \n",
    "    prev_pos, pos = node_start, node_start\n",
    "    node_end = node_start + n_parent\n",
    "    lowest = items[pos]\n",
    "    l_num, l_den = 0., 0.\n",
    "    r_num, r_den = parent_num, parent_den\n",
    "    \n",
    "    # Find the best split and store its score, threshold, position,\n",
    "    # as well as it's children's weighted class counts.\n",
    "    while pos < node_end:\n",
    "        while items[pos] == lowest: # When consecutive items have the same value.\n",
    "            if pos == node_end - 1: # When the final few samples all have the same value.\n",
    "                return\n",
    "            pos+=1\n",
    "        next_lowest = items[pos]\n",
    "        mid = lowest/2. + next_lowest/2. # Split threshold is always the mid-point between two consecutive values.\n",
    "        if mid == next_lowest: mid = lowest\n",
    "\n",
    "        # Move samples from the left to right child when it's quicker to do so.\n",
    "        if pos-prev_pos > node_end-pos-1:\n",
    "            l_num, l_den = parent_num, parent_den\n",
    "            r_num, r_den = 0., 0.\n",
    "            memcpy(l_wcc, parent_wcc, n_class*sizeof(DTYPE_t))\n",
    "            memset(r_wcc, 0, n_class*sizeof(DTYPE_t))\n",
    "            for r in reversed(range(pos, node_end)):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label]\n",
    "                r_num += w*( 2*r_wcc[label] + w); r_den += w\n",
    "                l_num += w*(-2*l_wcc[label] + w); l_den -= w \n",
    "                r_wcc[label] += w; l_wcc[label] -= w\n",
    "        else:\n",
    "            for r in range(prev_pos, pos):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label] \n",
    "                l_num += w*( 2.*l_wcc[label] + w); l_den += w\n",
    "                r_num += w*(-2.*r_wcc[label] + w); r_den -= w\n",
    "                l_wcc[label] += w; r_wcc[label] -= w\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf.\n",
    "        if pos - node_start < min_samples_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif node_end - pos < min_samples_leaf:\n",
    "            return\n",
    "        # l_den and r_den are left and right children's weighted sample sums.\n",
    "        elif l_den < min_weight_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif r_den < min_weight_leaf:\n",
    "            return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Only update best split stats if current score beats all\n",
    "            # other best found among all other features already explored \n",
    "            # at the current node.\n",
    "            best_split.score, best_split.thresh = score, mid\n",
    "            best_split.pos, best_split.feat = pos, current_feat\n",
    "        lowest = next_lowest\n",
    "        prev_pos = pos; pos+=1\n",
    "\n",
    "cdef inline void make_num_split(SIZE_t* rows, DTYPE_t* X, StackEntry* node_info, Split* best_split, \n",
    "                                SIZE_t n_samples) nogil:\n",
    "    cdef SIZE_t p, p_end\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[best_split.feat*n_samples + rows[p]] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p] \n",
    "\n",
    "# Necessary constants.\n",
    "cdef DTYPE_t NEG_INF = -np.inf\n",
    "cdef DTYPE_t NAN = np.nan\n",
    "            \n",
    "cdef class _DecisionTree:\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first algorithm.\n",
    "    \n",
    "    Based on page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    Keeps track of and avoids features that are constant for a given node's samples.\n",
    "    \n",
    "    Attributes:\n",
    "        m                       : Number of candidate features randomly selected to try to split each node.\n",
    "        min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                  of the sum of weights of *all* training samples used to fit \n",
    "                                  the tree.\n",
    "        class_weights           : Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "        seed                    : Seed of the random number generator used for tree growing.\n",
    "        rng                     : C++ 19937 32bit int random number generator.\n",
    "        rows                    : Row indices of all training samples. Shape: (`n_samples`,).\n",
    "        features                : Column indices of all training features. Shape: (`n_features`,).\n",
    "        n_class                 : Number of unique classes in the training set.\n",
    "        n_samples               : Number of samples in the training set.\n",
    "        n_features              : Number of features used to train.\n",
    "        mem_capacity            : Max number of tree nodes that can be stored in `self.nodes`\n",
    "                                  and `self.weighted_class_counts`.\n",
    "        min_weight_leaf         : Total weight of any leaf's samples will be at least this much.\n",
    "        n_nodes                 : Number of nodes in the tree.\n",
    "\n",
    "        Decision Tree data structure\n",
    "        ----------------------------\n",
    "        nodes                   : All nodes in the decision tree. Shape: (`n_nodes`,).\n",
    "        weighted_class_counts   : Weighted class counts of training samples in each node.\n",
    "                                  Shape: (`n_nodes` x `n_class`,).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Class attributes.\n",
    "    cdef SIZE_t seed\n",
    "    cdef mt19937 rng\n",
    "    cdef SIZE_t mem_capacity\n",
    "    cdef SIZE_t n_samples\n",
    "    cdef SIZE_t n_features\n",
    "    cdef SIZE_t n_class\n",
    "    cdef SIZE_t m\n",
    "    cdef SIZE_t min_samples_leaf, \n",
    "    cdef DTYPE_t min_weight_fraction_leaf\n",
    "    cdef DTYPE_t min_weight_leaf\n",
    "    cdef SIZE_t n_nodes\n",
    "    cdef SIZE_t* rows\n",
    "    cdef SIZE_t* features\n",
    "    cdef DTYPE_t* class_weights\n",
    "    cdef Node* nodes\n",
    "    cdef DTYPE_t* weighted_class_counts\n",
    "    def __cinit__(self, SIZE_t m, SIZE_t min_samples_leaf, DTYPE_t min_weight_fraction_leaf, SIZE_t seed): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                       : Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                      of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                    : A seed for the C++ mt19937 32bit int random generator. \n",
    "                                      Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m, self.min_samples_leaf = m, min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = NULL\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = NULL \n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        free(self.nodes)\n",
    "        free(self.weighted_class_counts)\n",
    "        \n",
    "    property size:\n",
    "        def __get__(self):\n",
    "            return self.n_nodes\n",
    "    \n",
    "    property left_children:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].l_child\n",
    "            return out\n",
    "\n",
    "    property right_children:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].r_child\n",
    "            return out\n",
    "        \n",
    "    property split_features: \n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].feat\n",
    "            return out\n",
    "        \n",
    "    property split_thresholds:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].thresh\n",
    "            return out\n",
    "        \n",
    "    property weighted_cc:\n",
    "        def __get__(self):\n",
    "            cdef SIZE_t out_size = self.n_nodes*self.n_class\n",
    "            out = np.empty(out_size, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(out_size):\n",
    "                    out_view[i] = self.weighted_class_counts[i]\n",
    "            out.resize(self.n_nodes, self.n_class)\n",
    "            return out\n",
    "    \n",
    "    property labels:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].label\n",
    "            return out\n",
    "    \n",
    "    cdef void _increase_mem_capacity(self, SIZE_t new_capacity) nogil:\n",
    "        self.nodes = safe_realloc(self.nodes, new_capacity)\n",
    "        self.weighted_class_counts = safe_realloc(self.weighted_class_counts, self.n_class*new_capacity)\n",
    "    \n",
    "    cdef void _make_leaf(self, Node* leaf_node, SIZE_t* y, SIZE_t node_start, SIZE_t node_id, \n",
    "                         SIZE_t n_classes_node, SIZE_t* max_wt_classes) nogil:\n",
    "        # Class with largest wcc becomes leaf node's label. Break ties with a random choice.\n",
    "        cdef SIZE_t label\n",
    "        cdef DTYPE_t max_wt = 0.\n",
    "        cdef SIZE_t lb = 0\n",
    "        cdef SIZE_t ub = -1\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t i, j\n",
    "        # If all node's samples have the same class.\n",
    "        if n_classes_node == 1:\n",
    "            label = y[self.rows[node_start]]\n",
    "        else:\n",
    "            # Otherwise find label with max weighted class count for the node.\n",
    "            for i in range(self.n_class):\n",
    "                max_wt = max(max_wt, self.weighted_class_counts[node_id*self.n_class + i])\n",
    "            # See if multiple classes share this max count.\n",
    "            for i in range(self.n_class):\n",
    "                if self.weighted_class_counts[node_id*self.n_class + i] == max_wt:\n",
    "                    ub += 1\n",
    "                    max_wt_classes[ub] = i\n",
    "            # If so, randomly choose leaf's label from among those classes.\n",
    "            if ub > 0:\n",
    "                dist = uniform_int_distribution[SIZE_t](lb, ub) # Choose an int w/in range lb, ub, inclusive.\n",
    "                j = dist(self.rng)\n",
    "                label = max_wt_classes[j]\n",
    "            else:\n",
    "                label = max_wt_classes[lb]\n",
    "        leaf_node.l_child = -1\n",
    "        leaf_node.r_child = -1    \n",
    "        leaf_node.feat = -1  \n",
    "        leaf_node.thresh = NAN\n",
    "        leaf_node.label = label \n",
    "\n",
    "    cdef _grow_tree(self, DTYPE_t* X, SIZE_t* y):\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        cdef stack[StackEntry] node_stack\n",
    "\n",
    "        #####################################################################\n",
    "        # Variables containing info of the node currently being investigated.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t start, end, node_id, parent_id, n_consts, n_samples_node\n",
    "        cdef DTYPE_t* node_wcc = NULL\n",
    "        cdef StackEntry node_info\n",
    "        cdef Node* node = NULL\n",
    "        \n",
    "        # Holds child node info if the current node gets split.\n",
    "        cdef SIZE_t l_child_id, r_child_id\n",
    "        cdef Node* l_child_node = NULL\n",
    "        cdef Node* r_child_node = NULL\n",
    "        \n",
    "        #####################################################################\n",
    "        # For finding the best split.\n",
    "        #####################################################################\n",
    "        cdef Split best_split\n",
    "        cdef DTYPE_t* l_wcc = NULL\n",
    "        cdef DTYPE_t* r_wcc = NULL\n",
    "        cdef DTYPE_t sum_node_wcc_sqr, sum_node_wcc # Parent node's proxy Gini score num and den.\n",
    "        \n",
    "        # Indicates a feature has been discovered to be constant during a\n",
    "        # split search within the search range permitted by min_samples_leaf \n",
    "        # and min_weight_leaf.\n",
    "        cdef bint current_feat_const \n",
    "\n",
    "        # Create a C-contiguous array of doubles to hold feature values of a \n",
    "        # given node's samples. Using Numpy to allocate memory to longer \n",
    "        # vectors is often faster than using realloc().\n",
    "        cdef DTYPE_t[::1] items_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* items = &items_buffer[0]\n",
    "        cdef SIZE_t r\n",
    "        \n",
    "        #####################################################################\n",
    "        # For random feature selection (w/out replacement) and keeping track \n",
    "        # of nodes' constant features. \n",
    "        #####################################################################\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t lb, ub, idx, feat_idx, n_drawn_feats, n_new_consts, n_total_consts\n",
    "        cdef SIZE_t[::1] features_buffer = np.empty(self.n_features, dtype=np.intp) \n",
    "        cdef SIZE_t* features = &features_buffer[0]\n",
    "        cdef SIZE_t[::1] constant_features_buffer = np.empty(self.n_features, dtype=np.intp)\n",
    "        cdef SIZE_t* constant_features = &constant_features_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For determining whether node should be a leaf.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t i, c, cc, n_classes_node, row, label\n",
    "        cdef DTYPE_t wcc, wt\n",
    "        # Stores classes that share a leaf's max class wt. When two or more \n",
    "        # present, leaf label randomly chosen from these classes\n",
    "        cdef SIZE_t* max_wt_classes = NULL\n",
    "        \n",
    "        with nogil:\n",
    "            # Allocate memory to pointers.\n",
    "            l_wcc = safe_realloc(l_wcc, self.n_class)\n",
    "            r_wcc = safe_realloc(r_wcc, self.n_class)\n",
    "            node_wcc = safe_realloc(node_wcc, self.n_class)\n",
    "            max_wt_classes = safe_realloc(max_wt_classes, self.n_class*sizeof(SIZE_t))\n",
    "            # Fill with feature column indices so we can track constant feats.\n",
    "            memcpy(features, self.features, self.n_features* sizeof(SIZE_t))\n",
    "            \n",
    "            # Push root node onto the LIFO stack.\n",
    "            node_stack.push({\"start\": 0, \"end\": self.n_samples, \"node_id\": 0, \n",
    "                             \"parent_id\": 0, \"n_const_feats\": 0})\n",
    "            self.n_nodes = 1\n",
    "            while not node_stack.empty():\n",
    "                node_info = node_stack.top()\n",
    "                node_stack.pop()\n",
    "                start, end = node_info.start, node_info.end\n",
    "                node_id, parent_id = node_info.node_id, node_info.parent_id # TODO: `parent_id` unused; is it necessary?\n",
    "                n_consts = node_info.n_const_feats\n",
    "                n_samples_node = end-start\n",
    "                node = &self.nodes[node_id]\n",
    "                \n",
    "                # Tabulate the current node's weighted class counts.\n",
    "                #\n",
    "                # Implementation detail #1: I tried storing the l and r child wt class cts\n",
    "                # of nodes' best splits so that this tabulation wouldn't need to be \n",
    "                # performed for each node. But found there was virtually no speed improvement\n",
    "                # to justify the more complicated code required to store and update these \n",
    "                # values during the best split search.\n",
    "                #\n",
    "                # Implementation detail #2: Setting aside a block of memory to \n",
    "                # store the current node's wt class cts and passing a pointer to\n",
    "                # this block to the split search function sped up training by 8%\n",
    "                # compared to passing a ptr to the location of node's wt class cts \n",
    "                # in the self.weighted_class_counts array.\n",
    "                memset(node_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                sum_node_wcc, sum_node_wcc_sqr = 0., 0.\n",
    "                for i in range(n_samples_node):\n",
    "                    row = self.rows[start + i]\n",
    "                    label = y[row]\n",
    "                    wt = self.class_weights[label]\n",
    "                    # Compute the node's proxy gini numerator and denominator while we're at it.\n",
    "                    sum_node_wcc_sqr += wt*(2*node_wcc[label] + wt) # numerator\n",
    "                    sum_node_wcc += wt                              # denominator\n",
    "                    node_wcc[label] += wt\n",
    "                memcpy(&self.weighted_class_counts[node_id*self.n_class], node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                \n",
    "                # Make a leaf if required to do so. \n",
    "                n_classes_node = 0\n",
    "                for c in range(self.n_class):\n",
    "                    wcc = node_wcc[c]\n",
    "                    if wcc > 0: n_classes_node += 1\n",
    "                if n_classes_node == 1:                   \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "\n",
    "                # Otherwise split the node.\n",
    "                else:\n",
    "                    # Initialize stats for best split of node.\n",
    "                    best_split.feat = -1\n",
    "                    best_split.thresh = 0.\n",
    "                    best_split.pos = -1\n",
    "                    best_split.score = NEG_INF\n",
    "\n",
    "                    # Ensure feats drawn w/out replacement.\n",
    "                    n_drawn_feats = 0\n",
    "                    n_new_consts = 0\n",
    "                    n_total_consts = n_consts\n",
    "                    lb = 0                      # Range in `features` array from which we \n",
    "                    ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "                        \n",
    "                    while n_drawn_feats < self.m:\n",
    "                        n_drawn_feats += 1\n",
    "\n",
    "                        # Breiman & Cutler's original Fortran random forests implementation \n",
    "                        # allows for known constant features to be drawn during a split-search.\n",
    "                        # I follow their example, as I believe that doing so allows individual \n",
    "                        # trees to be less correlated with each other. Since I don't pre-sort\n",
    "                        # features, I would prefer not to have to sort any more features than\n",
    "                        # necessary, and so I've adopted the technique Sklearn uses to track \n",
    "                        # constant features:\n",
    "                        #     https://github.com/scikit-learn/scikit-learn/blob/dbe39454f766ebefc3219f2c1871ac1774316532/sklearn/tree/_splitter.pyx#L310\n",
    "                        # \n",
    "                        # The idea is that feature idxs in `features` are organized into two sections:\n",
    "                        #\n",
    "                        #     [<indices of known constant feats>, <indices of non-constant feats>]\n",
    "                        #\n",
    "                        # As we begin drawing feature indices from this above list, those two sections\n",
    "                        # will each be further sub-divided into two sections:\n",
    "                        # \n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <undrawn non-constant feats>, <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # When we choose a feature that happens to be a known constant, we'll re-locate\n",
    "                        # its idx to the right-end of the first of those four sections. Then we \n",
    "                        # increment the lower bound threshold, `lb`, by one so that we don't re-draw \n",
    "                        # that feature again.\n",
    "                        #\n",
    "                        # Similarly, if we draw a non-constant feature idx, we'll move it to the \n",
    "                        # left-end of the last of the four partitions and reduce the upper bound\n",
    "                        # threshold, `ub`, by one so that the feature idx can't be drawn again\n",
    "                        # during this split-search. \n",
    "                        #\n",
    "                        # One last important detail: sometimes we'll draw a feature that \n",
    "                        # used to be non-constant for ancestor nodes, but will be found to be \n",
    "                        # constant for the current node. When this happens, we relocate its \n",
    "                        # index so that it sits to the right of the known constant feats section.\n",
    "                        # This means our `features` list could have up to five partitions:\n",
    "                        #\n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <newly discovered const feats>, <undrawn non-constant feats>, \n",
    "                        #      <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # Whenever we find a new constant feature, we increment the `n_new_consts`\n",
    "                        # counter by one. We also increment the `n_total_consts` counter by one. \n",
    "                        # During the split-search we have to use `n_total_consts` to keep track of\n",
    "                        # the total number of constant features. n_consts` mustn't be changed\n",
    "                        # because it tells us where the <newly discovered const feats> section\n",
    "                        # of the `features` list begins.\n",
    "\n",
    "                        # One last wrinkle. We subtract the # of newly discovered const feats from  \n",
    "                        # the upper bound before we select an index `i` from the `features` array, \n",
    "                        # and add it back to `i` after `i` has been genereated. This prevents us from \n",
    "                        # re-drawing any of these new const feats again during this split-search.\n",
    "                        dist = uniform_int_distribution[SIZE_t](lb, ub-n_new_consts)\n",
    "                        idx = dist(self.rng)\n",
    "\n",
    "                        # So that we don't draw a known constant feature again this split-search.\n",
    "                        if idx < n_consts:\n",
    "                            features[idx], features[lb] = features[lb], features[idx]\n",
    "                            lb += 1 \n",
    "                            continue\n",
    "\n",
    "                        # So that no new const feats get drawn more than once per split-search.\n",
    "                        idx += n_new_consts\n",
    "\n",
    "                        feat_idx = features[idx]\n",
    "                        # Prepare the rows' feature values for sorting.\n",
    "                        for r in range(start, end):\n",
    "                            # X is a pointer, so have to index into this 2d array in the C way \n",
    "                            # (also keeping in mind that the array is column-major).\n",
    "                            items[r] = X[feat_idx*self.n_samples + self.rows[r]]\n",
    "\n",
    "                        # Sort feature values and corresponding sample row indices\n",
    "                        # to prepare for numerical split finding.\n",
    "                        dual_introsort(items, self.rows, start, end)\n",
    "\n",
    "                        # Make sure the feature not constant for node's samples.\n",
    "                        if items[start] == items[end-1]:\n",
    "                            # Move the newly-discovered constant feat to the far right-end\n",
    "                            # of the left half of `features` list holding the known const\n",
    "                            # feats as well as any other const feats newly discovered \n",
    "                            # during this node's split-search.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Initialize weighted class counts of right and left children.\n",
    "                            # Right child's counts are initially the same as parent node's.\n",
    "                            memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                            memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "\n",
    "                            # If the feature has an impurity score that's better than the best score \n",
    "                            # found among all other features visited thus far for this node, find_num_split()\n",
    "                            # updates the attributes of the struct containing the node's best split info. \n",
    "                            # \n",
    "                            # But even if a new best score isn't reached, if an impurity score can\n",
    "                            # be calculated at least once during the feature's split search, the\n",
    "                            # following indicator will be toggled off, to indicate that the feature\n",
    "                            # is not constant.\n",
    "                            current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                            find_num_split(self.rows, items, y, start, n_samples_node, \n",
    "                                           self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                           self.class_weights, l_wcc, r_wcc, node_wcc,\n",
    "                                           &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc,\n",
    "                                           &current_feat_const)\n",
    "\n",
    "                            # The feature may be constant only within the search range \n",
    "                            # permitted by self.min_samples_leaf and self.min_weight_leaf. \n",
    "                            # If so, the feature is a newly discovered constant.\n",
    "                            if current_feat_const:\n",
    "                                features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                                n_new_consts += 1\n",
    "                                n_total_consts += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                # The feature is non-constant, so we ensure it's not drawn again\n",
    "                                # during this split-search.\n",
    "                                features[idx], features[ub] = features[ub], features[idx]\n",
    "                                ub -= 1 \n",
    "\n",
    "                    # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                    memcpy(&features[0], &constant_features[0], sizeof(SIZE_t)*n_consts)\n",
    "                    memcpy(&constant_features[n_consts], &features[n_consts], sizeof(SIZE_t)*n_new_consts)\n",
    "\n",
    "                    # Make node a leaf if constant for all randomly drawn feats.\n",
    "                    # (# drawn known constant feats + # drawn new constant feats)\n",
    "                    if lb + n_new_consts == n_drawn_feats: \n",
    "                        self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                    else: \n",
    "                        make_num_split(self.rows, X, &node_info, &best_split, self.n_samples) \n",
    "\n",
    "                        # Update tree info for node that's getting split.\n",
    "                        l_child_id = self.n_nodes\n",
    "                        r_child_id = l_child_id + 1\n",
    "                        node.l_child = l_child_id\n",
    "                        node.r_child = r_child_id\n",
    "                        node.feat    = best_split.feat\n",
    "                        node.thresh  = best_split.thresh\n",
    "                        node.label   = -1\n",
    "\n",
    "                        # Prepare for the left and right child nodes\n",
    "                        # by increasing tree data memory capacity if\n",
    "                        # necessary.\n",
    "                        if self.n_nodes + 2 > self.mem_capacity:\n",
    "                            # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                            # part of WhozCraig's SO answer at: \n",
    "                            #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                            # Add one after squaring so that the new capacity can\n",
    "                            # contain not only a tree of greater depth, but also\n",
    "                            # the maximum # nodes that that depth could have.\n",
    "                            new_capacity = 2*self.mem_capacity + 1\n",
    "                            self._increase_mem_capacity(new_capacity)\n",
    "                            self.mem_capacity = new_capacity\n",
    "                        \n",
    "                        # Push right child info onto the LIFO stack.\n",
    "                        node_stack.push({\"start\": best_split.pos, \"end\": end, \"node_id\": r_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "                        # Push left child info onto queue.\n",
    "                        node_stack.push({\"start\": start, \"end\": best_split.pos, \"node_id\": l_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "\n",
    "                        # And update size of the tree.\n",
    "                        self.n_nodes += 2\n",
    "                        \n",
    "        free(l_wcc)\n",
    "        free(r_wcc)\n",
    "        free(node_wcc)\n",
    "        free(max_wt_classes)\n",
    "    \n",
    "    def fit(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] y, \n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] rows, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] features,\n",
    "            np.ndarray[DTYPE_t, ndim=1, mode=\"c\"] class_weights, SIZE_t n_class): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "            class_weights (1D C-contiguous array of float64): Desired weight for each class. Shape: (`n_class`,).\n",
    "            n_class                                         : Number of classes in training data.  \n",
    "        \"\"\"\n",
    "        # Casting the raw data to pointers gives a 17% speed-up compared to getting\n",
    "        # pointer from the ndarray's buffer interface as recommended by DavidW in \n",
    "        # his SO answer at: https://stackoverflow.com/a/54832269/8628758. e.g.\n",
    "        #     cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        #     cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        # Not worried about unexpected behavior as all ndarrays' contiguousness and\n",
    "        # memory layout enforced prior to this point.\n",
    "        cdef DTYPE_t* X_ptr = <DTYPE_t*> X.data\n",
    "        cdef SIZE_t* y_ptr = <SIZE_t*> y.data\n",
    "        self.rows = <SIZE_t*> rows.data\n",
    "        self.features = <SIZE_t*> features.data\n",
    "        self.class_weights = <DTYPE_t*> class_weights.data\n",
    "        self.n_class = n_class\n",
    "        self.n_samples = rows.shape[0]\n",
    "        self.n_features = features.shape[0]\n",
    "        cdef random_device rd # Needed when using the C++ mt19937 rng w/out a seed.\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        cdef SIZE_t init_capacity = 15\n",
    "        \n",
    "        cdef SIZE_t i, row, label\n",
    "        cdef DTYPE_t wt\n",
    "        cdef DTYPE_t sum_wts = 0\n",
    "        cdef Node* root_node = NULL\n",
    "        with nogil:\n",
    "            # Allocate memory for the tree.\n",
    "            self._increase_mem_capacity(init_capacity)\n",
    "            self.mem_capacity = init_capacity\n",
    " \n",
    "            # And sum the class weights of all the root node's samples in\n",
    "            # order to know minimum total weight a leaf must have (which\n",
    "            # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "            for i in range(self.n_samples):\n",
    "                row = self.rows[i]\n",
    "                label = y_ptr[row]\n",
    "                wt = self.class_weights[label]\n",
    "                sum_wts += wt\n",
    "            self.min_weight_leaf = self.min_weight_fraction_leaf*sum_wts\n",
    "            \n",
    "            # Initialize the random number generator. Followed example from:\n",
    "            #     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/tests/run/cpp_stl_random.pyx#L16\n",
    "            if self.seed == -1:\n",
    "                self.rng = mt19937(rd()) # If using the random device engine std::random_device.\n",
    "            else:\n",
    "                self.rng = mt19937(self.seed)\n",
    "\n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X_ptr, y_ptr)\n",
    "    \n",
    "    cdef Node* _next_node(self, SIZE_t nxt) nogil: \n",
    "        return &self.nodes[nxt]\n",
    "    \n",
    "    cdef SIZE_t _get_leaf_idx(self, SIZE_t i, Node* leaf, SIZE_t n, DTYPE_t* X) nogil:\n",
    "        cdef SIZE_t idx\n",
    "        cdef SIZE_t root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[leaf.feat*n + i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else: \n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "    \n",
    "    def predict(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_preds = X.shape[0]\n",
    "        cdef SIZE_t i\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        cdef SIZE_t[::1] preds_view = preds\n",
    "        cdef Node leaf\n",
    "        with nogil:\n",
    "            for i in range(n_preds): \n",
    "                preds_view[i] = self.nodes[self._get_leaf_idx(i, &leaf, n_preds, X_ptr)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "            \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions. Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        cdef DTYPE_t[::1] wcc_view = wcc\n",
    "        cdef Node leaf\n",
    "        cdef SIZE_t i, j, idx\n",
    "        with nogil:\n",
    "            for i in range(n_probs):\n",
    "                idx = self._get_leaf_idx(i, &leaf, n_probs, X_ptr)\n",
    "                for j in range(self.n_class):\n",
    "                    wcc_view[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)\n",
    "\n",
    "class DecisionTreeLouppeCython():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first algorithm.\n",
    "    \n",
    "    Based on page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    Keeps track of and avoids features that are constant for a given node's samples.\n",
    "\n",
    "    Attributes:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            seed                         (int): Seed of the random number generator used for tree growing.\n",
    "            rows              (ndarray of int): Row indices of all training samples. Shape: (n training samples,).\n",
    "            features          (ndarray of int): Column indices of all training features. Shape: (n features,).\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            n_class                      (int): Number of unique classes in the training set.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights = [], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                         (int): Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C') \n",
    "        if seed is None:\n",
    "            self.seed = -1\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        self._tree = _DecisionTree(self.m, self.min_samples_leaf, self.min_weight_fraction_leaf, self.seed)\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self._tree.size\n",
    "    \n",
    "    @property\n",
    "    def left_children(self): return self._tree.left_children\n",
    "    \n",
    "    @property\n",
    "    def right_children(self): return self._tree.right_children\n",
    "            \n",
    "    @property \n",
    "    def split_features(self): return self._tree.split_features\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): return self._tree.split_thresholds\n",
    "    \n",
    "    @property\n",
    "    def weighted_class_counts(self): return self._tree.weighted_cc\n",
    "    \n",
    "    @property\n",
    "    def labels(self): return self._tree.labels\n",
    "    \n",
    "    def fit(self, X, y, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X (Fortran-style ndarray of float64): Pre-processed training data. \n",
    "                                                  Shape: (num train samples, num train features).\n",
    "            y                   (ndarray of int): Training labels. Shape: (num train samples,).\n",
    "            rows                          (list): Indices of the rows to be used for training. \n",
    "                                                  All rows used if empty.\n",
    "            features                      (list): Column indices of training features that will be used.\n",
    "                                                  All features used if empty.                          \n",
    "        Returns:\n",
    "            DecisionTreeLouppeCython: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        self.n_class = np.unique(y).size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "            \n",
    "        self._tree.fit(X, y, self.rows, self.features, self.class_weights, self.n_class)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict_probs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Louppe Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeLouppeCython(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Tree is different cause Cython uses C++ rng, not Numpy rng as Python version does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "538 Âµs Â± 22.7 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.54 Âµs Â± 216 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Wright's Numerical Split Finders\n",
    "To mimic what Ranger does, I'll need to write two split finder functions -- one to for when we split \"small Q\" nodes, and the other for splitting \"large Q\" nodes (where the ratio of the number of samples in the node to the number of unique values for a given feature in the training set is larger 0.02). I'll start by writing the \"small Q\" node split finder. \n",
    "\n",
    "Now I could just use the Sklearn numerical split finder to do this, but Marvin Wright designed Ranger to sort and explore split points using a different technique than Sklearn, and I think it'll be interesting to summarize how he approached the problem of cataloging and then iterating through numerical split points. Here's the process he used:\n",
    "1. Get the sorted unique raw feature values held by the node's samples. \n",
    "2. The number of possible split points will ultimately be the length of the above result, minus one.\n",
    "3. If the above result is one or less, the samples in the node are constant for the given feature, and the next feature can be investigated.\n",
    "4. Create a 1-d array storing weighted sample class counts per split-point (for each unique value of the feature), and use a for-loop to iterate over all samples in the node to tabulate these counts.\n",
    "5. At each split-point, we use one more for-loop to update the weighted class counts of the left and right children. A side-effect of shifting multiple samples (having the same raw feature value and class label) from the right to left child at the same time is that we won't be able to use an on-line algorithm to update the numerators used to calculate the proxy gini score. Instead, at each split point we'll have to use the updated left and right weighted class counts to calculate the right and left children's proxy gini numerators from scratch.\n",
    "\n",
    "Conceptually-speaking, I find Wright's approach to investigating numerical split-points to be more elegant and intuitive than what Louppe does with Sklearn:\n",
    "* No need to dual sort raw feature values and row indices.\n",
    "* By only iterating over unique split points, we don't have to keep track of two values (\"recent\" and \"next-most-recent\") to cover situations where we iterate over a batch of samples that all have the same feature value.\n",
    "* Easy to determine whether node is constant for the given feature early-on before iterating over any samples.\n",
    "* No having to specify what to do in the corner case where the last few samples all have the same value.\n",
    "\n",
    "Unfortunately, I expect that this simplicity comes at the cost of some speed:\n",
    "* We have to iterate over all rows to compile the per-split class counts, and then we have to iterate over all unique split-points. It's conceivable that oftentimes this will amount to roughly twice as many iterations as Louppe's method requires (in Sklearn, you just iterate over each sample, once.)\n",
    "* Can't on-line update numerators of proxy gini scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Small Q splits in `memory_saving_splitting` mode\n",
    "If you were to comb through Ranger's source code, you'd notice that its small Q splitter [has an option](https://github.com/imbs-hl/ranger/blob/ce497711884c783e133fb36750b60de4c140773f/src/TreeClassification.cpp#L221) for what Wright calls `memory_saving_splitting` mode. This setting controls the length of the `split_class_counts` list that's used whenever a split search happens. When the setting is active, the small Q splitter creates the list of split-point class counts from scratch for each new candidate splitting feature. The list's length is no longer than the <# unique feature values of the node's samples> x <# of classes>.\n",
    "\n",
    "If `memory_saving_splitting` mode is not engaged, before training begins Ranger [will allocate memory](https://github.com/imbs-hl/ranger/blob/ce497711884c783e133fb36750b60de4c140773f/src/TreeClassification.cpp#L37) for the creation of a single 1-d array that's of length <largest # unique values of any numerical feature in the training set> x <# classes>. This same list will be used to compile the split-point class counts for each and every split search, for all of a decision tree's nodes. Whenever a new candidate feature is drawn, the splitter will zero out the first several positions of this list such that the class counts for each unique raw feature value found among a given node's samples can be tabulated from scratch.\n",
    "\n",
    "I surmise that the advantage of using the same set of memory addresses for each split search is that it avoids the overhead that's necessary to allocate memory for brand new class count lists over and over, for each candidate feature at each node that's to be split. Since my goal in this notebook is to find the fastest numerical splitting algorithm, my smallQ splitter implemention below *will not* use memory-saving mode by default, and will allocate memory for arrays used by the splitter only once at the beginning of training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright Small Node \"Small Q\" numerical split finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_split_smallQ(X, rows, node_unique_vals_feat, labels, node_start, n_parent, n_class, \n",
    "                          min_samples_leaf, min_weight_leaf, c_wts, l_wcc, r_wcc,\n",
    "                          parent_wcc, best_split, current_feat, parent_num, parent_den,\n",
    "                          node_n_unique_vals_feat, split_counts_raw, split_class_counts_wt):\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's SmallQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L233\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        X                     (ndarray of float64): Training data. Shape: (n train samples, n features).\n",
    "        rows                      (ndarray of int): Indices of all rows in the training set. \n",
    "                                                    Shape: (n train samples,).\n",
    "        node_unique_vals_feat (ndarray of float64): The sorted unique feature values of the samples in the parent\n",
    "                                                    node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                    (ndarray of int): All training labels. Shape: (n training samples,).\n",
    "        node_start                           (int): Index of the beginning of the parent node in `rows`.\n",
    "        n_parent                             (int): Number of samples in the parent node.\n",
    "        n_class                              (int): Number of unique classes in the training set.\n",
    "        min_samples_leaf                     (int): Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf                  (float64): Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                 (ndarray of float64): Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                 (ndarray of float64): Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                 (ndarray of float64): Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc            (ndarray of float64): Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split                         (Split): Holds the feature, threshold, and impurity\n",
    "                                                    score of the parent node's current best split.\n",
    "        current_feat                         (int): Column index of feature under investigation.\n",
    "        parent_num                       (float64): Numerator of parent node's impurity score.\n",
    "        parent_den                       (float64): Denominator of parent node's impurity score.\n",
    "        node_n_unique_vals_feat              (int): Number of unique values for one feature found among \n",
    "                                                    the node's samples.\n",
    "        split_counts_raw          (ndarray of int): Stores sample counts found at each unique split point of\n",
    "                                                    a given feature in a given node. \n",
    "                                                    Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt (ndarray of float64): Stores weighted class counts of each class at each unique\n",
    "                                                    split point of a given feature in a given node. Shape:\n",
    "                                                    (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "              \n",
    "    Returns: \n",
    "        int: 1 if feature is constant for eligible split-points. 0, otherwise.\n",
    "    \"\"\"\n",
    "    # Whether or not feat is constant within search range permitted\n",
    "    # by min_samples_leaf and min_weight_leaf (0 if no, 1 if yes).\n",
    "    current_feat_const = 1\n",
    "    \n",
    "    # Tabulate both the sample counts at all possible split points\n",
    "    # as well as weighted class counts at each split point.\n",
    "    split_counts_raw[:node_n_unique_vals_feat] = 0\n",
    "    split_class_counts_wt[:node_n_unique_vals_feat*n_class] = 0.\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        value = X[row][current_feat]\n",
    "        label = labels[row]\n",
    "        split_point_idx = np.searchsorted(node_unique_vals_feat, value, side='left', sorter=None)\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += c_wts[label] \n",
    "        \n",
    "    # To keep track of num samples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(node_n_unique_vals_feat - 1):\n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "        \n",
    "        l_num, r_num = 0., 0. # To calculate numerators of proxy gini scores.\n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            l_wcc[j] += split_class_counts_wt[i*n_class + j]\n",
    "            r_wcc[j] -= split_class_counts_wt[i*n_class + j]\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            l_den += split_class_counts_wt[i*n_class + j]\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return current_feat_const\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return current_feat_const\n",
    "          \n",
    "        current_feat_const = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = node_unique_vals_feat[i]/2. + node_unique_vals_feat[i+1]/2. \n",
    "            if mid == node_unique_vals_feat[i+1]: mid = node_unique_vals_feat[i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "    return current_feat_const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_split(rows, X, node_info, best_split):\n",
    "    \"\"\"Split a decision tree node using a given ordered numerical feature and threshold. \n",
    "    \n",
    "    Uses the similar logic as Gilles Louppe's Cython implementation at: \n",
    "        https://github.com/scikit-learn/scikit-learn/blob/47e3358712d483a8e8dcb84d87386eb4f3d49070/sklearn/tree/_splitter.pyx#L605\n",
    "    \n",
    "    Arguments: \n",
    "        rows  (ndarray of int): Indices of all rows in the training set. \n",
    "                                Shape: (n train samples,).\n",
    "        X (ndarray of float64): The training data. Shape: (n train samples, n features).\n",
    "        node_info (StackEntry): Stats of the node to be split.\n",
    "        best_split     (Split): Stats of a node split.\n",
    "    \n",
    "    Returns: \n",
    "        Position of first item in split's right child node.\n",
    "    \"\"\"\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[rows[p]][best_split.feat] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright SmallQ Decision Tree Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For all Wright splitting algorithms (SmallQ, LargeQ, hybrids), \n",
    "# no need to store best split position in Split() struct.\n",
    "# Will be returned by make_num_split() instead.\n",
    "class Split():\n",
    "    \"\"\"Pertinent stats needed to compare node splits.\n",
    "    \n",
    "    Attributes:\n",
    "        feat       (int): Column index of splitting feature.\n",
    "        thresh (float64): Split threshold.\n",
    "        score  (float64): Impurity score.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat, thresh, score):\n",
    "        self.feat, self.thresh, self.score = feat, thresh, score\n",
    "\n",
    "class DecisionTreeSmallQ():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Marvin Wright's SmallQ numerical splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L233\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights=[], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                         (int): Use when reproducibility desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C')\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = np.empty(0, dtype=Node, order='C')\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = np.empty(0, dtype=np.float64, order='C')\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self.n_nodes\n",
    "    \n",
    "    @property \n",
    "    def left_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].l_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def right_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].r_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_features(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].feat\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): \n",
    "        out = np.empty(self.n_nodes, dtype='float64')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].thresh\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def weighted_cc(self):\n",
    "        out_size = self.n_nodes*self.n_class\n",
    "        out = np.empty(out_size, dtype='float64')\n",
    "        for i in range(out_size):\n",
    "            out[i] = self.weighted_class_counts[i]\n",
    "        out.resize(self.n_nodes, self.n_class)\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def labels(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].label\n",
    "        return out\n",
    "    \n",
    "    def _increase_mem_capacity(self, new_capacity):\n",
    "        \"\"\"Resize ndarrays that hold tree's nodes and weighted class counts.\n",
    "        \n",
    "        Arguments:\n",
    "            new_capacity (int): Amount of nodes that resized arrays will be able to hold.\n",
    "        \"\"\"\n",
    "        self.nodes.resize(new_capacity, refcheck=False)\n",
    "        self.weighted_class_counts.resize(new_capacity*self.n_class, refcheck=False)\n",
    "    \n",
    "    def _make_leaf(self, node_id, wcc, n_classes_node):\n",
    "        \"\"\"Set and store the class label of a leaf node.\n",
    "        \n",
    "        Break ties at random when multiple classes share the same max weight.\n",
    "        Doing this avoids a bias towards lower classes that would be a possible\n",
    "        consequence of using np.argmax (which is what Sklearn does).\n",
    "        \n",
    "        Arguments:\n",
    "            node_id            (int): Location of node in `self.nodes`.\n",
    "            wcc (ndarray of float64): Node's weighted class counts. Shape: (`self.n_class`,).\n",
    "            n_classes_node     (int): Number of unique class labels found among\n",
    "                                      node's training samples.\n",
    "        \"\"\"\n",
    "        if n_classes_node == 1: \n",
    "            label = max(enumerate(wcc), key=lambda f: f[1])[0]\n",
    "        else:              \n",
    "            label = self._rng.choice(np.argwhere(wcc==np.max(wcc)).flatten())\n",
    "        self.nodes[node_id] = Node(-1, -1, -1, np.nan, label) \n",
    "        \n",
    "    def _grow_tree(self, X, y):\n",
    "        \"\"\"Depth-first growth of a decision tree.\n",
    "        \n",
    "        Arguments:\n",
    "            X (ndarray of float64): Training samples. Shape: (n samples, n features).\n",
    "            y     (ndarray of int): Training labels. Shape: (n samples,).\n",
    "        \"\"\"\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        node_stack = []\n",
    "        \n",
    "        # Stores the weighted class counts of the current node.\n",
    "        node_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        ##############################################################\n",
    "        # For finding the best split.\n",
    "        ##############################################################\n",
    "        l_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        r_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        items = np.empty(self.n_samples, dtype=np.float64)\n",
    "        \n",
    "        # Make 1-d arrays containing sample counts and weighted class \n",
    "        # counts for each unique raw feature value.\n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        split_counts_raw = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        # Weighted class counts for each split-point.\n",
    "        split_class_counts_wt = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64)  \n",
    "\n",
    "        # Keeping track of nodes' constant features. \n",
    "        features = self.features.copy()\n",
    "        constant_features = np.empty(self.n_features, dtype=np.intp)\n",
    "        \n",
    "        # Push root node onto the LIFO stack.\n",
    "        node_stack.append(StackEntry(0, self.n_samples, 0, 0, 0))\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "        while len(node_stack) > 0:\n",
    "            node_info = node_stack.pop()\n",
    "            start, end = node_info.start, node_info.end\n",
    "            node_id, parent_id = node_info.node_id, node_info.parent_id\n",
    "            n_consts = node_info.n_const_feats\n",
    "            n_samples_node = end-start\n",
    "            \n",
    "            # Tabulate and store the current node's weighted class counts.\n",
    "            node_wcc[:] = 0.\n",
    "            for i in range(n_samples_node):\n",
    "                row = self.rows[start + i]\n",
    "                label = y[row]\n",
    "                wt = self.class_weights[label]\n",
    "                node_wcc[label] += wt \n",
    "            self.weighted_class_counts[node_id*self.n_class: (node_id + 1)* self.n_class] = node_wcc\n",
    "            \n",
    "            # Make a leaf if required to do so.\n",
    "            n_classes_node, sum_node_wcc, sum_node_wcc_sqr = 0, 0., 0.\n",
    "            for c in range(self.n_class):\n",
    "                wcc = node_wcc[c]\n",
    "                if wcc > 0: n_classes_node += 1\n",
    "                # Compute the current node's proxy gini numerator and denominator while we're at it.\n",
    "                sum_node_wcc_sqr += wcc**2 \n",
    "                sum_node_wcc += wcc \n",
    "            if n_classes_node == 1:                      \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            \n",
    "            # Or perform a split.\n",
    "            else:\n",
    "                # Initialize stats for best split of node.\n",
    "                best_split = Split(-1, 0., -np.inf)\n",
    "                \n",
    "                # Ensure feats drawn w/out replacement.\n",
    "                n_drawn_feats = 0\n",
    "                n_new_consts = 0\n",
    "                n_total_consts = n_consts\n",
    "                lb = 0                      # Range in `features` array from which we \n",
    "                ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "               \n",
    "                while n_drawn_feats < self.m:\n",
    "                    n_drawn_feats += 1\n",
    "                    idx = self._rng.choice(range(lb, ub-n_new_consts+1))\n",
    "                    \n",
    "                    # So that we don't draw a known constant feature again this split-search.\n",
    "                    if idx < n_consts:\n",
    "                        features[idx], features[lb] = features[lb], features[idx]\n",
    "                        lb += 1 \n",
    "                        continue\n",
    "                        \n",
    "                    # So that no new const feats get drawn more than once per split-search.\n",
    "                    idx += n_new_consts\n",
    "\n",
    "                    feat_idx = features[idx]  \n",
    "                    \n",
    "                    # Prepare the rows' feature values for sorting.\n",
    "                    items[:n_samples_node] = X[:,feat_idx][self.rows[start:end]]\n",
    "                    \n",
    "                    # Make sure the feature not constant for node's samples.\n",
    "                    node_unique_vals_feat = np.unique(items[:n_samples_node])\n",
    "                    node_n_unique_vals_feat = len(node_unique_vals_feat)\n",
    "                    if node_n_unique_vals_feat < 2:\n",
    "                        # Move the newly-discovered constant feat to the far right-end\n",
    "                        # of the left half of `features` list holding the known const\n",
    "                        # feats as well as any other const feats newly discovered \n",
    "                        # during this node's split-search.\n",
    "                        features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                        n_new_consts += 1\n",
    "                        n_total_consts += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # Initialize weighted class counts of right and left children.\n",
    "                        # Right child's counts are initially the same as parent node's.\n",
    "                        r_wcc[:] = node_wcc\n",
    "                        l_wcc[:] = 0.\n",
    "                    \n",
    "                        # If the feature has an impurity score that's better than the best score \n",
    "                        # found among all other features visited thus far for this node, find_num_split()\n",
    "                        # updates the attributes of the struct containing the node's best split info. \n",
    "                        # \n",
    "                        # But even if a new best score isn't reached, if an impurity score can\n",
    "                        # be calculated at least once during the feature's split search, the\n",
    "                        # following indicator will be toggled off, to indicate that the feature\n",
    "                        # is not constant (1 = is constant; 0 = not constant).\n",
    "                        current_feat_const = find_num_split_smallQ(X, self.rows, node_unique_vals_feat, y, start, n_samples_node, \n",
    "                                                                   self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                                   self.class_weights, l_wcc, r_wcc, node_wcc, best_split, feat_idx, \n",
    "                                                                   sum_node_wcc_sqr, sum_node_wcc, node_n_unique_vals_feat, split_counts_raw, \n",
    "                                                                   split_class_counts_wt)\n",
    "\n",
    "                        if current_feat_const:\n",
    "                            # The feature may be constant within the search range permitted\n",
    "                            # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                            # the feature is a newly discovered constant.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The feature is non-constant, so we ensure it's not drawn again\n",
    "                            # during this split-search.\n",
    "                            features[idx], features[ub] = features[ub], features[idx]\n",
    "                            ub -= 1 \n",
    "                            \n",
    "                # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                features[0:n_consts] = constant_features[0:n_consts]\n",
    "                constant_features[n_consts:n_consts+n_new_consts] = features[n_consts:n_consts+n_new_consts]\n",
    "                \n",
    "                # Make node a leaf if constant for all randomly drawn feats.\n",
    "                # (# drawn known constant feats + # drawn new constant feats)\n",
    "                if lb + n_new_consts == n_drawn_feats: \n",
    "                    self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "                else: \n",
    "                    split_pos = make_num_split(self.rows, X, node_info, best_split) \n",
    "\n",
    "                    # Update info for node that's getting split.\n",
    "                    l_child_id = self.n_nodes\n",
    "                    r_child_id = l_child_id + 1\n",
    "                    self.nodes[node_id] = Node(l_child_id, r_child_id, best_split.feat, best_split.thresh, -1)\n",
    "\n",
    "                    # Prepare for the left and right child nodes\n",
    "                    # by increasing tree data memory capacity if\n",
    "                    # necessary.\n",
    "                    if self.n_nodes + 2 > self.mem_capacity:\n",
    "                        # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                        # part of WhozCraig's SO answer at: \n",
    "                        #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                        # Add one after squaring so that the new capacity can\n",
    "                        # contain not only a tree of greater depth, but also\n",
    "                        # the maximum # nodes that that depth could have.\n",
    "                        new_capacity = 2*self.mem_capacity + 1\n",
    "                        self._increase_mem_capacity(new_capacity)\n",
    "                        self.mem_capacity = new_capacity\n",
    "                    \n",
    "                    # Push right child info onto the LIFO stack.\n",
    "                    node_stack.append(StackEntry(split_pos, end, r_child_id, node_id, n_total_consts))\n",
    "                    # Push left child info onto queue.\n",
    "                    node_stack.append(StackEntry(start, split_pos, l_child_id, node_id, n_total_consts))\n",
    "\n",
    "                    # And update size of the tree.\n",
    "                    self.n_nodes += 2\n",
    "    \n",
    "    def fit(self, X, y, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X (Fortran-style ndarray of float64): Pre-processed training data. \n",
    "                                                  Shape: (num train samples, num train features).\n",
    "            y                   (ndarray of int): Training labels. Shape: (num train samples,).\n",
    "            rows                          (list): Indices of the rows to be used for training. \n",
    "                                                  All rows used if empty.\n",
    "            features                      (list): Column indices of training features that will be used.\n",
    "                                                  All features used if empty.                           \n",
    "        Returns:\n",
    "            DecisionTreeSmallQ: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        # Determine # classes found among all training samples.\n",
    "        root_cc = np.unique(y, return_counts=True)[1] \n",
    "        self.n_class = root_cc.size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "\n",
    "        self.n_samples = len(self.rows)\n",
    "        self.n_features = len(self.features)\n",
    "        \n",
    "        # Get the max cardinality of all numerical feats.\n",
    "        self.max_n_unique_feat_vals = max([np.unique(X[:,i]).size for i in range(X.shape[1])])\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        init_capacity = 15\n",
    "        \n",
    "         # Allocate tree memory.\n",
    "        self._increase_mem_capacity(init_capacity)\n",
    "        self.mem_capacity = init_capacity\n",
    "        \n",
    "        # And sum the class weights of all the root node's samples in\n",
    "        # order to know minimum total weight a leaf must have (which\n",
    "        # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "        root_wcc = root_cc*self.class_weights\n",
    "        self.min_weight_leaf = self.min_weight_fraction_leaf*root_wcc.sum()\n",
    "        \n",
    "        # Initialize the random number generator.\n",
    "        self._rng = get_random_generator(self.seed)\n",
    "        \n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X, y)\n",
    "        return self\n",
    "        \n",
    "    def _next_node(self, nxt): return self.nodes[nxt]\n",
    "       \n",
    "    def _get_leaf_idx(self, i, X):\n",
    "        root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[:,leaf.feat][i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else:\n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        n_preds = X.shape[0]\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        for i in range(n_preds):\n",
    "            preds[i] = self.nodes[self._get_leaf_idx(i, X)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "       \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        for i in range(n_probs):\n",
    "            idx = self._get_leaf_idx(i, X)\n",
    "            for j in range(self.n_class):\n",
    "                wcc[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wright SmallQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeSmallQ(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Number of nodes in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752808988764045"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151 ms Â± 1.36 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "517 Âµs Â± 13.8 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright SmallQ Decision Tree Cython Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "# distutils: language = c++\n",
    "# distutils: extra_compile_args = -std=c++11\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t # Signed, same as ssize_t in C. See MSeifert's SO answer: https://stackoverflow.com/a/46416257/8628758\n",
    "cimport cython\n",
    "from libc.math cimport log as ln\n",
    "from libc.stdlib cimport realloc, free\n",
    "from libc.string cimport memcpy\n",
    "from libc.string cimport memset\n",
    "from libcpp.stack cimport stack\n",
    "\n",
    "# For C++ random number generation.\n",
    "from libc.stdint cimport uint_fast32_t \n",
    "\n",
    "# Swap helper func for sorting.\n",
    "cdef inline void swap(DTYPE_t* items, SIZE_t i, SIZE_t j) nogil:\n",
    "    items[i], items[j] = items[j], items[i]\n",
    "\n",
    "# Quicksort helpers\n",
    "\n",
    "cdef inline void med_three(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find the median-of-three pivot point of the second through final \n",
    "    items of a list of numbers. Once identified, the pivot is moved to \n",
    "    the front of the list. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t middle = <int>(first + (last - first)/2)\n",
    "    cdef SIZE_t second = first + 1\n",
    "    last -= 1\n",
    "    if items[second] < items[middle]:\n",
    "        if items[middle] < items[last]:\n",
    "            swap(items, first, middle)    \n",
    "        elif items[second] < items[last]:\n",
    "            swap(items, first, last)         \n",
    "        else:                        \n",
    "            swap(items, first, second)\n",
    "    elif items[second] < items[last]:\n",
    "        swap(items, first, second)\n",
    "    elif items[middle] < items[last]:\n",
    "        swap(items, first, last)\n",
    "    else:\n",
    "        swap(items, first, middle)\n",
    "\n",
    "cdef inline SIZE_t partition(DTYPE_t* items, SIZE_t first, SIZE_t last, SIZE_t pivot) nogil:\n",
    "    \"\"\"Group numbers less than the pivot value together on the left and\n",
    "    those that are greater on the right. Find the index that separates\n",
    "    these two groups, which will belong to the first item that is greater\n",
    "    than or equal to the pivot. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1885\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        pivot      : Index holding the median pivot value.\n",
    "        \n",
    "    Returns:\n",
    "        Index of cut point used to partition the items into two smaller sequences.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while first < last and items[first] < items[pivot]:\n",
    "            first += 1                      # Get index of first item greater than or equal to median-of-three pivot. \n",
    "        last -= 1\n",
    "        while items[pivot] < items[last]:\n",
    "            last -= 1                       # Get index of last item less than or equal to the pivot.\n",
    "        if not (first < last): \n",
    "            return first                    # After swaps are done, return index of first item in right partition.\n",
    "        \n",
    "        swap(items, first, last)            # Swap the first item greater than or equal to the pivot with the\n",
    "                                            # last item less than or equal to the pivot. \n",
    "        first += 1\n",
    "\n",
    "cdef inline void insertion_sort(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Follows the spirit of the Numpy implementation at: \n",
    "        https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t i\n",
    "    cdef SIZE_t j\n",
    "    cdef SIZE_t k\n",
    "    cdef DTYPE_t val\n",
    "    for i in range(first+1, last):\n",
    "        j = i\n",
    "        k = i - 1\n",
    "        val = items[i]\n",
    "        while (j > first) and val < items[k]:\n",
    "            items[j] = items[k]\n",
    "            j-=1\n",
    "            k-=1\n",
    "        items[j] = val\n",
    "\n",
    "# Heapsort\n",
    "\n",
    "cdef inline void sift_down(DTYPE_t* items, SIZE_t start, int n, SIZE_t p, \n",
    "                           SIZE_t c, DTYPE_t val) nogil:\n",
    "    \"\"\"Swap a heap item with one of its children if that child's value is \n",
    "    greater than or equal to that parent's value. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L61\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "        p    : Index of the parent.\n",
    "        c    : Index of the parent's first (left) child.\n",
    "        val  : The parent's value.\n",
    "    \"\"\"\n",
    "    while c < n:    # Look at the descendents of current parent, `p`.\n",
    "        if c < n-1 and items[start + c] < items[start + c + 1]: # Find larger of the first and second children.\n",
    "            c += 1\n",
    "        if val < items[start + c]: # If child greater than parent, swap child and parent.\n",
    "            items[start + p] = items[start + c]\n",
    "            p = c   # Current greater child becomes the parent.\n",
    "            c += c  # Look at this child's child, if it exists.\n",
    "        else:\n",
    "            break \n",
    "    items[start + p] = val\n",
    "\n",
    "cdef inline void sort_heap(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Sort a binary max heap of numbers. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L77\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        val = items[start + n]\n",
    "        items[start + n] = items[start]\n",
    "        sift_down(items, start, n, 0, 1, val)\n",
    "\n",
    "cdef inline void heapify(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Turn a list of items into a binary max heap. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L59\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t p\n",
    "    cdef SIZE_t last_p = (n-2)//2\n",
    "    for p in range(last_p, -1, -1):\n",
    "        val = items[start + p] # value of last parent\n",
    "        sift_down(items, start, n, p, 2*p + 1, val)\n",
    "\n",
    "cdef inline void heapsort(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Applies the heapsort algorithm to sort a list of items from least to greatest. \n",
    "    From Williams, 1964.\n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    heapify(items, start, n)\n",
    "    sort_heap(items, start, n)\n",
    "    \n",
    "# Introsort \n",
    "\n",
    "cdef void introsort_loop(DTYPE_t* items, SIZE_t first, SIZE_t last, int depth) nogil:\n",
    "    \"\"\"The recursive heart of the introsort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        depth      : Current recursion depth.\n",
    "    \"\"\"\n",
    "    cdef int MIN_SIZE_THRESH = 16\n",
    "    cdef SIZE_t cut\n",
    "    while last-first > MIN_SIZE_THRESH:\n",
    "        if depth == 0:\n",
    "            heapsort(items, first, last-first)\n",
    "        depth -= 1\n",
    "        med_three(items, first, last)\n",
    "        cut = partition(items, first+1, last, first)\n",
    "        introsort_loop(items, cut, last, depth)\n",
    "        last = cut\n",
    "\n",
    "# Log base-2 helper function. From Sklearn's implementation at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_utils.pyx#L7\n",
    "cdef inline DTYPE_t log2(DTYPE_t x) nogil:\n",
    "    return ln(x) / ln(2.0)\n",
    "\n",
    "cdef void introsort(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Implementation as described in Musser, 1997. Switches to heapsort\n",
    "    when max recursion depth exceeded. Otherwise uses median-of-three \n",
    "    quicksort (Bentley & McIlroy, 1993) with all the usual optimizations:\n",
    "        - Swap equal elements.\n",
    "        - Only process partitions longer than the minimum size threshold.\n",
    "        - When a new partition is made, recurse on the smaller half and \n",
    "          iterate over the larger half.\n",
    "        - Make a final pass with insertion sort over the entire list.\n",
    "\n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef int max_depth = 2 * <int>log2(last-first)\n",
    "    introsort_loop(items, first, last, max_depth)\n",
    "    insertion_sort(items, first, last)\n",
    "    \n",
    "cdef SIZE_t sort_unique(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Sort a 1-d array of numbers in-place using introsort and \n",
    "    place the unique values in consecutive ascending order at \n",
    "    the beginning of the array.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        \n",
    "    Returns: \n",
    "        Number of unique items.\n",
    "    \"\"\"\n",
    "    cdef SIZE_t i = 1\n",
    "    cdef SIZE_t j = 1\n",
    "    introsort(items, first, last)\n",
    "    while i < last-first:\n",
    "        if items[i] == items[i-1]:\n",
    "            i += 1\n",
    "        else:\n",
    "            if i - j < 1:\n",
    "                j += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                items[j] = items[i]\n",
    "                j += 1\n",
    "                i += 1\n",
    "    return j\n",
    "    \n",
    "# For convenient memory reallocation.\n",
    "ctypedef fused realloc_t:\n",
    "    SIZE_t\n",
    "    DTYPE_t\n",
    "    Node\n",
    "\n",
    "cdef inline realloc_t* safe_realloc(realloc_t* ptr, SIZE_t n_items) nogil except *:\n",
    "    # Inspired by Sklearn's safe_realloc() func. However, thankfully\n",
    "    # Cython now no longer requires us to send a pointer to a pointer\n",
    "    # in order to prevent crashes.\n",
    "    cdef realloc_t elem = ptr[0]\n",
    "    cdef SIZE_t n_bytes = n_items * sizeof(elem)\n",
    "    # Make sure we're not trying to allocate too much memory.\n",
    "    if n_bytes/sizeof(elem) != n_items:\n",
    "        with gil:\n",
    "            raise MemoryError(f\"Overflow error: unable to allocate {n_bytes} bytes.\")       \n",
    "    cdef realloc_t* res_ptr = <realloc_t *> realloc(ptr, n_bytes)\n",
    "    with gil:\n",
    "        if not res_ptr: raise MemoryError()\n",
    "    return res_ptr\n",
    "\n",
    "# C++ random number generator. Not yet a part of a Cython release so\n",
    "# pasted in from: \n",
    "#     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/Cython/Includes/libcpp/random.pxd#L1\n",
    "cdef extern from \"<random>\" namespace \"std\" nogil:\n",
    "    cdef cppclass random_device:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        random_device() except +\n",
    "        result_type operator()() except +\n",
    "\n",
    "    cdef cppclass mt19937:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        mt19937() except +\n",
    "        mt19937(result_type seed) except +\n",
    "        result_type operator()() except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        void discard(size_t z) except +\n",
    "        void seed(result_type seed) except +\n",
    "\n",
    "    cdef cppclass uniform_int_distribution[T]:\n",
    "        ctypedef T result_type\n",
    "        uniform_int_distribution() except +\n",
    "        uniform_int_distribution(T, T) except +\n",
    "        result_type operator()[Generator](Generator&) except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        \n",
    "# Info for any node that will eventually be split or made into a leaf.\n",
    "# Similar to what Sklearn does at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/a2c4d8b1f4471f52a4fcf1026f495e637a472568/sklearn/tree/_tree.pyx#L126\n",
    "cdef struct StackEntry:\n",
    "    SIZE_t start\n",
    "    SIZE_t end\n",
    "    SIZE_t node_id\n",
    "    SIZE_t parent_id\n",
    "    SIZE_t n_const_feats\n",
    "\n",
    "# To compare node splits.\n",
    "cdef struct Split:\n",
    "    SIZE_t feat\n",
    "    DTYPE_t thresh\n",
    "    DTYPE_t score  \n",
    "\n",
    "# Vital characteristics of a node. Set when it's added to the tree.\n",
    "cdef struct Node:\n",
    "    SIZE_t l_child # idx of left child, -1 if leaf\n",
    "    SIZE_t r_child # idx of right child, -1 if leaf\n",
    "    SIZE_t feat    # col idx of split feature, -1 if leaf\n",
    "    DTYPE_t thresh # double split threshold, NAN if leaf\n",
    "    SIZE_t label   # class label if leaf, -1 if non-leaf.\n",
    "    \n",
    "cdef inline SIZE_t find_first(DTYPE_t* items, DTYPE_t value, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find first occurrence of an element in a vector of sorted \n",
    "       (ascending order) elements.\n",
    "       \n",
    "    Uses same algorithm as Python's bisect_left() function:\n",
    "        https://github.com/python/cpython/blob/8fd2d36c1c6da78b2402fcb8bcefdad8428c8bc3/Lib/bisect.py#L68\n",
    "        \n",
    "    Arguments:\n",
    "        items      : The pre-sorted elements to be searched over.\n",
    "        value      : The value to search for.\n",
    "        first, last: The range of items to be searched.\n",
    "        \n",
    "    Returns:\n",
    "        Index of the first element in `items` that equals `value`.\n",
    "        \n",
    "        If no such element exists in `items` the returned index\n",
    "        merely indicates where the element would reside where it\n",
    "        present in the sorted vector.\n",
    "    \"\"\"\n",
    "    cdef SIZE_t mid\n",
    "    while first < last:\n",
    "        mid = (first + last) // 2\n",
    "        if items[mid] < value:\n",
    "            first = mid + 1\n",
    "        else:\n",
    "            last = mid\n",
    "    return first\n",
    "\n",
    "cdef inline void find_num_split_smallQ(DTYPE_t* X, SIZE_t* rows, DTYPE_t* node_unique_vals_feat, \n",
    "                                       SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, \n",
    "                                       SIZE_t n_samples, SIZE_t n_class, SIZE_t min_samples_leaf, \n",
    "                                       DTYPE_t min_weight_leaf, DTYPE_t* c_wts, DTYPE_t* l_wcc, \n",
    "                                       DTYPE_t* r_wcc, DTYPE_t* parent_wcc, Split* best_split, \n",
    "                                       SIZE_t current_feat, DTYPE_t parent_num, DTYPE_t parent_den, \n",
    "                                       SIZE_t node_n_unique_vals_feat, SIZE_t* split_counts_raw, \n",
    "                                       DTYPE_t* split_class_counts_wt, bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's SmallQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L233\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        X                      : Training data. Shape: (n train samples, n features).\n",
    "        rows                   : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        node_unique_vals_feat  : The sorted unique feature values of the samples in the parent\n",
    "                                 node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                 : All training labels. Shape: (n training samples,).\n",
    "        node_start             : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent               : Number of samples in the parent node.\n",
    "        n_samples              : Number of samples in the training data.\n",
    "        n_class                : Number of unique classes in the training set.\n",
    "        min_samples_leaf       : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf        : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                  : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                  : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                  : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc             : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split             : Holds the feature, threshold and impurity\n",
    "                                 score of the parent node's current best split.\n",
    "        current_feat           : Column index of feature under investigation.\n",
    "        parent_num             : Numerator of parent node's impurity score.\n",
    "        parent_den             : Denominator of parent node's impurity score.\n",
    "        node_n_unique_vals_feat: Number of unique values for one feature found among \n",
    "                                 the node's samples.\n",
    "        split_counts_raw       : Stores sample counts found at each unique split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt  : Stores weighted class counts of each class at each unique\n",
    "                                 split point of a given feature in a given node. Shape:\n",
    "                                 (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "        current_feat_const     : Whether current splitting feature is constant for all eligible split \n",
    "                                 thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used while tabulating sample and weighted\n",
    "    # class counts at all unique split points.\n",
    "    cdef SIZE_t row, label, split_point_idx\n",
    "    cdef DTYPE_t value\n",
    "    \n",
    "    # Variables to track progress during the split search.\n",
    "    cdef SIZE_t n_left, n_right, i, j\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, wt, score, mid\n",
    "    \n",
    "    # Tabulate both the sample counts at all possible split points\n",
    "    # as well as weighted class counts at each split point.\n",
    "    memset(&split_counts_raw[0], 0, sizeof(SIZE_t)*node_n_unique_vals_feat)\n",
    "    memset(&split_class_counts_wt[0], 0, sizeof(DTYPE_t)*node_n_unique_vals_feat*n_class)\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        value = X[current_feat*n_samples + row]\n",
    "        label = labels[row]\n",
    "        split_point_idx = find_first(node_unique_vals_feat, value, 0, node_n_unique_vals_feat)\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += c_wts[label] \n",
    "    \n",
    "    # To keep track of num amples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(node_n_unique_vals_feat - 1):\n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "\n",
    "        l_num, r_num = 0., 0. # To calculate numerators of proxy gini scores.\n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            l_den += wt\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = node_unique_vals_feat[i]/2. + node_unique_vals_feat[i+1]/2. \n",
    "            if mid == node_unique_vals_feat[i+1]: mid = node_unique_vals_feat[i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "\n",
    "cdef inline SIZE_t make_num_split(SIZE_t* rows, DTYPE_t* X, StackEntry* node_info, Split* best_split, \n",
    "                                SIZE_t n_samples) nogil:\n",
    "    cdef SIZE_t p, p_end\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[best_split.feat*n_samples + rows[p]] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p] \n",
    "    return p\n",
    "\n",
    "# Necessary constants.\n",
    "cdef DTYPE_t NEG_INF = -np.inf\n",
    "cdef DTYPE_t NAN = np.nan\n",
    "            \n",
    "cdef class _DecisionTree:\n",
    "    # Class attributes.\n",
    "    cdef SIZE_t seed\n",
    "    cdef mt19937 rng\n",
    "    cdef SIZE_t mem_capacity\n",
    "    cdef SIZE_t n_samples\n",
    "    cdef SIZE_t n_features\n",
    "    cdef SIZE_t n_class\n",
    "    cdef SIZE_t max_n_unique_feat_vals\n",
    "    cdef SIZE_t m\n",
    "    cdef SIZE_t min_samples_leaf, \n",
    "    cdef DTYPE_t min_weight_fraction_leaf\n",
    "    cdef DTYPE_t min_weight_leaf\n",
    "    cdef SIZE_t n_nodes\n",
    "    cdef SIZE_t* rows\n",
    "    cdef SIZE_t* features\n",
    "    cdef DTYPE_t* class_weights\n",
    "    cdef Node* nodes\n",
    "    cdef DTYPE_t* weighted_class_counts\n",
    "    def __cinit__(self, SIZE_t m, SIZE_t min_samples_leaf, DTYPE_t min_weight_fraction_leaf, SIZE_t seed): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                       : Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                      of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                    : A seed for the C++ mt19937 32bit int random generator. \n",
    "                                      Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m, self.min_samples_leaf = m, min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = NULL\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = NULL \n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        free(self.nodes)\n",
    "        free(self.weighted_class_counts)\n",
    "        \n",
    "    property size:\n",
    "        def __get__(self):\n",
    "            return self.n_nodes\n",
    "    \n",
    "    property left_children:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].l_child\n",
    "            return out\n",
    "\n",
    "    property right_children:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].r_child\n",
    "            return out\n",
    "        \n",
    "    property split_features: \n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].feat\n",
    "            return out\n",
    "        \n",
    "    property split_thresholds:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].thresh\n",
    "            return out\n",
    "        \n",
    "    property weighted_cc:\n",
    "        def __get__(self):\n",
    "            cdef SIZE_t out_size = self.n_nodes*self.n_class\n",
    "            out = np.empty(out_size, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(out_size):\n",
    "                    out_view[i] = self.weighted_class_counts[i]\n",
    "            out.resize(self.n_nodes, self.n_class)\n",
    "            return out\n",
    "    \n",
    "    property labels:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].label\n",
    "            return out\n",
    "    \n",
    "    cdef void _increase_mem_capacity(self, SIZE_t new_capacity) nogil:\n",
    "        self.nodes = safe_realloc(self.nodes, new_capacity)\n",
    "        self.weighted_class_counts = safe_realloc(self.weighted_class_counts, self.n_class*new_capacity)\n",
    "    \n",
    "    cdef void _make_leaf(self, Node* leaf_node, SIZE_t* y, SIZE_t node_start, SIZE_t node_id, \n",
    "                         SIZE_t n_classes_node, SIZE_t* max_wt_classes) nogil:\n",
    "        # Class with largest wcc becomes leaf node's label. Break ties with a random choice.\n",
    "        cdef SIZE_t label\n",
    "        cdef DTYPE_t max_wt = 0.\n",
    "        cdef SIZE_t lb = 0\n",
    "        cdef SIZE_t ub = -1\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t i, j\n",
    "        # If all node's samples have the same class.\n",
    "        if n_classes_node == 1:\n",
    "            label = y[self.rows[node_start]]\n",
    "        else:\n",
    "            # Otherwise find label with max weighted class count for the node.\n",
    "            for i in range(self.n_class):\n",
    "                max_wt = max(max_wt, self.weighted_class_counts[node_id*self.n_class + i])\n",
    "            # See if multiple classes share this max count.\n",
    "            for i in range(self.n_class):\n",
    "                if self.weighted_class_counts[node_id*self.n_class + i] == max_wt:\n",
    "                    ub += 1\n",
    "                    max_wt_classes[ub] = i\n",
    "            # If so, randomly choose leaf's label from among those classes.\n",
    "            if ub > 0:\n",
    "                dist = uniform_int_distribution[SIZE_t](lb, ub) # Choose an int w/in range lb, ub, inclusive.\n",
    "                j = dist(self.rng)\n",
    "                label = max_wt_classes[j]\n",
    "            else:\n",
    "                label = max_wt_classes[lb]\n",
    "        leaf_node.l_child = -1\n",
    "        leaf_node.r_child = -1    \n",
    "        leaf_node.feat = -1  \n",
    "        leaf_node.thresh = NAN\n",
    "        leaf_node.label = label \n",
    "\n",
    "    cdef _grow_tree(self, DTYPE_t* X, SIZE_t* y):\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        cdef stack[StackEntry] node_stack\n",
    "\n",
    "        #####################################################################\n",
    "        # Variables containing info of the node currently being investigated.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t start, end, node_id, parent_id, n_consts, n_samples_node\n",
    "        cdef DTYPE_t* node_wcc = NULL\n",
    "        cdef StackEntry node_info\n",
    "        cdef Node* node = NULL\n",
    "        \n",
    "        # Holds child node info if the current node gets split.\n",
    "        cdef SIZE_t l_child_id, r_child_id\n",
    "        cdef Node* l_child_node = NULL\n",
    "        cdef Node* r_child_node = NULL\n",
    "        \n",
    "        #####################################################################\n",
    "        # For finding the best split.\n",
    "        #####################################################################\n",
    "        cdef Split best_split\n",
    "        cdef DTYPE_t* l_wcc = NULL\n",
    "        cdef DTYPE_t* r_wcc = NULL\n",
    "        cdef DTYPE_t sum_node_wcc_sqr, sum_node_wcc # Parent node's proxy Gini score num and den.\n",
    "        cdef SIZE_t split_pos\n",
    "        \n",
    "        # Indicates a feature has been discovered to be constant during a\n",
    "        # split search within the search range permitted by min_samples_leaf \n",
    "        # and min_weight_leaf.\n",
    "        cdef bint current_feat_const \n",
    "\n",
    "        # Create a C-contiguous array of doubles to hold feature values of a \n",
    "        # given node's samples. Using Numpy to allocate memory to longer \n",
    "        # vectors is often faster than using realloc().\n",
    "        cdef DTYPE_t[::1] items_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* items = &items_buffer[0]\n",
    "        cdef SIZE_t r\n",
    "        \n",
    "        # An array to contain unique split points for a feature at a given node.\n",
    "        cdef DTYPE_t[::1] node_unique_vals_feat_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* node_unique_vals_feat = &node_unique_vals_feat_buffer[0]\n",
    "        cdef SIZE_t node_n_unique_vals_feat\n",
    "        \n",
    "        # Make 1-d arrays containing sample counts and weighted class \n",
    "        # counts for each unique raw feature value.\n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        cdef SIZE_t[::1] split_counts_raw_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        cdef SIZE_t* split_counts_raw = &split_counts_raw_buffer[0]\n",
    "        # Weighted class counts for each split-point.\n",
    "        cdef DTYPE_t[::1] split_class_counts_wt_buffer = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64) \n",
    "        cdef DTYPE_t* split_class_counts_wt = &split_class_counts_wt_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For random feature selection (w/out replacement) and keeping track \n",
    "        # of nodes' constant features. \n",
    "        #####################################################################\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t lb, ub, idx, feat_idx, n_drawn_feats, n_new_consts, n_total_consts\n",
    "        cdef SIZE_t[::1] features_buffer = np.empty(self.n_features, dtype=np.intp) \n",
    "        cdef SIZE_t* features = &features_buffer[0]\n",
    "        cdef SIZE_t[::1] constant_features_buffer = np.empty(self.n_features, dtype=np.intp)\n",
    "        cdef SIZE_t* constant_features = &constant_features_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For determining whether node should be a leaf.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t i, c, cc, n_classes_node, row, label\n",
    "        cdef DTYPE_t wcc, wt\n",
    "        # Stores classes that share a leaf's max class wt. When two or more \n",
    "        # present, leaf label randomly chosen from these classes\n",
    "        cdef SIZE_t* max_wt_classes = NULL\n",
    "        \n",
    "        with nogil:\n",
    "            # Allocate memory to pointers.\n",
    "            l_wcc = safe_realloc(l_wcc, self.n_class)\n",
    "            r_wcc = safe_realloc(r_wcc, self.n_class)\n",
    "            node_wcc = safe_realloc(node_wcc, self.n_class)\n",
    "            max_wt_classes = safe_realloc(max_wt_classes, self.n_class*sizeof(SIZE_t))\n",
    "            # Fill with feature column indices so we can track constant feats.\n",
    "            memcpy(features, self.features, self.n_features* sizeof(SIZE_t))\n",
    "            \n",
    "            # Push root node onto the LIFO stack.\n",
    "            node_stack.push({\"start\": 0, \"end\": self.n_samples, \"node_id\": 0, \n",
    "                             \"parent_id\": 0, \"n_const_feats\": 0})\n",
    "            self.n_nodes = 1\n",
    "            while not node_stack.empty():\n",
    "                node_info = node_stack.top()\n",
    "                node_stack.pop()\n",
    "                start, end = node_info.start, node_info.end\n",
    "                node_id, parent_id = node_info.node_id, node_info.parent_id # TODO: `parent_id` unused; is it necessary?\n",
    "                n_consts = node_info.n_const_feats\n",
    "                n_samples_node = end-start\n",
    "                node = &self.nodes[node_id]\n",
    "                \n",
    "                # Tabulate the current node's weighted class counts.\n",
    "                #\n",
    "                # Implementation detail #1: I tried storing the l and r child wt class cts\n",
    "                # of nodes' best splits so that this tabulation wouldn't need to be \n",
    "                # performed for each node. But found there was virtually no speed improvement\n",
    "                # to justify the more complicated code required to store and update these \n",
    "                # values during the best split search.\n",
    "                #\n",
    "                # Implementation detail #2: Setting aside a block of memory to \n",
    "                # store the current node's wt class cts and passing a pointer to\n",
    "                # this block to the split search function sped up training by 8%\n",
    "                # compared to passing a ptr to the location of node's wt class cts \n",
    "                # in the self.weighted_class_counts array.\n",
    "                memset(node_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                sum_node_wcc, sum_node_wcc_sqr = 0., 0.\n",
    "                for i in range(n_samples_node):\n",
    "                    row = self.rows[start + i]\n",
    "                    label = y[row]\n",
    "                    wt = self.class_weights[label]\n",
    "                    # Compute the node's proxy gini numerator and denominator while we're at it.\n",
    "                    sum_node_wcc_sqr += wt*(2*node_wcc[label] + wt) # numerator\n",
    "                    sum_node_wcc += wt                              # denominator\n",
    "                    node_wcc[label] += wt\n",
    "                memcpy(&self.weighted_class_counts[node_id*self.n_class], node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                \n",
    "                # Make a leaf if required to do so. \n",
    "                n_classes_node = 0\n",
    "                for c in range(self.n_class):\n",
    "                    wcc = node_wcc[c]\n",
    "                    if wcc > 0: n_classes_node += 1\n",
    "                if n_classes_node == 1:                   \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "\n",
    "                # Otherwise split the node.\n",
    "                else:\n",
    "                    # Initialize stats for best split of node.\n",
    "                    best_split.feat = -1\n",
    "                    best_split.thresh = 0.\n",
    "                    best_split.score = NEG_INF\n",
    "\n",
    "                    # Ensure feats drawn w/out replacement.\n",
    "                    n_drawn_feats = 0\n",
    "                    n_new_consts = 0\n",
    "                    n_total_consts = n_consts\n",
    "                    lb = 0                      # Range in `features` array from which we \n",
    "                    ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "                        \n",
    "                    while n_drawn_feats < self.m:\n",
    "                        n_drawn_feats += 1\n",
    "\n",
    "                        # Breiman & Cutler's original Fortran random forests implementation \n",
    "                        # allows for known constant features to be drawn during a split-search.\n",
    "                        # I follow their example, as I believe that doing so allows individual \n",
    "                        # trees to be less correlated with each other. Since I don't pre-sort\n",
    "                        # features, I would prefer not to have to sort any more features than\n",
    "                        # necessary, and so I've adopted the technique Sklearn uses to track \n",
    "                        # constant features:\n",
    "                        #     https://github.com/scikit-learn/scikit-learn/blob/dbe39454f766ebefc3219f2c1871ac1774316532/sklearn/tree/_splitter.pyx#L310\n",
    "                        # \n",
    "                        # The idea is that feature idxs in `features` are organized into two sections:\n",
    "                        #\n",
    "                        #     [<indices of known constant feats>, <indices of non-constant feats>]\n",
    "                        #\n",
    "                        # As we begin drawing feature indices from this above list, those two sections\n",
    "                        # will each be further sub-divided into two sections:\n",
    "                        # \n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <undrawn non-constant feats>, <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # When we choose a feature that happens to be a known constant, we'll re-locate\n",
    "                        # its idx to the right-end of the first of those four sections. Then we \n",
    "                        # increment the lower bound threshold, `lb`, by one so that we don't re-draw \n",
    "                        # that feature again.\n",
    "                        #\n",
    "                        # Similarly, if we draw a non-constant feature idx, we'll move it to the \n",
    "                        # left-end of the last of the four partitions and reduce the upper bound\n",
    "                        # threshold, `ub`, by one so that the feature idx can't be drawn again\n",
    "                        # during this split-search. \n",
    "                        #\n",
    "                        # One last important detail: sometimes we'll draw a feature that \n",
    "                        # used to be non-constant for ancestor nodes, but will be found to be \n",
    "                        # constant for the current node. When this happens, we relocate its \n",
    "                        # index so that it sits to the right of the known constant feats section.\n",
    "                        # This means our `features` list could have up to five partitions:\n",
    "                        #\n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <newly discovered const feats>, <undrawn non-constant feats>, \n",
    "                        #      <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # Whenever we find a new constant feature, we increment the `n_new_consts`\n",
    "                        # counter by one. We also increment the `n_total_consts` counter by one. \n",
    "                        # During the split-search we have to use `n_total_consts` to keep track of\n",
    "                        # the total number of constant features. n_consts` mustn't be changed\n",
    "                        # because it tells us where the <newly discovered const feats> section\n",
    "                        # of the `features` list begins.\n",
    "\n",
    "                        # One last wrinkle. We subtract the # of newly discovered const feats from  \n",
    "                        # the upper bound before we select an index `i` from the `features` array, \n",
    "                        # and add it back to `i` after `i` has been genereated. This prevents us from \n",
    "                        # re-drawing any of these new const feats again during this split-search.\n",
    "                        dist = uniform_int_distribution[SIZE_t](lb, ub-n_new_consts)\n",
    "                        idx = dist(self.rng)\n",
    "\n",
    "                        # So that we don't draw a known constant feature again this split-search.\n",
    "                        if idx < n_consts:\n",
    "                            features[idx], features[lb] = features[lb], features[idx]\n",
    "                            lb += 1 \n",
    "                            continue\n",
    "\n",
    "                        # So that no new const feats get drawn more than once per split-search.\n",
    "                        idx += n_new_consts\n",
    "\n",
    "                        feat_idx = features[idx]\n",
    "                        # Place all samples' feat values into contiguous storage.\n",
    "                        for r in range(n_samples_node):\n",
    "                            # X is a pointer, so have to index into this 2d array in the C way \n",
    "                            # (also keeping in mind that the array is column-major).\n",
    "                            items[r] = X[feat_idx*self.n_samples + self.rows[start + r]]\n",
    "\n",
    "                        # Place all unique split points, in ascending order, inside the \n",
    "                        # first <node_n_unique_vals_feat> indices of the items array.\n",
    "                        node_n_unique_vals_feat = sort_unique(items, 0, n_samples_node)\n",
    "\n",
    "                        # Make sure the feature not constant for node's samples.\n",
    "                        if node_n_unique_vals_feat < 2:\n",
    "                            # Move the newly-discovered constant feat to the far right-end\n",
    "                            # of the left half of `features` list holding the known const\n",
    "                            # feats as well as any other const feats newly discovered \n",
    "                            # during this node's split-search.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Initialize weighted class counts of right and left children.\n",
    "                            # Right child's counts are initially the same as parent node's.\n",
    "                            memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                            memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "\n",
    "                            # If the feature has an impurity score that's better than the best score \n",
    "                            # found among all other features visited thus far for this node, find_num_split()\n",
    "                            # updates the attributes of the struct containing the node's best split info. \n",
    "                            # \n",
    "                            # But even if a new best score isn't reached, if an impurity score can\n",
    "                            # be calculated at least once during the feature's split search, the\n",
    "                            # following indicator will be toggled off, to indicate that the feature\n",
    "                            # is not constant.\n",
    "                            current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                            find_num_split_smallQ(X, self.rows, items, y, start, n_samples_node, self.n_samples, \n",
    "                                                  self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                  self.class_weights, l_wcc, r_wcc, node_wcc,\n",
    "                                                  &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc,\n",
    "                                                  node_n_unique_vals_feat, split_counts_raw, split_class_counts_wt,\n",
    "                                                  &current_feat_const)\n",
    "\n",
    "                            if current_feat_const:\n",
    "                                # The feature may be constant within the search range permitted\n",
    "                                # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                                # the feature is a newly discovered constant.\n",
    "                                features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                                n_new_consts += 1\n",
    "                                n_total_consts += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                # The feature is non-constant, so we ensure it's not drawn again\n",
    "                                # during this split-search.\n",
    "                                features[idx], features[ub] = features[ub], features[idx]\n",
    "                                ub -= 1 \n",
    "\n",
    "                    # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                    memcpy(&features[0], &constant_features[0], sizeof(SIZE_t)*n_consts)\n",
    "                    memcpy(&constant_features[n_consts], &features[n_consts], sizeof(SIZE_t)*n_new_consts)\n",
    "\n",
    "                    # Make node a leaf if constant for all randomly drawn feats.\n",
    "                    # (# drawn known constant feats + # drawn new constant feats)\n",
    "                    if lb + n_new_consts == n_drawn_feats: \n",
    "                        self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                    else: \n",
    "                        split_pos = make_num_split(self.rows, X, &node_info, &best_split, self.n_samples) \n",
    "\n",
    "                        # Update tree info for node that's getting split.\n",
    "                        l_child_id = self.n_nodes\n",
    "                        r_child_id = l_child_id + 1\n",
    "                        node.l_child = l_child_id\n",
    "                        node.r_child = r_child_id\n",
    "                        node.feat    = best_split.feat\n",
    "                        node.thresh  = best_split.thresh\n",
    "                        node.label   = -1\n",
    "\n",
    "                        # Prepare for the left and right child nodes\n",
    "                        # by increasing tree data memory capacity if\n",
    "                        # necessary.\n",
    "                        if self.n_nodes + 2 > self.mem_capacity:\n",
    "                            # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                            # part of WhozCraig's SO answer at: \n",
    "                            #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                            # Add one after squaring so that the new capacity can\n",
    "                            # contain not only a tree of greater depth, but also\n",
    "                            # the maximum # nodes that that depth could have.\n",
    "                            new_capacity = 2*self.mem_capacity + 1\n",
    "                            self._increase_mem_capacity(new_capacity)\n",
    "                            self.mem_capacity = new_capacity\n",
    "                        \n",
    "                        # Push right child info onto the LIFO stack.\n",
    "                        node_stack.push({\"start\": split_pos, \"end\": end, \"node_id\": r_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "                        # Push left child info onto queue.\n",
    "                        node_stack.push({\"start\": start, \"end\": split_pos, \"node_id\": l_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "\n",
    "                        # And update size of the tree.\n",
    "                        self.n_nodes += 2\n",
    "                        \n",
    "        free(l_wcc)\n",
    "        free(r_wcc)\n",
    "        free(node_wcc)\n",
    "        free(max_wt_classes)\n",
    "    \n",
    "    def fit(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] y, \n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] rows, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] features,\n",
    "            np.ndarray[DTYPE_t, ndim=1, mode=\"c\"] class_weights, SIZE_t n_class): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "            class_weights (1D C-contiguous array of float64): Desired weight for each class. Shape: (`n_class`,).\n",
    "            n_class                                         : Number of classes in training data.   \n",
    "        \"\"\"\n",
    "        # Casting the raw data to pointers gives a 17% speed-up compared to getting\n",
    "        # pointer from the ndarray's buffer interface, as recommended by DavidW in \n",
    "        # his SO answer at: https://stackoverflow.com/a/54832269/8628758. e.g.\n",
    "        #     cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        #     cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        # Not worried about unexpected behavior as all ndarrays' contiguousness and\n",
    "        # memory layout enforced prior to this point.\n",
    "        cdef DTYPE_t* X_ptr = <DTYPE_t*> X.data\n",
    "        cdef SIZE_t* y_ptr = <SIZE_t*> y.data\n",
    "        self.rows = <SIZE_t*> rows.data\n",
    "        self.features = <SIZE_t*> features.data\n",
    "        self.class_weights = <DTYPE_t*> class_weights.data\n",
    "        self.n_class = n_class\n",
    "        self.n_samples = rows.shape[0]\n",
    "        self.n_features = features.shape[0]\n",
    "        cdef random_device rd # Needed when using the C++ mt19937 rng w/out a seed.\n",
    "        \n",
    "        # Get the max cardinality of all numerical feats.\n",
    "        self.max_n_unique_feat_vals = max([np.unique(X[:,i]).size for i in range(X.shape[1])])\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        cdef SIZE_t init_capacity = 15\n",
    "        \n",
    "        cdef SIZE_t i, row, label\n",
    "        cdef DTYPE_t wt\n",
    "        cdef DTYPE_t sum_wts = 0\n",
    "        cdef Node* root_node = NULL\n",
    "        with nogil:\n",
    "            # Allocate memory for the tree.\n",
    "            self._increase_mem_capacity(init_capacity)\n",
    "            self.mem_capacity = init_capacity\n",
    " \n",
    "            # And sum the class weights of all the root node's samples in\n",
    "            # order to know minimum total weight a leaf must have (which\n",
    "            # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "            for i in range(self.n_samples):\n",
    "                row = self.rows[i]\n",
    "                label = y_ptr[row]\n",
    "                wt = self.class_weights[label]\n",
    "                sum_wts += wt\n",
    "            self.min_weight_leaf = self.min_weight_fraction_leaf*sum_wts\n",
    "            \n",
    "            # Initialize the random number generator. Followed example from:\n",
    "            #     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/tests/run/cpp_stl_random.pyx#L16\n",
    "            if self.seed == -1:\n",
    "                self.rng = mt19937(rd()) # If using the random device engine std::random_device.\n",
    "            else:\n",
    "                self.rng = mt19937(self.seed)\n",
    "\n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X_ptr, y_ptr)\n",
    "    \n",
    "    cdef Node* _next_node(self, SIZE_t nxt) nogil: \n",
    "        return &self.nodes[nxt]\n",
    "    \n",
    "    cdef SIZE_t _get_leaf_idx(self, SIZE_t i, Node* leaf, SIZE_t n, DTYPE_t* X) nogil:\n",
    "        cdef SIZE_t idx\n",
    "        cdef SIZE_t root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[leaf.feat*n + i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else: \n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "    \n",
    "    def predict(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_preds = X.shape[0]\n",
    "        cdef SIZE_t i\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        cdef SIZE_t[::1] preds_view = preds\n",
    "        cdef Node leaf\n",
    "        with nogil:\n",
    "            for i in range(n_preds): \n",
    "                preds_view[i] = self.nodes[self._get_leaf_idx(i, &leaf, n_preds, X_ptr)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions. Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        cdef DTYPE_t[::1] wcc_view = wcc\n",
    "        cdef Node leaf\n",
    "        cdef SIZE_t i, j, idx\n",
    "        with nogil:\n",
    "            for i in range(n_probs):\n",
    "                idx = self._get_leaf_idx(i, &leaf, n_probs, X_ptr)\n",
    "                for j in range(self.n_class):\n",
    "                    wcc_view[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)\n",
    "\n",
    "class DecisionTreeSmallQCython():\n",
    "    \"\"\"Fit a decision tree using a depth-first algorithm.\n",
    "    \n",
    "    Uses Marvin Wright's SmallQ numerical splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L233\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights = [], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                         (int): Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C') \n",
    "        if seed is None:\n",
    "            self.seed = -1\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        self._tree = _DecisionTree(self.m, self.min_samples_leaf, self.min_weight_fraction_leaf, self.seed)\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self._tree.size\n",
    "    \n",
    "    @property\n",
    "    def left_children(self): return self._tree.left_children\n",
    "    \n",
    "    @property\n",
    "    def right_children(self): return self._tree.right_children\n",
    "            \n",
    "    @property \n",
    "    def split_features(self): return self._tree.split_features\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): return self._tree.split_thresholds\n",
    "    \n",
    "    @property\n",
    "    def weighted_class_counts(self): return self._tree.weighted_cc\n",
    "    \n",
    "    @property\n",
    "    def labels(self): return self._tree.labels\n",
    "    \n",
    "    def fit(self, X, y, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X (Fortran-style ndarray of float64): Pre-processed training data. \n",
    "                                                  Shape: (num train samples, num train features).\n",
    "            y                   (ndarray of int): Training labels. Shape: (num train samples,).\n",
    "            rows                          (list): Indices of the rows to be used for training. \n",
    "                                                  All rows used if empty.\n",
    "            features                      (list): Column indices of training features that will be used.\n",
    "                                                  All features used if empty.                          \n",
    "        Returns:\n",
    "            DecisionTreeSmallQCython: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        self.n_class = np.unique(y).size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "            \n",
    "        self._tree.fit(X, y, self.rows, self.features, self.class_weights, self.n_class)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict_probs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright SmallQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeSmallQCython(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.03 ms Â± 16.6 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc) #beat 989us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.27 Âµs Â± 82.6 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, and likely for the reasons described above (possibly twice as much iteration required), the Wright's Small Q splitter runs nearly 100% slower than Sklearn's numerical splitter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright Large Node \"Large Q\" numerical split finder\n",
    "The \"large Q\" split finder will look quite similar to the \"small Q\" split finder. The only difference is that when searching for the best split, instead of iterating through the sorted, unique raw feature values of just *the node's samples*, we will use the pre-sorted data that holds the unique values of *all training samples*. And so, most of our work here will be to write some pre-processing helper functions that sort the training data's numerical features before training begins.\n",
    "\n",
    "Here's how Wright designed Ranger's pre-sorting to work:\n",
    "1. Create new arrays that contain all the unique values for each numerical feature in the training data, and then sort these arrays in ascending order. Once sorted, these unique values lists will represent all possible split points whenever a particular feature is evaluated to see whether or not it should be used to split a node.\n",
    "2. Create a look-up table of size <# training rows> x <# numerical features>. For each numerical feature, each row will contain the index at which that row's raw feature value can be found in the appropriate unique values list.\n",
    "\n",
    "In order to iterate through the split-points of a particular numerical feature, the \"large Q\" numerical splitter will use the look-up table to build a somewhat longer version of `split_class_counts_wt` list that was constructed by the \"small Q\" splitter. This time around, the per-split class counts list will be long enough to contain all of a feature's split-points (the unique values) that are found inside the entire training set. \n",
    "\n",
    "To fill in this list's class counts, the splitter will iterate through each row in the node, grab its raw feature value, and then go to the look-up table to find the position of that value in the appropriate feature's unique values list. This result tells us the location of the row's split point. \n",
    "\n",
    "In other words, once we know where the row's raw feature value ranks amongst all values of that feature observed in the training set, we'll be able to figure out the position in the `split_class_counts_wt` list at which we should increment the counter that corresponds to the row's class label.\n",
    "\n",
    "Once this list is tabulated, the splitter can iterate through each split point, from left to right, adjusting the left and right child weight class counts and calculating the proxy gini score as it goes. Many times the class counts for a given split-point will all be zero because the node has no samples that have that split-point's raw feature value. To easily skip these split points, we'll also compile a list of `split_weighted_sample_counts` inside the smae for-loop we use to tabulate the `split_class_counts_wt` list. As the splitter proceeds through split-points, it will skip past all splits where the weighted sample count is zero.\n",
    "\n",
    "#### Ranger's difference to Breiman & Cutler's pre-sorting\n",
    "What stood out to me most about Ranger's pre-sorting is that it gets the a lot of the benefit that Breiman & Cutler's pre-sorting strategy provides, but without constant upkeep that their approach requires. To be clear, however, when it comes to the specific act of iterating through the pre-sorted values, Breiman's method will be faster because each node's values for every numerical feature will already have been pre-sorted and stored contiguously. The downside of this, of course, is that everytime a node is split its samples' pre-sorted values for all other numerical feature's *that weren't* used to split the node must also then be partitioned into left and right children.\n",
    "\n",
    "With Ranger, split-finding Wright's pre-sorted values will be a bit slower because the values of the node's samples aren't stored contiguously. Instead, the index of each row's split point must be looked up, one-by-one, inside a for-loop. On the other hand, the benefit of using such a for-loop is that unlike with Breiman and Cutler, no other extra upkeep need be done to update the ordering of all other numerical features' pre-sorted values.\n",
    "\n",
    "Indeed, in datasets with thousands or tens of thousands of features, I would imagine that Breiman and Cutler's strategy simply wouldn't be feasible.\n",
    "\n",
    "In his paper that introduced Ranger, Wright was clear that it was only after extensive runtime profiling that he landed on the strategy of alternating between \"small Q\" and \"large Q\" numerical splitting algorithms depending on the number of samples in the node as well as the number of unique values found in the training set under the candidate feature. I'm still hopeful that this strategy will ultimately prove competitive with Louppe's numerical splitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Re-implementing `preprocess_train` to perform \"large Q\" preprocesssing\n",
    "Without further ado, let's write up logic that will generate the numerical feature unique values lists, and the index table of numerical split points for each training row. I'll add this code to revised version of my training data pre-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_train(x, y, cat_feats=[], largeQ=False):\n",
    "    \"\"\"Pre-process training data and labels for training via random forests.\n",
    "    \n",
    "    Uses these heuristics:\n",
    "        1. Categorical features by default are preprocessed using PCA encoding from:\n",
    "               Coppersmith et. al. (1999): https://link.springer.com/article/10.1023%2FA%3A1009869804967\n",
    "           This done only once at the beginning of training (and not at each split), from:\n",
    "               Wright and KÃ¶nig (2019): https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6368971/pdf/peerj-07-6339.pdf\n",
    "           Avoids the absent levels problem described in:\n",
    "               Wu (2018): https://dl.acm.org/doi/abs/10.5555/3291125.3309607\n",
    "        2. Numerical feature NaNs replaced by median value.\n",
    "        3. Categorical feature NaNs replaced by mode level. Both these NaN strategies\n",
    "           are Breiman's \"current preferred method\" from:\n",
    "               Breiman (2002): https://www.stat.berkeley.edu/~breiman/Using_random_forests_V3.1.pdf\n",
    "    \n",
    "    Arguments: \n",
    "        x (Pandas or Numpy array): The original un-preprocessed training data.\n",
    "        y (Pandas or Numpy array): Numerically encoded training labels.\n",
    "        cat_feats          (list): Categorical features' column indices.\n",
    "        largeQ             (bool): Whether tree fitting will use \"large Q\" \n",
    "                                   numerical splitting.\n",
    "        \n",
    "    Returns: \n",
    "        The processed training data and labels, a dictionary of values used to fill\n",
    "        each column's NaN values, and a dictionary containing categorical level-to-PCA maps.\n",
    "        The contents of both these dicts are stored under column index numbers.\n",
    "        \n",
    "        Also returns if \"large Q\" numerical splitting is enabled:\n",
    "            - Column-major Numpy array where each row holds the index (amongst a \n",
    "              feature's ordered, ascending unique values) at which that row's raw value \n",
    "              for that feature would be located. These indices hold split-point locations \n",
    "              used for LargeQ numerical splitting.\n",
    "              \n",
    "            - Column-major Numpy array where first n rows of each column contains the\n",
    "              feature's n unique values in ascending order. Shape of array is\n",
    "              <max cardinality (of all feats)> x <nfeats>.\n",
    "              \n",
    "            - 1-d array holding cardinality (num unique vals) of each feature.\n",
    "    \"\"\"\n",
    "    x, y = np.asfortranarray(x), np.ascontiguousarray(y)\n",
    "    n, nclass, nfeat, has_cat_feats = len(x), len(np.unique(y)), x.shape[1], len(cat_feats) > 0\n",
    "    num_feats = [i for i in range(nfeat) if i not in cat_feats]; has_num_feats = nfeat > len(cat_feats)\n",
    "    nan_fillers, pca_maps = {}, {} # NaN fill values and cat level-to-PCA mappings stored under feat col idxs.\n",
    "    if has_cat_feats:\n",
    "        for i in cat_feats:\n",
    "            values = x[:,i]\n",
    "            nans = pd.isna(values); has_nans = nans.sum() > 0\n",
    "            # Step 1: Naive ordinal encode all non-NaN categorical values.\n",
    "            values[~nans], levels = integer_encode(values[~nans])\n",
    "            # Step 2: Store modes of all categorical features.\n",
    "            mode = stats.mode(values.astype(float), nan_policy='omit').mode[0]\n",
    "            # Step 3: Replace any NaNs in cat cols with cols' modes.\n",
    "            if has_nans: values[nans] = mode\n",
    "            # Step 4: PCA rank-encode all categorical features.\n",
    "            x[:,i], pca_maps[i] = PCA_rank_encode(values.astype(int), y, levels, n, nclass)\n",
    "            nan_fillers[i] = pca_maps[i][levels[int(mode)]] # Store the PCA rank-encoded mode value for each cat feat.\n",
    "    if has_num_feats:                                       # The levels list stores orig. level strings in order of \n",
    "        values = x[:,num_feats].astype(float)               # their naive ordinal encodings.\n",
    "        nans = np.isnan(values); has_nans = nans.sum() > 0\n",
    "        # Step 5: Store medians of all numerical features.\n",
    "        medians = np.nanmedian(values, axis=0)\n",
    "        for i,m in enumerate(medians): nan_fillers[num_feats[i]] = m\n",
    "        # Step 6: Replace any NaNs in num cols with cols' medians.\n",
    "        if has_nans: x[:,num_feats] = np.where(nans, medians, x[:,num_feats])\n",
    "    if largeQ:\n",
    "        unique_vals_feats = [np.unique(x[:,i]) for i in range(nfeat)]\n",
    "        split_point_idxs = np.asfortranarray([[np.searchsorted(unique_vals_feats[i], x[j][i], \n",
    "                                                      side='left', sorter=None) \n",
    "                                      for j in range(x.shape[0])]\n",
    "                                      for i in range(x.shape[1])], dtype='int').T\n",
    "        n_unique_vals_feats = np.array([len(arr) for arr in unique_vals_feats], dtype=np.intp, order='C')\n",
    "        unique_vals_feats_out = np.empty((n_unique_vals_feats.max(), nfeat), dtype=np.float64, order='F')\n",
    "        for i in range(nfeat): unique_vals_feats_out[:n_unique_vals_feats[i],i] = unique_vals_feats[i]\n",
    "        return (x.astype('float64'), y, nan_fillers, pca_maps, np.asfortranarray(split_point_idxs), \n",
    "                np.asfortranarray(unique_vals_feats_out), n_unique_vals_feats)\n",
    "    else:\n",
    "        return x.astype('float64'), y, nan_fillers, pca_maps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_num_split_largeQ(rows, split_point_idxs, unique_vals_feats, labels, node_start, n_parent, \n",
    "                          n_class, min_samples_leaf, min_weight_leaf, c_wts, l_wcc, r_wcc,\n",
    "                          parent_wcc, best_split, current_feat, parent_num, parent_den, \n",
    "                          n_unique_vals_feat, split_counts_raw, split_counts_wt, split_class_counts_wt):\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's LargeQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows                      (ndarray of int): Indices of all rows in the training set. \n",
    "                                                    Shape: (n train samples,).\n",
    "        split_point_idxs          (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                    Shape: (n training samples, n features).\n",
    "        unique_vals_feats     (ndarray of float64): Columns contain sorted unique values for all features. \n",
    "                                                    Shape: (max cardinality of all feats, n features).\n",
    "        labels                    (ndarray of int): All training labels. Shape: (n training samples,).\n",
    "        node_start                           (int): Index of the beginning of the parent node in `rows`.\n",
    "        n_parent                             (int): Number of samples in the parent node.\n",
    "        n_class                              (int): Number of unique classes in the training set.\n",
    "        min_samples_leaf                     (int): Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf                  (float64): Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                 (ndarray of float64): Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                 (ndarray of float64): Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                 (ndarray of float64): Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc            (ndarray of float64): Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split                         (Split): Holds the feature, threshold, and impurity\n",
    "                                                    score of the parent node's current best split.\n",
    "        current_feat                         (int): Column index of feature under investigation.\n",
    "        parent_num                       (float64): Numerator of parent node's impurity score.\n",
    "        parent_den                       (float64): Denominator of parent node's impurity score.\n",
    "        n_unique_vals_feat                   (int): Number of unique values for one feature found among \n",
    "                                                    all training samples.\n",
    "        split_counts_raw          (ndarray of int): Stores sample counts found at each unique split point of\n",
    "                                                    a given feature for a given node. \n",
    "                                                    Shape: (max cardinality of all features,).\n",
    "        split_counts_wt       (ndarary of float64): Stores weighted sample counts found at each unique split point\n",
    "                                                    of a given feature for a given node. \n",
    "                                                    Shape: (max cardinality of all features,).\n",
    "        split_class_counts_wt (ndarray of float64): Stores weighted class counts of each class at each unique\n",
    "                                                    split point of a given feature for a given node. Shape:\n",
    "                                                    (<max cardinalty of all features> x `n_class`,)\n",
    "              \n",
    "    Returns: \n",
    "        int: 1 if feature is constant for eligible split-points. 0, otherwise.\n",
    "    \"\"\"\n",
    "    # Whether or not feat is constant within search range permitted\n",
    "    # by min_samples_leaf and min_weight_leaf (0 if no, 1 if yes).\n",
    "    current_feat_const = 1\n",
    "    \n",
    "    # Tabulate sample counts, weighted counts, and weighted class counts at \n",
    "    # each split point. Values at split points not belonging to node's rows\n",
    "    # will remain zero.\n",
    "    split_counts_raw[:n_unique_vals_feat] = 0\n",
    "    split_counts_wt[:n_unique_vals_feat] = 0.\n",
    "    split_class_counts_wt[:n_unique_vals_feat*n_class] = 0.\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        label = labels[row]\n",
    "        wt = c_wts[label]\n",
    "        split_point_idx = split_point_idxs[row][current_feat]\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_counts_wt[split_point_idx] += wt\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += wt\n",
    "        \n",
    "    # If feat is constant for the node.\n",
    "    if (split_counts_raw[:n_unique_vals_feat] > 0).sum() < 2: \n",
    "        return current_feat_const\n",
    "      \n",
    "    # To keep track of num samples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(n_unique_vals_feat - 1):\n",
    "        if split_counts_raw[i] == 0: continue # Move to next split-point if no samples at this one.\n",
    "            \n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "        if n_right == 0: return current_feat_const # Stop search when right child empty.\n",
    "        \n",
    "        # Calculate denominators of proxy gini scores.\n",
    "        l_den += split_counts_wt[i]\n",
    "        r_den = parent_den - l_den\n",
    "        \n",
    "        # Calculate numerators of proxy gini scores.\n",
    "        l_num, r_num = 0., 0. \n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "        \n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return current_feat_const\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return current_feat_const\n",
    "        \n",
    "        current_feat_const = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Find raw feature value of sample(s) at next-closest split-point.\n",
    "            k = i+1\n",
    "            while split_counts_raw[k] == 0: k+=1\n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = unique_vals_feats[i][current_feat]/2. + unique_vals_feats[k][current_feat]/2. \n",
    "            if mid == unique_vals_feats[k][current_feat]: mid = unique_vals_feats[i][current_feat]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "    return current_feat_const"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright LargeQ Decision Tree Python Version\n",
    "To attain the fastest speed, Wright designed Ranger to have the tree-growing algorithm alternate between \"small Q\" and \"large Q\" node splitting, depending on the size of the node and number of unique values held by the candidate splitting feature.\n",
    "\n",
    "I plan to implement this as well, but first I'd like to see how the exclusive use of Ranger's \"large Q\" style splitting compares to the \"small Q\" method, as well as to Louppe/Sklearn-style splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeLargeQ():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Marvin Wright's LargeQ numerical splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights=[], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                         (int): Use when reproducibility desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C')\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = np.empty(0, dtype=Node, order='C')\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = np.empty(0, dtype=np.float64, order='C')\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self.n_nodes\n",
    "    \n",
    "    @property \n",
    "    def left_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].l_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def right_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].r_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_features(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].feat\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): \n",
    "        out = np.empty(self.n_nodes, dtype='float64')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].thresh\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def weighted_cc(self):\n",
    "        out_size = self.n_nodes*self.n_class\n",
    "        out = np.empty(out_size, dtype='float64')\n",
    "        for i in range(out_size):\n",
    "            out[i] = self.weighted_class_counts[i]\n",
    "        out.resize(self.n_nodes, self.n_class)\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def labels(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].label\n",
    "        return out\n",
    "    \n",
    "    def _increase_mem_capacity(self, new_capacity):\n",
    "        \"\"\"Resize ndarrays that hold tree's nodes and weighted class counts.\n",
    "        \n",
    "        Arguments:\n",
    "            new_capacity (int): Amount of nodes that resized arrays will be able to hold.\n",
    "        \"\"\"\n",
    "        self.nodes.resize(new_capacity, refcheck=False)\n",
    "        self.weighted_class_counts.resize(new_capacity*self.n_class, refcheck=False)\n",
    "    \n",
    "    def _make_leaf(self, node_id, wcc, n_classes_node):\n",
    "        \"\"\"Set and store the class label of a leaf node.\n",
    "        \n",
    "        Break ties at random when multiple classes share the same max weight.\n",
    "        Doing this avoids a bias towards lower classes that would be a possible\n",
    "        consequence of using np.argmax (which is what Sklearn does).\n",
    "        \n",
    "        Arguments:\n",
    "            node_id            (int): Location of node in `self.nodes`.\n",
    "            wcc (ndarray of float64): Node's weighted class counts. Shape: (`self.n_class`,).\n",
    "            n_classes_node     (int): Number of unique class labels found among\n",
    "                                      node's training samples.\n",
    "        \"\"\"\n",
    "        if n_classes_node == 1: \n",
    "            label = max(enumerate(wcc), key=lambda f: f[1])[0]\n",
    "        else:              \n",
    "            label = self._rng.choice(np.argwhere(wcc==np.max(wcc)).flatten())\n",
    "        self.nodes[node_id] = Node(-1, -1, -1, np.nan, label) \n",
    "        \n",
    "    def _grow_tree(self, X, y, split_point_idxs, unique_feat_vals):\n",
    "        \"\"\"Depth-first growth of a decision tree.\n",
    "        \n",
    "        Arguments:\n",
    "            X                     (ndarray of float64): Training samples. Shape: (n samples, n features).\n",
    "            y                         (ndarray of int): Training labels. Shape: (n samples,).\n",
    "            split_point_idxs          (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                        Shape: (n training samples, n features).\n",
    "            unique_vals_feats     (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                        Shape: (max cardinality of all feats, n features).\n",
    "        \"\"\"\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        node_stack = []\n",
    "        \n",
    "        # Stores the weighted class counts of the current node.\n",
    "        node_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        ##############################################################\n",
    "        # For finding the best split.\n",
    "        ##############################################################\n",
    "        l_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        r_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        # Make 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value.\n",
    "        #\n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        split_counts_raw = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        # Weighted sample counts at each split-point.\n",
    "        split_counts_wt = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        # Weighted class counts for each split-point.\n",
    "        split_class_counts_wt = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64)  \n",
    "\n",
    "        # Keeping track of nodes' constant features. \n",
    "        features = self.features.copy()\n",
    "        constant_features = np.empty(self.n_features, dtype=np.intp)\n",
    "        \n",
    "        # Push root node onto the LIFO stack.\n",
    "        node_stack.append(StackEntry(0, self.n_samples, 0, 0, 0))\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "        while len(node_stack) > 0:\n",
    "            node_info = node_stack.pop()\n",
    "            start, end = node_info.start, node_info.end\n",
    "            node_id, parent_id = node_info.node_id, node_info.parent_id\n",
    "            n_consts = node_info.n_const_feats\n",
    "            n_samples_node = end-start\n",
    "            \n",
    "            # Tabulate and store the current node's weighted class counts.\n",
    "            node_wcc[:] = 0.\n",
    "            for i in range(n_samples_node):\n",
    "                row = self.rows[start + i]\n",
    "                label = y[row]\n",
    "                wt = self.class_weights[label]\n",
    "                node_wcc[label] += wt \n",
    "            self.weighted_class_counts[node_id*self.n_class: (node_id + 1)* self.n_class] = node_wcc\n",
    "            \n",
    "            # Make a leaf if required to do so.\n",
    "            n_classes_node, sum_node_wcc, sum_node_wcc_sqr = 0, 0., 0.\n",
    "            for c in range(self.n_class):\n",
    "                wcc = node_wcc[c]\n",
    "                if wcc > 0: n_classes_node += 1\n",
    "                # Compute the current node's proxy gini numerator and denominator while we're at it.\n",
    "                sum_node_wcc_sqr += wcc**2 \n",
    "                sum_node_wcc += wcc \n",
    "            if n_classes_node == 1:                      \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            \n",
    "            # Or perform a split.\n",
    "            else:\n",
    "                # Initialize stats for best split of node.\n",
    "                best_split = Split(-1, 0., -np.inf)\n",
    "                \n",
    "                # Ensure feats drawn w/out replacement.\n",
    "                n_drawn_feats = 0\n",
    "                n_new_consts = 0\n",
    "                n_total_consts = n_consts\n",
    "                lb = 0                      # Range in `features` array from which we \n",
    "                ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "               \n",
    "                while n_drawn_feats < self.m:\n",
    "                    n_drawn_feats += 1\n",
    "                    idx = self._rng.choice(range(lb, ub-n_new_consts+1))\n",
    "                    \n",
    "                    # So that we don't draw a known constant feature again this split-search.\n",
    "                    if idx < n_consts:\n",
    "                        features[idx], features[lb] = features[lb], features[idx]\n",
    "                        lb += 1 \n",
    "                        continue\n",
    "                        \n",
    "                    # So that no new const feats get drawn more than once per split-search.\n",
    "                    idx += n_new_consts\n",
    "                    \n",
    "                    feat_idx = features[idx]\n",
    "                  \n",
    "                    # Num split points found among training samples for given feat.\n",
    "                    n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                    \n",
    "                    # Initialize weighted class counts of right and left children.\n",
    "                    # Right child's counts are initially the same as parent node's.\n",
    "                    r_wcc[:] = node_wcc\n",
    "                    l_wcc[:] = 0.\n",
    "                    \n",
    "                    # If the feature has an impurity score that's better than the best score \n",
    "                    # found among all other features visited thus far for this node, find_num_split()\n",
    "                    # updates the attributes of the struct containing the node's best split info. \n",
    "                    # \n",
    "                    # But even if a new best score isn't reached, if an impurity score can\n",
    "                    # be calculated at least once during the feature's split search, the\n",
    "                    # following indicator will be toggled off, to indicate that the feature\n",
    "                    # is not constant (1 = is constant; 0 = not constant).\n",
    "                    current_feat_const = find_num_split_largeQ(self.rows, split_point_idxs, unique_feat_vals, y, start, n_samples_node, \n",
    "                                                       self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                       self.class_weights, l_wcc, r_wcc, node_wcc, best_split, feat_idx, \n",
    "                                                       sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, split_counts_raw, \n",
    "                                                       split_counts_wt, split_class_counts_wt)\n",
    "\n",
    "                    if current_feat_const:\n",
    "                        # The feature may be constant within the search range permitted\n",
    "                        # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                        # the feature is a newly discovered constant.\n",
    "                        features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                        n_new_consts += 1\n",
    "                        n_total_consts += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # The feature is non-constant, so we ensure it's not drawn again\n",
    "                        # during this split-search.\n",
    "                        features[idx], features[ub] = features[ub], features[idx]\n",
    "                        ub -= 1 \n",
    "                            \n",
    "                # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                features[0:n_consts] = constant_features[0:n_consts]\n",
    "                constant_features[n_consts:n_consts+n_new_consts] = features[n_consts:n_consts+n_new_consts]\n",
    "                \n",
    "                # Make node a leaf if constant for all randomly drawn feats.\n",
    "                # (# drawn known constant feats + # drawn new constant feats)\n",
    "                if lb + n_new_consts == n_drawn_feats: \n",
    "                    self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "                else: \n",
    "                    split_pos = make_num_split(self.rows, X, node_info, best_split) \n",
    "\n",
    "                    # Update info for node that's getting split.\n",
    "                    l_child_id = self.n_nodes\n",
    "                    r_child_id = l_child_id + 1\n",
    "                    self.nodes[node_id] = Node(l_child_id, r_child_id, best_split.feat, best_split.thresh, -1)\n",
    "\n",
    "                    # Prepare for the left and right child nodes\n",
    "                    # by increasing tree data memory capacity if\n",
    "                    # necessary.\n",
    "                    if self.n_nodes + 2 > self.mem_capacity:\n",
    "                        # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                        # part of WhozCraig's SO answer at: \n",
    "                        #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                        # Add one after squaring so that the new capacity can\n",
    "                        # contain not only a tree of greater depth, but also\n",
    "                        # the maximum # nodes that that depth could have.\n",
    "                        new_capacity = 2*self.mem_capacity + 1\n",
    "                        self._increase_mem_capacity(new_capacity)\n",
    "                        self.mem_capacity = new_capacity\n",
    "                    \n",
    "                    # Push right child info onto the LIFO stack.\n",
    "                    node_stack.append(StackEntry(split_pos, end, r_child_id, node_id, n_total_consts))\n",
    "                    # Push left child info onto queue.\n",
    "                    node_stack.append(StackEntry(start, split_pos, l_child_id, node_id, n_total_consts))\n",
    "\n",
    "                    # And update size of the tree.\n",
    "                    self.n_nodes += 2\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_vals_feats, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X    (Fortan-style ndarray of float64): Pre-processed training data.\n",
    "            y                     (ndarray of int): Training labels.\n",
    "            split_point_idxs      (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                    Shape: (n training samples, n features).\n",
    "            unique_vals_feats (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                    Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats      (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows                            (list): Indices of the rows to be used for training. \n",
    "                                                    All rows used if empty.\n",
    "            features                        (list): Column indices of training features that will be used.\n",
    "                                                    All features used if empty.    \n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        # Determine # classes found among all training samples.\n",
    "        root_cc = np.unique(y, return_counts=True)[1] \n",
    "        self.n_class = root_cc.size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "\n",
    "        self.n_samples = len(self.rows)\n",
    "        self.n_features = len(self.features)\n",
    "        \n",
    "        # Store the num unique vals for each numerical feat and\n",
    "        # find the maximum cardinality.\n",
    "        self.n_unique_vals_feats = n_unique_vals_feats\n",
    "        self.max_n_unique_feat_vals = self.n_unique_vals_feats.max()\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        init_capacity = 15\n",
    "        \n",
    "         # Allocate tree memory.\n",
    "        self._increase_mem_capacity(init_capacity)\n",
    "        self.mem_capacity = init_capacity\n",
    "        \n",
    "        # And sum the class weights of all the root node's samples in\n",
    "        # order to know minimum total weight a leaf must have (which\n",
    "        # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "        root_wcc = root_cc*self.class_weights\n",
    "        self.min_weight_leaf = self.min_weight_fraction_leaf*root_wcc.sum()\n",
    "        \n",
    "        # Initialize the random number generator.\n",
    "        self._rng = get_random_generator(self.seed)\n",
    "        \n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X, y, split_point_idxs, unique_vals_feats)\n",
    "        return self\n",
    "        \n",
    "    def _next_node(self, nxt): return self.nodes[nxt]\n",
    "       \n",
    "    def _get_leaf_idx(self, i, X):\n",
    "        root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[:,leaf.feat][i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else:\n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        n_preds = X.shape[0]\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        for i in range(n_preds):\n",
    "            preds[i] = self.nodes[self._get_leaf_idx(i, X)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "       \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        for i in range(n_probs):\n",
    "            idx = self._get_leaf_idx(i, X)\n",
    "            for j in range(self.n_class):\n",
    "                wcc[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wright LargeQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain_proc, yTrain_proc, nan_fillers, \n",
    " pca_maps, split_point_idxs, unique_vals_feats, \n",
    " n_unique_vals_feats) = preprocess_train(xTrain_titanic, yTrain_titanic, cat_feats, largeQ=True)\n",
    "xVal_proc = preprocess_test(xVal_titanic, nan_fillers, cat_feats, pca_maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeLargeQ(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Number of nodes in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752808988764045"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103 ms Â± 2.87 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 Âµs Â± 9.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright LargeQ Decision Tree Cython Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "# distutils: language = c++\n",
    "# distutils: extra_compile_args = -std=c++11\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t # Signed, same as ssize_t in C. See MSeifert's SO answer: https://stackoverflow.com/a/46416257/8628758\n",
    "cimport cython\n",
    "from libc.math cimport log as ln\n",
    "from libc.stdlib cimport realloc, free\n",
    "from libc.string cimport memcpy\n",
    "from libc.string cimport memset\n",
    "from libcpp.stack cimport stack\n",
    "\n",
    "# For C++ random number generation.\n",
    "from libc.stdint cimport uint_fast32_t \n",
    "    \n",
    "# For convenient memory reallocation.\n",
    "ctypedef fused realloc_t:\n",
    "    SIZE_t\n",
    "    DTYPE_t\n",
    "    Node\n",
    "\n",
    "cdef inline realloc_t* safe_realloc(realloc_t* ptr, SIZE_t n_items) nogil except *:\n",
    "    # Inspired by Sklearn's safe_realloc() func. However, thankfully\n",
    "    # Cython now no longer requires us to send a pointer to a pointer\n",
    "    # in order to prevent crashes.\n",
    "    cdef realloc_t elem = ptr[0]\n",
    "    cdef SIZE_t n_bytes = n_items * sizeof(elem)\n",
    "    # Make sure we're not trying to allocate too much memory.\n",
    "    if n_bytes/sizeof(elem) != n_items:\n",
    "        with gil:\n",
    "            raise MemoryError(f\"Overflow error: unable to allocate {n_bytes} bytes.\")       \n",
    "    cdef realloc_t* res_ptr = <realloc_t *> realloc(ptr, n_bytes)\n",
    "    with gil:\n",
    "        if not res_ptr: raise MemoryError()\n",
    "    return res_ptr\n",
    "\n",
    "# C++ random number generator. Not yet a part of a Cython release so\n",
    "# pasted in from: \n",
    "#     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/Cython/Includes/libcpp/random.pxd#L1\n",
    "cdef extern from \"<random>\" namespace \"std\" nogil:\n",
    "    cdef cppclass random_device:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        random_device() except +\n",
    "        result_type operator()() except +\n",
    "\n",
    "    cdef cppclass mt19937:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        mt19937() except +\n",
    "        mt19937(result_type seed) except +\n",
    "        result_type operator()() except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        void discard(size_t z) except +\n",
    "        void seed(result_type seed) except +\n",
    "\n",
    "    cdef cppclass uniform_int_distribution[T]:\n",
    "        ctypedef T result_type\n",
    "        uniform_int_distribution() except +\n",
    "        uniform_int_distribution(T, T) except +\n",
    "        result_type operator()[Generator](Generator&) except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        \n",
    "# Info for any node that will eventually be split or made into a leaf.\n",
    "# Similar to what Sklearn does at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/a2c4d8b1f4471f52a4fcf1026f495e637a472568/sklearn/tree/_tree.pyx#L126\n",
    "cdef struct StackEntry:\n",
    "    SIZE_t start\n",
    "    SIZE_t end\n",
    "    SIZE_t node_id\n",
    "    SIZE_t parent_id\n",
    "    SIZE_t n_const_feats\n",
    "\n",
    "# To compare node splits.\n",
    "cdef struct Split:\n",
    "    SIZE_t feat\n",
    "    DTYPE_t thresh\n",
    "    DTYPE_t score  \n",
    "\n",
    "# Vital characteristics of a node. Set when it's added to the tree.\n",
    "cdef struct Node:\n",
    "    SIZE_t l_child # idx of left child, -1 if leaf\n",
    "    SIZE_t r_child # idx of right child, -1 if leaf\n",
    "    SIZE_t feat    # col idx of split feature, -1 if leaf\n",
    "    DTYPE_t thresh # double split threshold, NAN if leaf\n",
    "    SIZE_t label   # class label if leaf, -1 if non-leaf.\n",
    "\n",
    "cdef inline void find_num_split_largeQ(SIZE_t* rows, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats, \n",
    "                                       SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, SIZE_t n_samples,\n",
    "                                       SIZE_t n_class, SIZE_t min_samples_leaf, DTYPE_t min_weight_leaf, \n",
    "                                       DTYPE_t* c_wts, DTYPE_t* l_wcc, DTYPE_t* r_wcc, DTYPE_t* parent_wcc, \n",
    "                                       Split* best_split, SIZE_t current_feat, DTYPE_t parent_num, \n",
    "                                       DTYPE_t parent_den, SIZE_t n_unique_vals_feat, SIZE_t max_n_unique_vals, \n",
    "                                       SIZE_t* split_counts_raw, DTYPE_t* split_counts_wt, \n",
    "                                       DTYPE_t* split_class_counts_wt, bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's SmallQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows                   : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        split_point_idxs       : All numerical feature split-point locations for all rows.\n",
    "                                 Shape: (n training samples, n features).\n",
    "        unique_vals_feats      : Columns contain sorted unique values for all features. \n",
    "                                 Shape: (max cardinality of all feats, n features).\n",
    "                                 node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                 : All training labels. Shape: (n training samples,).\n",
    "        node_start             : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent               : Number of samples in the parent node.\n",
    "        n_samples              : Number of samples in the training data.\n",
    "        n_class                : Number of unique classes in the training set.\n",
    "        min_samples_leaf       : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf        : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                  : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                  : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                  : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc             : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split             : Holds the feature, threshold and impurity\n",
    "                                 score of the parent node's current best split.\n",
    "        current_feat           : Column index of feature under investigation.\n",
    "        parent_num             : Numerator of parent node's impurity score.\n",
    "        parent_den             : Denominator of parent node's impurity score.\n",
    "        n_unique_vals_feat     : Number of unique values for one feature found among \n",
    "                                 all training samples.\n",
    "        max_n_unique_vals      : Maximum cardinality of all features in dataset.\n",
    "        split_counts_raw       : Stores sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_counts_wt        : Stores weighted sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt  : Stores weighted class counts of each class at each\n",
    "                                 split point of a given feature in a given node. Shape:\n",
    "                                 (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "        current_feat_const     : Whether current splitting feature is constant for all eligible split \n",
    "                                 thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used while tabulating raw and weighted sample counts, \n",
    "    # as well as weighted class counts at all unique split points.\n",
    "    cdef SIZE_t row, label, split_point_idx\n",
    "    \n",
    "    # Make sure node's samples aren't all constant for feature.\n",
    "    cdef SIZE_t n_splits_node = 0\n",
    "    \n",
    "    # Variables to track progress during the split search.\n",
    "    cdef SIZE_t n_left, n_right, i, j, k\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, wt, score, mid\n",
    "    \n",
    "    # Tabulate sample counts, weighted counts, and weighted class counts at \n",
    "    # each split point. Values at split points not belonging to node's rows\n",
    "    # will remain zero.\n",
    "    memset(split_counts_raw, 0, sizeof(SIZE_t)*n_unique_vals_feat)\n",
    "    memset(split_counts_wt, 0, sizeof(DTYPE_t)*n_unique_vals_feat)\n",
    "    memset(split_class_counts_wt, 0, sizeof(SIZE_t)*n_unique_vals_feat*n_class)\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        label = labels[row]\n",
    "        wt = c_wts[label]\n",
    "        split_point_idx = split_point_idxs[n_samples*current_feat + row]\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_counts_wt[split_point_idx] += wt\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += wt\n",
    "        \n",
    "    # If feat is constant for the node.\n",
    "    for i in range(n_unique_vals_feat):\n",
    "        if split_counts_raw[i] > 0: n_splits_node += 1\n",
    "    if n_splits_node < 2: return\n",
    "    \n",
    "    # To keep track of num samples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(n_unique_vals_feat - 1):\n",
    "        if split_counts_raw[i] == 0: continue # Move to next split-point if no samples at this one.\n",
    "        \n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "        if n_right == 0: return # Make sure to stop search when right child empty.\n",
    "        \n",
    "        # Calculate denominators of proxy gini scores.\n",
    "        l_den += split_counts_wt[i]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Calculate numerators of proxy gini scores.\n",
    "        l_num, r_num = 0., 0. \n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Find raw feature value of sample(s) at next-closest split-point.\n",
    "            k = i+1\n",
    "            while split_counts_raw[k] == 0: k+=1\n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = (unique_vals_feats[current_feat*max_n_unique_vals + i]/2. + \n",
    "                   unique_vals_feats[current_feat*max_n_unique_vals + k]/2.) \n",
    "            if mid == unique_vals_feats[current_feat*max_n_unique_vals + k]: \n",
    "                mid = unique_vals_feats[current_feat*max_n_unique_vals + i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "\n",
    "cdef inline SIZE_t make_num_split(SIZE_t* rows, DTYPE_t* X, StackEntry* node_info, Split* best_split, \n",
    "                                SIZE_t n_samples) nogil:\n",
    "    cdef SIZE_t p, p_end\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[best_split.feat*n_samples + rows[p]] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p] \n",
    "    return p\n",
    "\n",
    "# Necessary constants.\n",
    "cdef DTYPE_t NEG_INF = -np.inf\n",
    "cdef DTYPE_t NAN = np.nan\n",
    "            \n",
    "cdef class _DecisionTree:\n",
    "    # Class attributes.\n",
    "    cdef SIZE_t seed\n",
    "    cdef mt19937 rng\n",
    "    cdef SIZE_t mem_capacity\n",
    "    cdef SIZE_t n_samples\n",
    "    cdef SIZE_t n_features\n",
    "    cdef SIZE_t n_class\n",
    "    cdef SIZE_t m\n",
    "    cdef SIZE_t min_samples_leaf, \n",
    "    cdef DTYPE_t min_weight_fraction_leaf\n",
    "    cdef DTYPE_t min_weight_leaf\n",
    "    cdef SIZE_t n_nodes\n",
    "    cdef SIZE_t max_n_unique_feat_vals\n",
    "    cdef SIZE_t* n_unique_vals_feats\n",
    "    cdef SIZE_t* rows\n",
    "    cdef SIZE_t* features\n",
    "    cdef DTYPE_t* class_weights\n",
    "    cdef Node* nodes\n",
    "    cdef DTYPE_t* weighted_class_counts\n",
    "    def __cinit__(self, SIZE_t m, SIZE_t min_samples_leaf, DTYPE_t min_weight_fraction_leaf, SIZE_t seed): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                       : Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                      of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                    : A seed for the C++ mt19937 32bit int random generator. \n",
    "                                      Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m, self.min_samples_leaf = m, min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = NULL\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = NULL \n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        free(self.nodes)\n",
    "        free(self.weighted_class_counts)\n",
    "        \n",
    "    property size:\n",
    "        def __get__(self):\n",
    "            return self.n_nodes\n",
    "    \n",
    "    property left_children:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].l_child\n",
    "            return out\n",
    "\n",
    "    property right_children:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].r_child\n",
    "            return out\n",
    "        \n",
    "    property split_features: \n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].feat\n",
    "            return out\n",
    "        \n",
    "    property split_thresholds:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].thresh\n",
    "            return out\n",
    "        \n",
    "    property weighted_cc:\n",
    "        def __get__(self):\n",
    "            cdef SIZE_t out_size = self.n_nodes*self.n_class\n",
    "            out = np.empty(out_size, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(out_size):\n",
    "                    out_view[i] = self.weighted_class_counts[i]\n",
    "            out.resize(self.n_nodes, self.n_class)\n",
    "            return out\n",
    "    \n",
    "    property labels:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].label\n",
    "            return out\n",
    "    \n",
    "    cdef void _increase_mem_capacity(self, SIZE_t new_capacity) nogil:\n",
    "        self.nodes = safe_realloc(self.nodes, new_capacity)\n",
    "        self.weighted_class_counts = safe_realloc(self.weighted_class_counts, self.n_class*new_capacity)\n",
    "    \n",
    "    cdef void _make_leaf(self, Node* leaf_node, SIZE_t* y, SIZE_t node_start, SIZE_t node_id, \n",
    "                         SIZE_t n_classes_node, SIZE_t* max_wt_classes) nogil:\n",
    "        # Class with largest wcc becomes leaf node's label. Break ties with a random choice.\n",
    "        cdef SIZE_t label\n",
    "        cdef DTYPE_t max_wt = 0.\n",
    "        cdef SIZE_t lb = 0\n",
    "        cdef SIZE_t ub = -1\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t i, j\n",
    "        # If all node's samples have the same class.\n",
    "        if n_classes_node == 1:\n",
    "            label = y[self.rows[node_start]]\n",
    "        else:\n",
    "            # Otherwise find label with max weighted class count for the node.\n",
    "            for i in range(self.n_class):\n",
    "                max_wt = max(max_wt, self.weighted_class_counts[node_id*self.n_class + i])\n",
    "            # See if multiple classes share this max count.\n",
    "            for i in range(self.n_class):\n",
    "                if self.weighted_class_counts[node_id*self.n_class + i] == max_wt:\n",
    "                    ub += 1\n",
    "                    max_wt_classes[ub] = i\n",
    "            # If so, randomly choose leaf's label from among those classes.\n",
    "            if ub > 0:\n",
    "                dist = uniform_int_distribution[SIZE_t](lb, ub) # Choose an int w/in range lb, ub, inclusive.\n",
    "                j = dist(self.rng)\n",
    "                label = max_wt_classes[j]\n",
    "            else:\n",
    "                label = max_wt_classes[lb]\n",
    "        leaf_node.l_child = -1\n",
    "        leaf_node.r_child = -1    \n",
    "        leaf_node.feat = -1  \n",
    "        leaf_node.thresh = NAN\n",
    "        leaf_node.label = label \n",
    "\n",
    "    cdef _grow_tree(self, DTYPE_t* X, SIZE_t* y, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats):\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        cdef stack[StackEntry] node_stack\n",
    "\n",
    "        #####################################################################\n",
    "        # Variables containing info of the node currently being investigated.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t start, end, node_id, parent_id, n_consts, n_samples_node\n",
    "        cdef DTYPE_t* node_wcc = NULL\n",
    "        cdef StackEntry node_info\n",
    "        cdef Node* node = NULL\n",
    "        \n",
    "        # Holds child node info if the current node gets split.\n",
    "        cdef SIZE_t l_child_id, r_child_id\n",
    "        cdef Node* l_child_node = NULL\n",
    "        cdef Node* r_child_node = NULL\n",
    "        \n",
    "        #####################################################################\n",
    "        # For finding the best split.\n",
    "        #####################################################################\n",
    "        cdef Split best_split\n",
    "        cdef DTYPE_t* l_wcc = NULL\n",
    "        cdef DTYPE_t* r_wcc = NULL\n",
    "        cdef DTYPE_t sum_node_wcc_sqr, sum_node_wcc # Parent node's proxy Gini score num and den.\n",
    "        cdef SIZE_t split_pos\n",
    "        \n",
    "        # Indicates a feature has been discovered to be constant during a\n",
    "        # split search within the search range permitted by min_samples_leaf \n",
    "        # and min_weight_leaf.\n",
    "        cdef bint current_feat_const \n",
    "        \n",
    "        # Num unique vals for given feat found among all training samples.\n",
    "        cdef SIZE_t n_unique_vals_feat\n",
    "        \n",
    "        # Make 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value.\n",
    "        #\n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        cdef SIZE_t[::1] split_counts_raw_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        cdef SIZE_t* split_counts_raw = &split_counts_raw_buffer[0]\n",
    "        # Weighted sample counts at each split-point.\n",
    "        cdef DTYPE_t[::1] split_counts_wt_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        cdef DTYPE_t* split_counts_wt = &split_counts_wt_buffer[0]\n",
    "        # Weighted class counts for each split-point.\n",
    "        cdef DTYPE_t[::1] split_class_counts_wt_buffer = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64) \n",
    "        cdef DTYPE_t* split_class_counts_wt = &split_class_counts_wt_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For random feature selection (w/out replacement) and keeping track \n",
    "        # of nodes' constant features. \n",
    "        #####################################################################\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t lb, ub, idx, feat_idx, n_drawn_feats, n_new_consts, n_total_consts\n",
    "        cdef SIZE_t[::1] features_buffer = np.empty(self.n_features, dtype=np.intp) \n",
    "        cdef SIZE_t* features = &features_buffer[0]\n",
    "        cdef SIZE_t[::1] constant_features_buffer = np.empty(self.n_features, dtype=np.intp)\n",
    "        cdef SIZE_t* constant_features = &constant_features_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For determining whether node should be a leaf.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t i, c, cc, n_classes_node, row, label\n",
    "        cdef DTYPE_t wcc, wt\n",
    "        # Stores classes that share a leaf's max class wt. When two or more \n",
    "        # present, leaf label randomly chosen from these classes\n",
    "        cdef SIZE_t* max_wt_classes = NULL\n",
    "        \n",
    "        with nogil:\n",
    "            # Allocate memory to pointers.\n",
    "            l_wcc = safe_realloc(l_wcc, self.n_class)\n",
    "            r_wcc = safe_realloc(r_wcc, self.n_class)\n",
    "            node_wcc = safe_realloc(node_wcc, self.n_class)\n",
    "            max_wt_classes = safe_realloc(max_wt_classes, self.n_class*sizeof(SIZE_t))\n",
    "            # Fill with feature column indices so we can track constant feats.\n",
    "            memcpy(features, self.features, self.n_features* sizeof(SIZE_t))\n",
    "            \n",
    "            # Push root node onto the LIFO stack.\n",
    "            node_stack.push({\"start\": 0, \"end\": self.n_samples, \"node_id\": 0, \n",
    "                             \"parent_id\": 0, \"n_const_feats\": 0})\n",
    "            self.n_nodes = 1\n",
    "            while not node_stack.empty():\n",
    "                node_info = node_stack.top()\n",
    "                node_stack.pop()\n",
    "                start, end = node_info.start, node_info.end\n",
    "                node_id, parent_id = node_info.node_id, node_info.parent_id # TODO: `parent_id` unused; is it necessary?\n",
    "                n_consts = node_info.n_const_feats\n",
    "                n_samples_node = end-start\n",
    "                node = &self.nodes[node_id]\n",
    "                \n",
    "                # Tabulate the current node's weighted class counts.\n",
    "                #\n",
    "                # Implementation detail #1: I tried storing the l and r child wt class cts\n",
    "                # of nodes' best splits so that this tabulation wouldn't need to be \n",
    "                # performed for each node. But found there was virtually no speed improvement\n",
    "                # to justify the more complicated code required to store and update these \n",
    "                # values during the best split search.\n",
    "                #\n",
    "                # Implementation detail #2: Setting aside a block of memory to \n",
    "                # store the current node's wt class cts and passing a pointer to\n",
    "                # this block to the split search function sped up training by 8%\n",
    "                # compared to passing a ptr to the location of node's wt class cts \n",
    "                # in the self.weighted_class_counts array.\n",
    "                memset(node_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                sum_node_wcc, sum_node_wcc_sqr = 0., 0.\n",
    "                for i in range(n_samples_node):\n",
    "                    row = self.rows[start + i]\n",
    "                    label = y[row]\n",
    "                    wt = self.class_weights[label]\n",
    "                    # Compute the node's proxy gini numerator and denominator while we're at it.\n",
    "                    sum_node_wcc_sqr += wt*(2*node_wcc[label] + wt) # numerator\n",
    "                    sum_node_wcc += wt                              # denominator\n",
    "                    node_wcc[label] += wt\n",
    "                memcpy(&self.weighted_class_counts[node_id*self.n_class], node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                \n",
    "                # Make a leaf if required to do so. \n",
    "                n_classes_node = 0\n",
    "                for c in range(self.n_class):\n",
    "                    wcc = node_wcc[c]\n",
    "                    if wcc > 0: n_classes_node += 1\n",
    "                if n_classes_node == 1:                   \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "\n",
    "                # Otherwise split the node.\n",
    "                else:\n",
    "                    # Initialize stats for best split of node.\n",
    "                    best_split.feat = -1\n",
    "                    best_split.thresh = 0.\n",
    "                    best_split.score = NEG_INF\n",
    "\n",
    "                    # Ensure feats drawn w/out replacement.\n",
    "                    n_drawn_feats = 0\n",
    "                    n_new_consts = 0\n",
    "                    n_total_consts = n_consts\n",
    "                    lb = 0                      # Range in `features` array from which we \n",
    "                    ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "                        \n",
    "                    while n_drawn_feats < self.m:\n",
    "                        n_drawn_feats += 1\n",
    "\n",
    "                        # Breiman & Cutler's original Fortran random forests implementation \n",
    "                        # allows for known constant features to be drawn during a split-search.\n",
    "                        # I follow their example, as I believe that doing so allows individual \n",
    "                        # trees to be less correlated with each other. Since I don't pre-sort\n",
    "                        # features, I would prefer not to have to sort any more features than\n",
    "                        # necessary, and so I've adopted the technique Sklearn uses to track \n",
    "                        # constant features:\n",
    "                        #     https://github.com/scikit-learn/scikit-learn/blob/dbe39454f766ebefc3219f2c1871ac1774316532/sklearn/tree/_splitter.pyx#L310\n",
    "                        # \n",
    "                        # The idea is that feature idxs in `features` are organized into two sections:\n",
    "                        #\n",
    "                        #     [<indices of known constant feats>, <indices of non-constant feats>]\n",
    "                        #\n",
    "                        # As we begin drawing feature indices from this above list, those two sections\n",
    "                        # will each be further sub-divided into two sections:\n",
    "                        # \n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <undrawn non-constant feats>, <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # When we choose a feature that happens to be a known constant, we'll re-locate\n",
    "                        # its idx to the right-end of the first of those four sections. Then we \n",
    "                        # increment the lower bound threshold, `lb`, by one so that we don't re-draw \n",
    "                        # that feature again.\n",
    "                        #\n",
    "                        # Similarly, if we draw a non-constant feature idx, we'll move it to the \n",
    "                        # left-end of the last of the four partitions and reduce the upper bound\n",
    "                        # threshold, `ub`, by one so that the feature idx can't be drawn again\n",
    "                        # during this split-search. \n",
    "                        #\n",
    "                        # One last important detail: sometimes we'll draw a feature that \n",
    "                        # used to be non-constant for ancestor nodes, but will be found to be \n",
    "                        # constant for the current node. When this happens, we relocate its \n",
    "                        # index so that it sits to the right of the known constant feats section.\n",
    "                        # This means our `features` list could have up to five partitions:\n",
    "                        #\n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <newly discovered const feats>, <undrawn non-constant feats>, \n",
    "                        #      <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # Whenever we find a new constant feature, we increment the `n_new_consts`\n",
    "                        # counter by one. We also increment the `n_total_consts` counter by one. \n",
    "                        # During the split-search we have to use `n_total_consts` to keep track of\n",
    "                        # the total number of constant features. n_consts` mustn't be changed\n",
    "                        # because it tells us where the <newly discovered const feats> section\n",
    "                        # of the `features` list begins.\n",
    "\n",
    "                        # One last wrinkle. We subtract the # of newly discovered const feats from  \n",
    "                        # the upper bound before we select an index `i` from the `features` array, \n",
    "                        # and add it back to `i` after `i` has been genereated. This prevents us from \n",
    "                        # re-drawing any of these new const feats again during this split-search.\n",
    "                        dist = uniform_int_distribution[SIZE_t](lb, ub-n_new_consts)\n",
    "                        idx = dist(self.rng)\n",
    "\n",
    "                        # So that we don't draw a known constant feature again this split-search.\n",
    "                        if idx < n_consts:\n",
    "                            features[idx], features[lb] = features[lb], features[idx]\n",
    "                            lb += 1 \n",
    "                            continue\n",
    "\n",
    "                        # So that no new const feats get drawn more than once per split-search.\n",
    "                        idx += n_new_consts\n",
    "\n",
    "                        feat_idx = features[idx]\n",
    "                        \n",
    "                        # Num split points found among training samples for given feat.\n",
    "                        n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                        \n",
    "                        # Initialize weighted class counts of right and left children.\n",
    "                        # Right child's counts are initially the same as parent node's.\n",
    "                        memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                        memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "\n",
    "                        # If the feature has an impurity score that's better than the best score \n",
    "                        # found among all other features visited thus far for this node, find_num_split()\n",
    "                        # updates the attributes of the struct containing the node's best split info. \n",
    "                        # \n",
    "                        # But even if a new best score isn't reached, if an impurity score can\n",
    "                        # be calculated at least once during the feature's split search, the\n",
    "                        # following indicator will be toggled off, to indicate that the feature\n",
    "                        # is not constant.\n",
    "                        current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                        find_num_split_largeQ(self.rows, split_point_idxs, unique_vals_feats, y, start, \n",
    "                                              n_samples_node, self.n_samples, self.n_class, self.min_samples_leaf, \n",
    "                                              self.min_weight_leaf, self.class_weights, l_wcc, r_wcc, node_wcc, \n",
    "                                              &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, \n",
    "                                              self.max_n_unique_feat_vals, split_counts_raw, split_counts_wt, \n",
    "                                              split_class_counts_wt, &current_feat_const)\n",
    "\n",
    "                        if current_feat_const:\n",
    "                            # The feature may be constant within the search range permitted\n",
    "                            # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                            # the feature is a newly discovered constant.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The feature is non-constant, so we ensure it's not drawn again\n",
    "                            # during this split-search.\n",
    "                            features[idx], features[ub] = features[ub], features[idx]\n",
    "                            ub -= 1 \n",
    "\n",
    "                    # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                    memcpy(&features[0], &constant_features[0], sizeof(SIZE_t)*n_consts)\n",
    "                    memcpy(&constant_features[n_consts], &features[n_consts], sizeof(SIZE_t)*n_new_consts)\n",
    "\n",
    "                    # Make node a leaf if constant for all randomly drawn feats.\n",
    "                    # (# drawn known constant feats + # drawn new constant feats)\n",
    "                    if lb + n_new_consts == n_drawn_feats: \n",
    "                        self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                    else: \n",
    "                        split_pos = make_num_split(self.rows, X, &node_info, &best_split, self.n_samples) \n",
    "\n",
    "                        # Update tree info for node that's getting split.\n",
    "                        l_child_id = self.n_nodes\n",
    "                        r_child_id = l_child_id + 1\n",
    "                        node.l_child = l_child_id\n",
    "                        node.r_child = r_child_id\n",
    "                        node.feat    = best_split.feat\n",
    "                        node.thresh  = best_split.thresh\n",
    "                        node.label   = -1\n",
    "\n",
    "                        # Prepare for the left and right child nodes\n",
    "                        # by increasing tree data memory capacity if\n",
    "                        # necessary.\n",
    "                        if self.n_nodes + 2 > self.mem_capacity:\n",
    "                            # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                            # part of WhozCraig's SO answer at: \n",
    "                            #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                            # Add one after squaring so that the new capacity can\n",
    "                            # contain not only a tree of greater depth, but also\n",
    "                            # the maximum # nodes that that depth could have.\n",
    "                            new_capacity = 2*self.mem_capacity + 1\n",
    "                            self._increase_mem_capacity(new_capacity)\n",
    "                            self.mem_capacity = new_capacity\n",
    "                        \n",
    "                        # Push right child info onto the LIFO stack.\n",
    "                        node_stack.push({\"start\": split_pos, \"end\": end, \"node_id\": r_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "                        # Push left child info onto queue.\n",
    "                        node_stack.push({\"start\": start, \"end\": split_pos, \"node_id\": l_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "\n",
    "                        # And update size of the tree.\n",
    "                        self.n_nodes += 2\n",
    "                        \n",
    "        free(l_wcc)\n",
    "        free(r_wcc)\n",
    "        free(node_wcc)\n",
    "        free(max_wt_classes)\n",
    "    \n",
    "    def fit(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] y,\n",
    "            np.ndarray[SIZE_t, ndim=2, mode=\"fortran\"] split_point_idxs, \n",
    "            np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] n_unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] rows, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] features,\n",
    "            np.ndarray[DTYPE_t, ndim=1, mode=\"c\"] class_weights, SIZE_t n_class): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "            class_weights (1D C-contiguous array of float64): Desired weight for each class. Shape: (`n_class`,).\n",
    "            n_class                                         : Number of classes in training data.   \n",
    "        \"\"\"\n",
    "        # Casting the raw data to pointers gives a 17% speed-up compared to getting\n",
    "        # pointer from the ndarray's buffer interface, as recommended by DavidW in \n",
    "        # his SO answer at: https://stackoverflow.com/a/54832269/8628758. e.g.\n",
    "        #     cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        #     cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        # Not worried about unexpected behavior as all ndarrays' contiguousness and\n",
    "        # memory layout enforced prior to this point.\n",
    "        cdef DTYPE_t* X_ptr = <DTYPE_t*> X.data\n",
    "        cdef SIZE_t* y_ptr = <SIZE_t*> y.data\n",
    "        cdef SIZE_t* split_point_idxs_ptr = <SIZE_t*> split_point_idxs.data\n",
    "        cdef DTYPE_t* unique_vals_feats_ptr = <DTYPE_t*> unique_vals_feats.data\n",
    "        self.n_unique_vals_feats = <SIZE_t*> n_unique_vals_feats.data\n",
    "        self.rows = <SIZE_t*> rows.data\n",
    "        self.features = <SIZE_t*> features.data\n",
    "        self.class_weights = <DTYPE_t*> class_weights.data\n",
    "        self.n_class = n_class\n",
    "        self.n_samples = rows.shape[0]\n",
    "        self.n_features = features.shape[0]\n",
    "        cdef random_device rd # Needed when using the C++ mt19937 rng w/out a seed.\n",
    "        \n",
    "        # Get the max cardinality of all numerical feats.\n",
    "        self.max_n_unique_feat_vals = n_unique_vals_feats.max()\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        cdef SIZE_t init_capacity = 15\n",
    "        \n",
    "        cdef SIZE_t i, row, label\n",
    "        cdef DTYPE_t wt\n",
    "        cdef DTYPE_t sum_wts = 0\n",
    "        cdef Node* root_node = NULL\n",
    "        with nogil:\n",
    "            # Allocate memory for the tree.\n",
    "            self._increase_mem_capacity(init_capacity)\n",
    "            self.mem_capacity = init_capacity\n",
    " \n",
    "            # And sum the class weights of all the root node's samples in\n",
    "            # order to know minimum total weight a leaf must have (which\n",
    "            # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "            for i in range(self.n_samples):\n",
    "                row = self.rows[i]\n",
    "                label = y_ptr[row]\n",
    "                wt = self.class_weights[label]\n",
    "                sum_wts += wt\n",
    "            self.min_weight_leaf = self.min_weight_fraction_leaf*sum_wts\n",
    "            \n",
    "            # Initialize the random number generator. Followed example from:\n",
    "            #     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/tests/run/cpp_stl_random.pyx#L16\n",
    "            if self.seed == -1:\n",
    "                self.rng = mt19937(rd()) # If using the random device engine std::random_device.\n",
    "            else:\n",
    "                self.rng = mt19937(self.seed)\n",
    "\n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X_ptr, y_ptr, split_point_idxs_ptr, unique_vals_feats_ptr)\n",
    "    \n",
    "    cdef Node* _next_node(self, SIZE_t nxt) nogil: \n",
    "        return &self.nodes[nxt]\n",
    "    \n",
    "    cdef SIZE_t _get_leaf_idx(self, SIZE_t i, Node* leaf, SIZE_t n, DTYPE_t* X) nogil:\n",
    "        cdef SIZE_t idx\n",
    "        cdef SIZE_t root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[leaf.feat*n + i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else: \n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "    \n",
    "    def predict(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_preds = X.shape[0]\n",
    "        cdef SIZE_t i\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        cdef SIZE_t[::1] preds_view = preds\n",
    "        cdef Node leaf\n",
    "        with nogil:\n",
    "            for i in range(n_preds): \n",
    "                preds_view[i] = self.nodes[self._get_leaf_idx(i, &leaf, n_preds, X_ptr)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions. Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        cdef DTYPE_t[::1] wcc_view = wcc\n",
    "        cdef Node leaf\n",
    "        cdef SIZE_t i, j, idx\n",
    "        with nogil:\n",
    "            for i in range(n_probs):\n",
    "                idx = self._get_leaf_idx(i, &leaf, n_probs, X_ptr)\n",
    "                for j in range(self.n_class):\n",
    "                    wcc_view[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)\n",
    "\n",
    "class DecisionTreeLargeQCython():\n",
    "    \"\"\"Fit a decision tree using a depth-first algorithm.\n",
    "    \n",
    "    Uses Marvin Wright's LargeQ numerical splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights = [], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                         (int): Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C') \n",
    "        if seed is None:\n",
    "            self.seed = -1\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        self._tree = _DecisionTree(self.m, self.min_samples_leaf, self.min_weight_fraction_leaf, self.seed)\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self._tree.size\n",
    "    \n",
    "    @property\n",
    "    def left_children(self): return self._tree.left_children\n",
    "    \n",
    "    @property\n",
    "    def right_children(self): return self._tree.right_children\n",
    "            \n",
    "    @property \n",
    "    def split_features(self): return self._tree.split_features\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): return self._tree.split_thresholds\n",
    "    \n",
    "    @property\n",
    "    def weighted_class_counts(self): return self._tree.weighted_cc\n",
    "    \n",
    "    @property\n",
    "    def labels(self): return self._tree.labels\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "                                                              All rows used if empty.\n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "                                                              All rows used if empty.\n",
    "                                                              \n",
    "        Returns:\n",
    "            DecisionTreeLargeQCython: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        self.n_class = np.unique(y).size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "            \n",
    "        self._tree.fit(X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, \n",
    "                       self.rows, self.features, self.class_weights, self.n_class)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict_probs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright LargeQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeLargeQCython(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Number of nodes in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315 Âµs Â± 9.87 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.11 Âµs Â± 715 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although my Cython version of Wright's SmallQ splitting was well over twice as slow as my Cython version of Louppe's splitting technique, Wright's LargeQ Cython implementation is dangerousy close to being twice as fast as Louppe's!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright SmallQ/LargeQ Decision Tree Python Version\n",
    "Now, let's do things just as Wright intended for the Ranger library and see what happens when we fit a decision tree using the \"small Q\" splitter on nodes whose ratio, `q`, of <# samples in the node> to <current feature's # unique values in the dataset> is [less than 0.02](https://github.com/imbs-hl/ranger/blob/ce497711884c783e133fb36750b60de4c140773f/src/globals.h#L106), and using the \"large Q\" splitter for all other nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_THRESHOLD = 0.02\n",
    "\n",
    "class DecisionTreeSmallLargeQ():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Marvin Wright's SmallQ and LargeQ numerical splitting algorithms:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L173\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights=[], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                         (int): Use when reproducibility desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C')\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = np.empty(0, dtype=Node, order='C')\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = np.empty(0, dtype=np.float64, order='C')\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self.n_nodes\n",
    "    \n",
    "    @property \n",
    "    def left_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].l_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def right_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].r_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_features(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].feat\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): \n",
    "        out = np.empty(self.n_nodes, dtype='float64')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].thresh\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def weighted_cc(self):\n",
    "        out_size = self.n_nodes*self.n_class\n",
    "        out = np.empty(out_size, dtype='float64')\n",
    "        for i in range(out_size):\n",
    "            out[i] = self.weighted_class_counts[i]\n",
    "        out.resize(self.n_nodes, self.n_class)\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def labels(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].label\n",
    "        return out\n",
    "    \n",
    "    def _increase_mem_capacity(self, new_capacity):\n",
    "        \"\"\"Resize ndarrays that hold tree's nodes and weighted class counts.\n",
    "        \n",
    "        Arguments:\n",
    "            new_capacity (int): Amount of nodes that resized arrays will be able to hold.\n",
    "        \"\"\"\n",
    "        self.nodes.resize(new_capacity, refcheck=False)\n",
    "        self.weighted_class_counts.resize(new_capacity*self.n_class, refcheck=False)\n",
    "    \n",
    "    def _make_leaf(self, node_id, wcc, n_classes_node):\n",
    "        \"\"\"Set and store the class label of a leaf node.\n",
    "        \n",
    "        Break ties at random when multiple classes share the same max weight.\n",
    "        Doing this avoids a bias towards lower classes that would be a possible\n",
    "        consequence of using np.argmax (which is what Sklearn does).\n",
    "        \n",
    "        Arguments:\n",
    "            node_id            (int): Location of node in `self.nodes`.\n",
    "            wcc (ndarray of float64): Node's weighted class counts. Shape: (`self.n_class`,).\n",
    "            n_classes_node     (int): Number of unique class labels found among\n",
    "                                      node's training samples.\n",
    "        \"\"\"\n",
    "        if n_classes_node == 1: \n",
    "            label = max(enumerate(wcc), key=lambda f: f[1])[0]\n",
    "        else:              \n",
    "            label = self._rng.choice(np.argwhere(wcc==np.max(wcc)).flatten())\n",
    "        self.nodes[node_id] = Node(-1, -1, -1, np.nan, label) \n",
    "        \n",
    "    def _grow_tree(self, X, y, split_point_idxs, unique_feat_vals):\n",
    "        \"\"\"Depth-first growth of a decision tree.\n",
    "        \n",
    "        Arguments:\n",
    "            X                     (ndarray of float64): Training samples. Shape: (n samples, n features).\n",
    "            y                         (ndarray of int): Training labels. Shape: (n samples,).\n",
    "            split_point_idxs          (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                        Shape: (n training samples, n features).\n",
    "            unique_vals_feats     (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                        Shape: (max cardinality of all feats, n features).\n",
    "        \"\"\"\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        node_stack = []\n",
    "        \n",
    "        # Stores the weighted class counts of the current node.\n",
    "        node_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        ##############################################################\n",
    "        # For finding the best split.\n",
    "        ##############################################################\n",
    "        l_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        r_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        # For SmallQ Splitting, when we sort just the unique values for a feature\n",
    "        # that are found inside a single node.\n",
    "        items = np.empty(self.n_samples, dtype=np.float64)\n",
    "        \n",
    "        # 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value:\n",
    "        #\n",
    "        # Raw, non-weighted, sample counts at each split-point (used by SmallQ and LargeQ).\n",
    "        split_counts_raw = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        # Weighted sample counts at each split-point (just used by LargeQ).\n",
    "        split_counts_wt = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        # Weighted class counts for each split-point (used by SmallQ and LargeQ).\n",
    "        split_class_counts_wt = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64)  \n",
    "\n",
    "        # Keeping track of nodes' constant features. \n",
    "        features = self.features.copy()\n",
    "        constant_features = np.empty(self.n_features, dtype=np.intp)\n",
    "        \n",
    "        # Push root node onto the LIFO stack.\n",
    "        node_stack.append(StackEntry(0, self.n_samples, 0, 0, 0))\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "        while len(node_stack) > 0:\n",
    "            node_info = node_stack.pop()\n",
    "            start, end = node_info.start, node_info.end\n",
    "            node_id, parent_id = node_info.node_id, node_info.parent_id\n",
    "            n_consts = node_info.n_const_feats\n",
    "            n_samples_node = end-start\n",
    "            \n",
    "            # Tabulate and store the current node's weighted class counts.\n",
    "            node_wcc[:] = 0.\n",
    "            for i in range(n_samples_node):\n",
    "                row = self.rows[start + i]\n",
    "                label = y[row]\n",
    "                wt = self.class_weights[label]\n",
    "                node_wcc[label] += wt \n",
    "            self.weighted_class_counts[node_id*self.n_class: (node_id + 1)* self.n_class] = node_wcc\n",
    "            \n",
    "            # Make a leaf if required to do so.\n",
    "            n_classes_node, sum_node_wcc, sum_node_wcc_sqr = 0, 0., 0.\n",
    "            for c in range(self.n_class):\n",
    "                wcc = node_wcc[c]\n",
    "                if wcc > 0: n_classes_node += 1\n",
    "                # Compute the current node's proxy gini numerator and denominator while we're at it.\n",
    "                sum_node_wcc_sqr += wcc**2 \n",
    "                sum_node_wcc += wcc \n",
    "            if n_classes_node == 1:                      \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            \n",
    "            # Or perform a split.\n",
    "            else:\n",
    "                # Initialize stats for best split of node.\n",
    "                best_split = Split(-1, 0., -np.inf)\n",
    "                \n",
    "                # Ensure feats drawn w/out replacement.\n",
    "                n_drawn_feats = 0\n",
    "                n_new_consts = 0\n",
    "                n_total_consts = n_consts\n",
    "                lb = 0                      # Range in `features` array from which we \n",
    "                ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "               \n",
    "                while n_drawn_feats < self.m:\n",
    "                    n_drawn_feats += 1\n",
    "                    idx = self._rng.choice(range(lb, ub-n_new_consts+1))\n",
    "                    \n",
    "                    # So that we don't draw a known constant feature again this split-search.\n",
    "                    if idx < n_consts:\n",
    "                        features[idx], features[lb] = features[lb], features[idx]\n",
    "                        lb += 1 \n",
    "                        continue\n",
    "                        \n",
    "                    # So that no new const feats get drawn more than once per split-search.\n",
    "                    idx += n_new_consts\n",
    "                    \n",
    "                    feat_idx = features[idx]\n",
    "                  \n",
    "                    # Num split points found among training samples for given feat.\n",
    "                    n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                    \n",
    "                    q = n_samples_node/n_unique_vals_feat\n",
    "                    \n",
    "                    if q < Q_THRESHOLD:\n",
    "                        self.num_small_Q += 1\n",
    "                        items[:n_samples_node] = X[:,feat_idx][self.rows[start:end]]\n",
    "                        \n",
    "                        # Make sure the feature not constant for node's samples.\n",
    "                        node_unique_vals_feat = np.unique(items[:n_samples_node])\n",
    "                        node_n_unique_vals_feat = len(node_unique_vals_feat)\n",
    "                        if node_n_unique_vals_feat < 2:\n",
    "                            # Move the newly-discovered constant feat to the far right-end\n",
    "                            # of the left half of `features` list holding the known const\n",
    "                            # feats as well as any other const feats newly discovered \n",
    "                            # during this node's split-search.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Initialize weighted class counts of right and left children.\n",
    "                            # Right child's counts are initially the same as parent node's.\n",
    "                            r_wcc[:] = node_wcc\n",
    "                            l_wcc[:] = 0.\n",
    "\n",
    "                            # If the feature has an impurity score that's better than the best score \n",
    "                            # found among all other features visited thus far for this node, find_num_split()\n",
    "                            # updates the attributes of the struct containing the node's best split info. \n",
    "                            # \n",
    "                            # But even if a new best score isn't reached, if an impurity score can\n",
    "                            # be calculated at least once during the feature's split search, the\n",
    "                            # following indicator will be toggled off, to indicate that the feature\n",
    "                            # is not constant (1 = is constant; 0 = not constant).\n",
    "                            current_feat_const = find_num_split_smallQ(X, self.rows, node_unique_vals_feat, y, start, n_samples_node, \n",
    "                                                                       self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                                       self.class_weights, l_wcc, r_wcc, node_wcc, best_split, feat_idx, \n",
    "                                                                       sum_node_wcc_sqr, sum_node_wcc, node_n_unique_vals_feat, split_counts_raw, \n",
    "                                                                       split_class_counts_wt)\n",
    "                    else:\n",
    "                        self.num_large_Q += 1\n",
    "                        r_wcc[:] = node_wcc\n",
    "                        l_wcc[:] = 0.\n",
    "                        current_feat_const = find_num_split_largeQ(self.rows, split_point_idxs, unique_vals_feats, y, start, n_samples_node, \n",
    "                                                           self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                           self.class_weights, l_wcc, r_wcc, node_wcc, best_split, feat_idx, \n",
    "                                                           sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, split_counts_raw, \n",
    "                                                           split_counts_wt, split_class_counts_wt)\n",
    "\n",
    "                    if current_feat_const:\n",
    "                        # The feature may be constant within the search range permitted\n",
    "                        # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                        # the feature is a newly discovered constant.\n",
    "                        features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                        n_new_consts += 1\n",
    "                        n_total_consts += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # The feature is non-constant, so we ensure it's not drawn again\n",
    "                        # during this split-search.\n",
    "                        features[idx], features[ub] = features[ub], features[idx]\n",
    "                        ub -= 1 \n",
    "                            \n",
    "                # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                features[0:n_consts] = constant_features[0:n_consts]\n",
    "                constant_features[n_consts:n_consts+n_new_consts] = features[n_consts:n_consts+n_new_consts]\n",
    "                \n",
    "                # Make node a leaf if constant for all randomly drawn feats.\n",
    "                # (# drawn known constant feats + # drawn new constant feats)\n",
    "                if lb + n_new_consts == n_drawn_feats: \n",
    "                    self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "                else: \n",
    "                    split_pos = make_num_split(self.rows, X, node_info, best_split) \n",
    "\n",
    "                    # Update info for node that's getting split.\n",
    "                    l_child_id = self.n_nodes\n",
    "                    r_child_id = l_child_id + 1\n",
    "                    self.nodes[node_id] = Node(l_child_id, r_child_id, best_split.feat, best_split.thresh, -1)\n",
    "\n",
    "                    # Prepare for the left and right child nodes\n",
    "                    # by increasing tree data memory capacity if\n",
    "                    # necessary.\n",
    "                    if self.n_nodes + 2 > self.mem_capacity:\n",
    "                        # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                        # part of WhozCraig's SO answer at: \n",
    "                        #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                        # Add one after squaring so that the new capacity can\n",
    "                        # contain not only a tree of greater depth, but also\n",
    "                        # the maximum # nodes that that depth could have.\n",
    "                        new_capacity = 2*self.mem_capacity + 1\n",
    "                        self._increase_mem_capacity(new_capacity)\n",
    "                        self.mem_capacity = new_capacity\n",
    "                    \n",
    "                    # Push right child info onto the LIFO stack.\n",
    "                    node_stack.append(StackEntry(split_pos, end, r_child_id, node_id, n_total_consts))\n",
    "                    # Push left child info onto queue.\n",
    "                    node_stack.append(StackEntry(start, split_pos, l_child_id, node_id, n_total_consts))\n",
    "\n",
    "                    # And update size of the tree.\n",
    "                    self.n_nodes += 2\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X    (Fortan-style ndarray of float64): Pre-processed training data.\n",
    "            y                     (ndarray of int): Training labels.\n",
    "            split_point_idxs      (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                    Shape: (n training samples, n features).\n",
    "            unique_vals_feats (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                    Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats      (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows                            (list): Indices of the rows to be used for training. \n",
    "                                                    All rows used if empty.\n",
    "            features                        (list): Column indices of training features that will be used.\n",
    "                                                    All features used if empty.    \n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        # Determine # classes found among all training samples.\n",
    "        root_cc = np.unique(y, return_counts=True)[1] \n",
    "        self.n_class = root_cc.size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "\n",
    "        self.n_samples = len(self.rows)\n",
    "        self.n_features = len(self.features)\n",
    "        \n",
    "        # Store the num unique vals for each numerical feat and\n",
    "        # find the maximum cardinality of all features.\n",
    "        self.n_unique_vals_feats = n_unique_vals_feats\n",
    "        self.max_n_unique_feat_vals = self.n_unique_vals_feats.max()\n",
    "        \n",
    "        # To track how often the \"small Q\" and \"large Q\" splitters are used.\n",
    "        self.num_small_Q = 0\n",
    "        self.num_large_Q = 0\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        init_capacity = 15\n",
    "        \n",
    "         # Allocate tree memory.\n",
    "        self._increase_mem_capacity(init_capacity)\n",
    "        self.mem_capacity = init_capacity\n",
    "        \n",
    "        # And sum the class weights of all the root node's samples in\n",
    "        # order to know minimum total weight a leaf must have (which\n",
    "        # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "        root_wcc = root_cc*self.class_weights\n",
    "        self.min_weight_leaf = self.min_weight_fraction_leaf*root_wcc.sum()\n",
    "        \n",
    "        # Initialize the random number generator.\n",
    "        self._rng = get_random_generator(self.seed)\n",
    "        \n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X, y, split_point_idxs, unique_feat_vals)\n",
    "        return self\n",
    "        \n",
    "    def _next_node(self, nxt): return self.nodes[nxt]\n",
    "       \n",
    "    def _get_leaf_idx(self, i, X):\n",
    "        root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[:,leaf.feat][i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else:\n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        n_preds = X.shape[0]\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        for i in range(n_preds):\n",
    "            preds[i] = self.nodes[self._get_leaf_idx(i, X)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        for i in range(n_probs):\n",
    "            idx = self._get_leaf_idx(i, X)\n",
    "            for j in range(self.n_class):\n",
    "                wcc[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Wright SmallQ/Large Q Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeSmallLargeQ(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size # Number of nodes in the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.num_small_Q # Number of times the \"small Q\" splitter called during tree fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.num_large_Q # Number of times the \"large Q\" splitter called during tree fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752808988764045"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 ms Â± 6.77 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "503 Âµs Â± 6.99 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wright SmallQ/LargeQ Decision Tree Cython Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "# distutils: language = c++\n",
    "# distutils: extra_compile_args = -std=c++11\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t # Signed, same as ssize_t in C. See MSeifert's SO answer: https://stackoverflow.com/a/46416257/8628758\n",
    "cimport cython\n",
    "from libc.math cimport log as ln\n",
    "from libc.stdlib cimport realloc, free\n",
    "from libc.string cimport memcpy\n",
    "from libc.string cimport memset\n",
    "from libcpp.stack cimport stack\n",
    "\n",
    "# For C++ random number generation.\n",
    "from libc.stdint cimport uint_fast32_t \n",
    "\n",
    "# Swap helper func for sorting.\n",
    "cdef inline void swap(DTYPE_t* items, SIZE_t i, SIZE_t j) nogil:\n",
    "    items[i], items[j] = items[j], items[i]\n",
    "\n",
    "# Quicksort helpers\n",
    "\n",
    "cdef inline void med_three(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find the median-of-three pivot point of the second through final \n",
    "    items of a list of numbers. Once identified, the pivot is moved to \n",
    "    the front of the list. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t middle = <int>(first + (last - first)/2)\n",
    "    cdef SIZE_t second = first + 1\n",
    "    last -= 1\n",
    "    if items[second] < items[middle]:\n",
    "        if items[middle] < items[last]:\n",
    "            swap(items, first, middle)    \n",
    "        elif items[second] < items[last]:\n",
    "            swap(items, first, last)         \n",
    "        else:                        \n",
    "            swap(items, first, second)\n",
    "    elif items[second] < items[last]:\n",
    "        swap(items, first, second)\n",
    "    elif items[middle] < items[last]:\n",
    "        swap(items, first, last)\n",
    "    else:\n",
    "        swap(items, first, middle)\n",
    "\n",
    "cdef inline SIZE_t partition(DTYPE_t* items, SIZE_t first, SIZE_t last, SIZE_t pivot) nogil:\n",
    "    \"\"\"Group numbers less than the pivot value together on the left and\n",
    "    those that are greater on the right. Find the index that separates\n",
    "    these two groups, which will belong to the first item that is greater\n",
    "    than or equal to the pivot. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1885\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        pivot      : Index holding the median pivot value.\n",
    "        \n",
    "    Returns:\n",
    "        Index of cut point used to partition the items into two smaller sequences.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while first < last and items[first] < items[pivot]:\n",
    "            first += 1                      # Get index of first item greater than or equal to median-of-three pivot. \n",
    "        last -= 1\n",
    "        while items[pivot] < items[last]:\n",
    "            last -= 1                       # Get index of last item less than or equal to the pivot.\n",
    "        if not (first < last): \n",
    "            return first                    # After swaps are done, return index of first item in right partition.\n",
    "        \n",
    "        swap(items, first, last)            # Swap the first item greater than or equal to the pivot with the\n",
    "                                            # last item less than or equal to the pivot. \n",
    "        first += 1\n",
    "\n",
    "cdef inline void insertion_sort(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Follows the spirit of the Numpy implementation at: \n",
    "        https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t i\n",
    "    cdef SIZE_t j\n",
    "    cdef SIZE_t k\n",
    "    cdef DTYPE_t val\n",
    "    for i in range(first+1, last):\n",
    "        j = i\n",
    "        k = i - 1\n",
    "        val = items[i]\n",
    "        while (j > first) and val < items[k]:\n",
    "            items[j] = items[k]\n",
    "            j-=1\n",
    "            k-=1\n",
    "        items[j] = val\n",
    "\n",
    "# Heapsort\n",
    "\n",
    "cdef inline void sift_down(DTYPE_t* items, SIZE_t start, int n, SIZE_t p, \n",
    "                           SIZE_t c, DTYPE_t val) nogil:\n",
    "    \"\"\"Swap a heap item with one of its children if that child's value is \n",
    "    greater than or equal to that parent's value. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L61\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "        p    : Index of the parent.\n",
    "        c    : Index of the parent's first (left) child.\n",
    "        val  : The parent's value.\n",
    "    \"\"\"\n",
    "    while c < n:    # Look at the descendents of current parent, `p`.\n",
    "        if c < n-1 and items[start + c] < items[start + c + 1]: # Find larger of the first and second children.\n",
    "            c += 1\n",
    "        if val < items[start + c]: # If child greater than parent, swap child and parent.\n",
    "            items[start + p] = items[start + c]\n",
    "            p = c   # Current greater child becomes the parent.\n",
    "            c += c  # Look at this child's child, if it exists.\n",
    "        else:\n",
    "            break \n",
    "    items[start + p] = val\n",
    "\n",
    "cdef inline void sort_heap(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Sort a binary max heap of numbers. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L77\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        val = items[start + n]\n",
    "        items[start + n] = items[start]\n",
    "        sift_down(items, start, n, 0, 1, val)\n",
    "\n",
    "cdef inline void heapify(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Turn a list of items into a binary max heap. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L59\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t p\n",
    "    cdef SIZE_t last_p = (n-2)//2\n",
    "    for p in range(last_p, -1, -1):\n",
    "        val = items[start + p] # value of last parent\n",
    "        sift_down(items, start, n, p, 2*p + 1, val)\n",
    "\n",
    "cdef inline void heapsort(DTYPE_t* items, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Applies the heapsort algorithm to sort a list of items from least to greatest. \n",
    "    From Williams, 1964.\n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    heapify(items, start, n)\n",
    "    sort_heap(items, start, n)\n",
    "    \n",
    "# Introsort \n",
    "\n",
    "cdef void introsort_loop(DTYPE_t* items, SIZE_t first, SIZE_t last, int depth) nogil:\n",
    "    \"\"\"The recursive heart of the introsort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        depth      : Current recursion depth.\n",
    "    \"\"\"\n",
    "    cdef int MIN_SIZE_THRESH = 16\n",
    "    cdef SIZE_t cut\n",
    "    while last-first > MIN_SIZE_THRESH:\n",
    "        if depth == 0:\n",
    "            heapsort(items, first, last-first)\n",
    "        depth -= 1\n",
    "        med_three(items, first, last)\n",
    "        cut = partition(items, first+1, last, first)\n",
    "        introsort_loop(items, cut, last, depth)\n",
    "        last = cut\n",
    "\n",
    "# Log base-2 helper function. From Sklearn's implementation at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_utils.pyx#L7\n",
    "cdef inline DTYPE_t log2(DTYPE_t x) nogil:\n",
    "    return ln(x) / ln(2.0)\n",
    "\n",
    "cdef void introsort(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Implementation as described in Musser, 1997. Switches to heapsort\n",
    "    when max recursion depth exceeded. Otherwise uses median-of-three \n",
    "    quicksort (Bentley & McIlroy, 1993) with all the usual optimizations:\n",
    "        - Swap equal elements.\n",
    "        - Only process partitions longer than the minimum size threshold.\n",
    "        - When a new partition is made, recurse on the smaller half and \n",
    "          iterate over the larger half.\n",
    "        - Make a final pass with insertion sort over the entire list.\n",
    "\n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef int max_depth = 2 * <int>log2(last-first)\n",
    "    introsort_loop(items, first, last, max_depth)\n",
    "    insertion_sort(items, first, last)\n",
    "    \n",
    "cdef SIZE_t sort_unique(DTYPE_t* items, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Sort a 1-d array of numbers in-place using introsort and \n",
    "    place the unique values in consecutive ascending order at \n",
    "    the beginning of the array.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        first, last: The range of items to be sorted. \n",
    "        \n",
    "    Returns: \n",
    "        Number of unique items.\n",
    "    \"\"\"\n",
    "    cdef SIZE_t i = 1\n",
    "    cdef SIZE_t j = 1\n",
    "    introsort(items, first, last)\n",
    "    while i < last-first:\n",
    "        if items[i] == items[i-1]:\n",
    "            i += 1\n",
    "        else:\n",
    "            if i - j < 1:\n",
    "                j += 1\n",
    "                i += 1\n",
    "            else:\n",
    "                items[j] = items[i]\n",
    "                j += 1\n",
    "                i += 1\n",
    "    return j\n",
    "    \n",
    "# For convenient memory reallocation.\n",
    "ctypedef fused realloc_t:\n",
    "    SIZE_t\n",
    "    DTYPE_t\n",
    "    Node\n",
    "\n",
    "cdef inline realloc_t* safe_realloc(realloc_t* ptr, SIZE_t n_items) nogil except *:\n",
    "    # Inspired by Sklearn's safe_realloc() func. However, thankfully\n",
    "    # Cython now no longer requires us to send a pointer to a pointer\n",
    "    # in order to prevent crashes.\n",
    "    cdef realloc_t elem = ptr[0]\n",
    "    cdef SIZE_t n_bytes = n_items * sizeof(elem)\n",
    "    # Make sure we're not trying to allocate too much memory.\n",
    "    if n_bytes/sizeof(elem) != n_items:\n",
    "        with gil:\n",
    "            raise MemoryError(f\"Overflow error: unable to allocate {n_bytes} bytes.\")       \n",
    "    cdef realloc_t* res_ptr = <realloc_t *> realloc(ptr, n_bytes)\n",
    "    with gil:\n",
    "        if not res_ptr: raise MemoryError()\n",
    "    return res_ptr\n",
    "\n",
    "# C++ random number generator. Not yet a part of a Cython release so\n",
    "# pasted in from: \n",
    "#     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/Cython/Includes/libcpp/random.pxd#L1\n",
    "cdef extern from \"<random>\" namespace \"std\" nogil:\n",
    "    cdef cppclass random_device:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        random_device() except +\n",
    "        result_type operator()() except +\n",
    "\n",
    "    cdef cppclass mt19937:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        mt19937() except +\n",
    "        mt19937(result_type seed) except +\n",
    "        result_type operator()() except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        void discard(size_t z) except +\n",
    "        void seed(result_type seed) except +\n",
    "\n",
    "    cdef cppclass uniform_int_distribution[T]:\n",
    "        ctypedef T result_type\n",
    "        uniform_int_distribution() except +\n",
    "        uniform_int_distribution(T, T) except +\n",
    "        result_type operator()[Generator](Generator&) except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        \n",
    "# Info for any node that will eventually be split or made into a leaf.\n",
    "# Similar to what Sklearn does at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/a2c4d8b1f4471f52a4fcf1026f495e637a472568/sklearn/tree/_tree.pyx#L126\n",
    "cdef struct StackEntry:\n",
    "    SIZE_t start\n",
    "    SIZE_t end\n",
    "    SIZE_t node_id\n",
    "    SIZE_t parent_id\n",
    "    SIZE_t n_const_feats\n",
    "\n",
    "# To compare node splits.\n",
    "cdef struct Split:\n",
    "    SIZE_t feat\n",
    "    DTYPE_t thresh\n",
    "    DTYPE_t score  \n",
    "\n",
    "# Vital characteristics of a node. Set when it's added to the tree.\n",
    "cdef struct Node:\n",
    "    SIZE_t l_child # idx of left child, -1 if leaf\n",
    "    SIZE_t r_child # idx of right child, -1 if leaf\n",
    "    SIZE_t feat    # col idx of split feature, -1 if leaf\n",
    "    DTYPE_t thresh # double split threshold, NAN if leaf\n",
    "    SIZE_t label   # class label if leaf, -1 if non-leaf.\n",
    "    \n",
    "cdef inline SIZE_t find_first(DTYPE_t* items, DTYPE_t value, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find first occurrence of an element in a vector of sorted \n",
    "       (ascending order) elements.\n",
    "       \n",
    "    Uses same algorithm as Python's bisect_left() function:\n",
    "        https://github.com/python/cpython/blob/8fd2d36c1c6da78b2402fcb8bcefdad8428c8bc3/Lib/bisect.py#L68\n",
    "        \n",
    "    Arguments:\n",
    "        items      : The pre-sorted elements to be searched over.\n",
    "        value      : The value to search for.\n",
    "        first, last: The range of items to be searched.\n",
    "        \n",
    "    Returns:\n",
    "        Index of the first element in `items` that equals `value`.\n",
    "        \n",
    "        If no such element exists in `items` the returned index\n",
    "        merely indicates where the element would reside where it\n",
    "        present in the sorted vector.\n",
    "    \"\"\"\n",
    "    cdef SIZE_t mid\n",
    "    while first < last:\n",
    "        mid = (first + last) // 2\n",
    "        if items[mid] < value:\n",
    "            first = mid + 1\n",
    "        else:\n",
    "            last = mid\n",
    "    return first\n",
    "\n",
    "cdef inline void find_num_split_smallQ(DTYPE_t* X, SIZE_t* rows, DTYPE_t* node_unique_vals_feat, \n",
    "                                       SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, \n",
    "                                       SIZE_t n_samples, SIZE_t n_class, SIZE_t min_samples_leaf, \n",
    "                                       DTYPE_t min_weight_leaf, DTYPE_t* c_wts, DTYPE_t* l_wcc, \n",
    "                                       DTYPE_t* r_wcc, DTYPE_t* parent_wcc, Split* best_split, \n",
    "                                       SIZE_t current_feat, DTYPE_t parent_num, DTYPE_t parent_den, \n",
    "                                       SIZE_t node_n_unique_vals_feat, SIZE_t* split_counts_raw, \n",
    "                                       DTYPE_t* split_class_counts_wt, bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's SmallQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L233\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        X                      : Training data. Shape: (n train samples, n features).\n",
    "        rows                   : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        node_unique_vals_feat  : The sorted unique feature values of the samples in the parent\n",
    "                                 node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                 : All training labels. Shape: (n training samples,).\n",
    "        node_start             : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent               : Number of samples in the parent node.\n",
    "        n_samples              : Number of samples in the training data.\n",
    "        n_class                : Number of unique classes in the training set.\n",
    "        min_samples_leaf       : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf        : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                  : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                  : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                  : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc             : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split             : Holds the feature, threshold and impurity\n",
    "                                 score of the parent node's current best split.\n",
    "        current_feat           : Column index of feature under investigation.\n",
    "        parent_num             : Numerator of parent node's impurity score.\n",
    "        parent_den             : Denominator of parent node's impurity score.\n",
    "        node_n_unique_vals_feat: Number of unique values for one feature found among \n",
    "                                 the node's samples.\n",
    "        split_counts_raw       : Stores sample counts found at each unique split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt  : Stores weighted class counts of each class at each unique\n",
    "                                 split point of a given feature in a given node. Shape:\n",
    "                                 (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "        current_feat_const     : Whether current splitting feature is constant for all eligible split \n",
    "                                 thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used while tabulating sample and weighted\n",
    "    # class counts at all unique split points.\n",
    "    cdef SIZE_t row, label, split_point_idx\n",
    "    cdef DTYPE_t value\n",
    "    \n",
    "    # Variables to track progress during the split search.\n",
    "    cdef SIZE_t n_left, n_right, i, j\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, wt, score, mid\n",
    "    \n",
    "    # Tabulate both the sample counts at all possible split points\n",
    "    # as well as weighted class counts at each split point.\n",
    "    memset(&split_counts_raw[0], 0, sizeof(SIZE_t)*node_n_unique_vals_feat)\n",
    "    memset(&split_class_counts_wt[0], 0, sizeof(DTYPE_t)*node_n_unique_vals_feat*n_class)\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        value = X[current_feat*n_samples + row]\n",
    "        label = labels[row]\n",
    "        split_point_idx = find_first(node_unique_vals_feat, value, 0, node_n_unique_vals_feat)\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += c_wts[label] \n",
    "    \n",
    "    # To keep track of num amples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(node_n_unique_vals_feat - 1):\n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "\n",
    "        l_num, r_num = 0., 0. # To calculate numerators of proxy gini scores.\n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            l_den += wt\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = node_unique_vals_feat[i]/2. + node_unique_vals_feat[i+1]/2. \n",
    "            if mid == node_unique_vals_feat[i+1]: mid = node_unique_vals_feat[i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "\n",
    "cdef inline void find_num_split_largeQ(SIZE_t* rows, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats, \n",
    "                                       SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, SIZE_t n_samples,\n",
    "                                       SIZE_t n_class, SIZE_t min_samples_leaf, DTYPE_t min_weight_leaf, \n",
    "                                       DTYPE_t* c_wts, DTYPE_t* l_wcc, DTYPE_t* r_wcc, DTYPE_t* parent_wcc, \n",
    "                                       Split* best_split, SIZE_t current_feat, DTYPE_t parent_num, \n",
    "                                       DTYPE_t parent_den, SIZE_t n_unique_vals_feat, SIZE_t max_n_unique_vals, \n",
    "                                       SIZE_t* split_counts_raw, DTYPE_t* split_counts_wt, \n",
    "                                       DTYPE_t* split_class_counts_wt, bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's LargeQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows                   : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        split_point_idxs       : All numerical feature split-point locations for all rows.\n",
    "                                 Shape: (n training samples, n features).\n",
    "        unique_vals_feats      : Columns contain sorted unique values for all features. \n",
    "                                 Shape: (max cardinality of all feats, n features).\n",
    "                                 node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                 : All training labels. Shape: (n training samples,).\n",
    "        node_start             : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent               : Number of samples in the parent node.\n",
    "        n_samples              : Number of samples in the training data.\n",
    "        n_class                : Number of unique classes in the training set.\n",
    "        min_samples_leaf       : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf        : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                  : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                  : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                  : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc             : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split             : Holds the feature, threshold and impurity\n",
    "                                 score of the parent node's current best split.\n",
    "        current_feat           : Column index of feature under investigation.\n",
    "        parent_num             : Numerator of parent node's impurity score.\n",
    "        parent_den             : Denominator of parent node's impurity score.\n",
    "        n_unique_vals_feat     : Number of unique values for one feature found among \n",
    "                                 all training samples.\n",
    "        max_n_unique_vals      : Maximum cardinality of all features in dataset.\n",
    "        split_counts_raw       : Stores sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_counts_wt        : Stores weighted sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt  : Stores weighted class counts of each class at each\n",
    "                                 split point of a given feature in a given node. Shape:\n",
    "                                 (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "        current_feat_const     : Whether current splitting feature is constant for all eligible split \n",
    "                                 thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used while tabulating raw and weighted sample counts, \n",
    "    # as well as weighted class counts at all unique split points.\n",
    "    cdef SIZE_t row, label, split_point_idx\n",
    "    \n",
    "    # Make sure node's samples aren't all constant for feature.\n",
    "    cdef SIZE_t n_splits_node = 0\n",
    "    \n",
    "    # Variables to track progress during the split search.\n",
    "    cdef SIZE_t n_left, n_right, i, j, k\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, wt, score, mid\n",
    "    \n",
    "    # Tabulate sample counts, weighted counts, and weighted class counts at \n",
    "    # each split point. Values at split points not belonging to node's rows\n",
    "    # will remain zero.\n",
    "    memset(split_counts_raw, 0, sizeof(SIZE_t)*n_unique_vals_feat)\n",
    "    memset(split_counts_wt, 0, sizeof(DTYPE_t)*n_unique_vals_feat)\n",
    "    memset(split_class_counts_wt, 0, sizeof(SIZE_t)*n_unique_vals_feat*n_class)\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        label = labels[row]\n",
    "        wt = c_wts[label]\n",
    "        split_point_idx = split_point_idxs[n_samples*current_feat + row]\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_counts_wt[split_point_idx] += wt\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += wt\n",
    "        \n",
    "    # If feat is constant for the node.\n",
    "    for i in range(n_unique_vals_feat):\n",
    "        if split_counts_raw[i] > 0: n_splits_node += 1\n",
    "    if n_splits_node < 2: return\n",
    "    \n",
    "    # To keep track of num samples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(n_unique_vals_feat - 1):\n",
    "        if split_counts_raw[i] == 0: continue # Move to next split-point if no samples at this one.\n",
    "        \n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "        if n_right == 0: return # Make sure to stop search when right child empty.\n",
    "        \n",
    "        # Calculate denominators of proxy gini scores.\n",
    "        l_den += split_counts_wt[i]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Calculate numerators of proxy gini scores.\n",
    "        l_num, r_num = 0., 0. \n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Find raw feature value of sample(s) at next-closest split-point.\n",
    "            k = i+1\n",
    "            while split_counts_raw[k] == 0: k+=1\n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = (unique_vals_feats[current_feat*max_n_unique_vals + i]/2. + \n",
    "                   unique_vals_feats[current_feat*max_n_unique_vals + k]/2.) \n",
    "            if mid == unique_vals_feats[current_feat*max_n_unique_vals + k]: \n",
    "                mid = unique_vals_feats[current_feat*max_n_unique_vals + i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "\n",
    "cdef inline SIZE_t make_num_split(SIZE_t* rows, DTYPE_t* X, StackEntry* node_info, Split* best_split, \n",
    "                                SIZE_t n_samples) nogil:\n",
    "    cdef SIZE_t p, p_end\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[best_split.feat*n_samples + rows[p]] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p] \n",
    "    return p\n",
    "\n",
    "# Necessary constants.\n",
    "cdef DTYPE_t NEG_INF = -np.inf\n",
    "cdef DTYPE_t NAN = np.nan\n",
    "cdef DTYPE_t Q_THRESHOLD = 0.02\n",
    "            \n",
    "cdef class _DecisionTree:\n",
    "    # Class attributes.\n",
    "    cdef SIZE_t seed\n",
    "    cdef mt19937 rng\n",
    "    cdef SIZE_t mem_capacity\n",
    "    cdef SIZE_t n_samples\n",
    "    cdef SIZE_t n_features\n",
    "    cdef SIZE_t n_class\n",
    "    cdef SIZE_t m\n",
    "    cdef SIZE_t min_samples_leaf, \n",
    "    cdef DTYPE_t min_weight_fraction_leaf\n",
    "    cdef DTYPE_t min_weight_leaf\n",
    "    cdef SIZE_t n_nodes\n",
    "    cdef SIZE_t max_n_unique_feat_vals\n",
    "    cdef SIZE_t* n_unique_vals_feats\n",
    "    cdef SIZE_t* rows\n",
    "    cdef SIZE_t* features\n",
    "    cdef DTYPE_t* class_weights\n",
    "    cdef Node* nodes\n",
    "    cdef DTYPE_t* weighted_class_counts\n",
    "    def __cinit__(self, SIZE_t m, SIZE_t min_samples_leaf, DTYPE_t min_weight_fraction_leaf, SIZE_t seed): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                       : Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                      of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                    : A seed for the C++ mt19937 32bit int random generator. \n",
    "                                      Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m, self.min_samples_leaf = m, min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = NULL\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = NULL \n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        free(self.nodes)\n",
    "        free(self.weighted_class_counts)\n",
    "        \n",
    "    property size:\n",
    "        def __get__(self):\n",
    "            return self.n_nodes\n",
    "    \n",
    "    property left_children:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].l_child\n",
    "            return out\n",
    "\n",
    "    property right_children:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].r_child\n",
    "            return out\n",
    "        \n",
    "    property split_features: \n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].feat\n",
    "            return out\n",
    "        \n",
    "    property split_thresholds:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].thresh\n",
    "            return out\n",
    "        \n",
    "    property weighted_cc:\n",
    "        def __get__(self):\n",
    "            cdef SIZE_t out_size = self.n_nodes*self.n_class\n",
    "            out = np.empty(out_size, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(out_size):\n",
    "                    out_view[i] = self.weighted_class_counts[i]\n",
    "            out.resize(self.n_nodes, self.n_class)\n",
    "            return out\n",
    "    \n",
    "    property labels:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].label\n",
    "            return out\n",
    "    \n",
    "    cdef void _increase_mem_capacity(self, SIZE_t new_capacity) nogil:\n",
    "        self.nodes = safe_realloc(self.nodes, new_capacity)\n",
    "        self.weighted_class_counts = safe_realloc(self.weighted_class_counts, self.n_class*new_capacity)\n",
    "    \n",
    "    cdef void _make_leaf(self, Node* leaf_node, SIZE_t* y, SIZE_t node_start, SIZE_t node_id, \n",
    "                         SIZE_t n_classes_node, SIZE_t* max_wt_classes) nogil:\n",
    "        # Class with largest wcc becomes leaf node's label. Break ties with a random choice.\n",
    "        cdef SIZE_t label\n",
    "        cdef DTYPE_t max_wt = 0.\n",
    "        cdef SIZE_t lb = 0\n",
    "        cdef SIZE_t ub = -1\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t i, j\n",
    "        # If all node's samples have the same class.\n",
    "        if n_classes_node == 1:\n",
    "            label = y[self.rows[node_start]]\n",
    "        else:\n",
    "            # Otherwise find label with max weighted class count for the node.\n",
    "            for i in range(self.n_class):\n",
    "                max_wt = max(max_wt, self.weighted_class_counts[node_id*self.n_class + i])\n",
    "            # See if multiple classes share this max count.\n",
    "            for i in range(self.n_class):\n",
    "                if self.weighted_class_counts[node_id*self.n_class + i] == max_wt:\n",
    "                    ub += 1\n",
    "                    max_wt_classes[ub] = i\n",
    "            # If so, randomly choose leaf's label from among those classes.\n",
    "            if ub > 0:\n",
    "                dist = uniform_int_distribution[SIZE_t](lb, ub) # Choose an int w/in range lb, ub, inclusive.\n",
    "                j = dist(self.rng)\n",
    "                label = max_wt_classes[j]\n",
    "            else:\n",
    "                label = max_wt_classes[lb]\n",
    "        leaf_node.l_child = -1\n",
    "        leaf_node.r_child = -1    \n",
    "        leaf_node.feat = -1  \n",
    "        leaf_node.thresh = NAN\n",
    "        leaf_node.label = label \n",
    "\n",
    "    cdef _grow_tree(self, DTYPE_t* X, SIZE_t* y, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats):\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        cdef stack[StackEntry] node_stack\n",
    "\n",
    "        #####################################################################\n",
    "        # Variables containing info of the node currently being investigated.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t start, end, node_id, parent_id, n_consts, n_samples_node\n",
    "        cdef DTYPE_t* node_wcc = NULL\n",
    "        cdef StackEntry node_info\n",
    "        cdef Node* node = NULL\n",
    "        \n",
    "        # Holds child node info if the current node gets split.\n",
    "        cdef SIZE_t l_child_id, r_child_id\n",
    "        cdef Node* l_child_node = NULL\n",
    "        cdef Node* r_child_node = NULL\n",
    "        \n",
    "        #####################################################################\n",
    "        # For finding the best split.\n",
    "        #####################################################################\n",
    "        cdef Split best_split\n",
    "        cdef DTYPE_t* l_wcc = NULL\n",
    "        cdef DTYPE_t* r_wcc = NULL\n",
    "        cdef DTYPE_t sum_node_wcc_sqr, sum_node_wcc # Parent node's proxy Gini score num and den.\n",
    "        cdef SIZE_t split_pos\n",
    "        \n",
    "        # Indicates a feature has been discovered to be constant during a\n",
    "        # split search within the search range permitted by min_samples_leaf \n",
    "        # and min_weight_leaf.\n",
    "        cdef bint current_feat_const \n",
    "        \n",
    "        # Determines whether to use SmallQ or LargeQ splitting. \n",
    "        cdef DTYPE_t q\n",
    "        cdef SIZE_t n_unique_vals_feat\n",
    "\n",
    "        # Following two buffers used for SmallQ splitting.\n",
    "        \n",
    "        # Create a C-contiguous array of doubles to hold feature values of a \n",
    "        # given node's samples. Using Numpy to allocate memory to longer \n",
    "        # vectors is often faster than using realloc().\n",
    "        cdef DTYPE_t[::1] items_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* items = &items_buffer[0]\n",
    "        cdef SIZE_t r\n",
    "        \n",
    "        # An array to contain unique split points for a feature at a given node.\n",
    "        cdef DTYPE_t[::1] node_unique_vals_feat_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* node_unique_vals_feat = &node_unique_vals_feat_buffer[0]\n",
    "        cdef SIZE_t node_n_unique_vals_feat\n",
    "        \n",
    "        # Three 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value.\n",
    "        # Used for both SmallQ and LargeQ splitting, except for split_counts_wt,\n",
    "        # which is just used for LargeQ.\n",
    "        \n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        cdef SIZE_t[::1] split_counts_raw_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        cdef SIZE_t* split_counts_raw = &split_counts_raw_buffer[0]\n",
    "        # Weighted sample counts at each split-point.\n",
    "        cdef DTYPE_t[::1] split_counts_wt_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        cdef DTYPE_t* split_counts_wt = &split_counts_wt_buffer[0]\n",
    "        # Weighted class counts for each split-point.\n",
    "        cdef DTYPE_t[::1] split_class_counts_wt_buffer = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64) \n",
    "        cdef DTYPE_t* split_class_counts_wt = &split_class_counts_wt_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For random feature selection (w/out replacement) and keeping track \n",
    "        # of nodes' constant features. \n",
    "        #####################################################################\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t lb, ub, idx, feat_idx, n_drawn_feats, n_new_consts, n_total_consts\n",
    "        cdef SIZE_t[::1] features_buffer = np.empty(self.n_features, dtype=np.intp) \n",
    "        cdef SIZE_t* features = &features_buffer[0]\n",
    "        cdef SIZE_t[::1] constant_features_buffer = np.empty(self.n_features, dtype=np.intp)\n",
    "        cdef SIZE_t* constant_features = &constant_features_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For determining whether node should be a leaf.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t i, c, cc, n_classes_node, row, label\n",
    "        cdef DTYPE_t wcc, wt\n",
    "        # Stores classes that share a leaf's max class wt. When two or more \n",
    "        # present, leaf label randomly chosen from these classes\n",
    "        cdef SIZE_t* max_wt_classes = NULL\n",
    "        \n",
    "        with nogil:\n",
    "            # Allocate memory to pointers.\n",
    "            l_wcc = safe_realloc(l_wcc, self.n_class)\n",
    "            r_wcc = safe_realloc(r_wcc, self.n_class)\n",
    "            node_wcc = safe_realloc(node_wcc, self.n_class)\n",
    "            max_wt_classes = safe_realloc(max_wt_classes, self.n_class*sizeof(SIZE_t))\n",
    "            # Fill with feature column indices so we can track constant feats.\n",
    "            memcpy(features, self.features, self.n_features* sizeof(SIZE_t))\n",
    "            \n",
    "            # Push root node onto the LIFO stack.\n",
    "            node_stack.push({\"start\": 0, \"end\": self.n_samples, \"node_id\": 0, \n",
    "                             \"parent_id\": 0, \"n_const_feats\": 0})\n",
    "            self.n_nodes = 1\n",
    "            while not node_stack.empty():\n",
    "                node_info = node_stack.top()\n",
    "                node_stack.pop()\n",
    "                start, end = node_info.start, node_info.end\n",
    "                node_id, parent_id = node_info.node_id, node_info.parent_id # TODO: `parent_id` unused; is it necessary?\n",
    "                n_consts = node_info.n_const_feats\n",
    "                n_samples_node = end-start\n",
    "                node = &self.nodes[node_id]\n",
    "                \n",
    "                # Tabulate the current node's weighted class counts.\n",
    "                #\n",
    "                # Implementation detail #1: I tried storing the l and r child wt class cts\n",
    "                # of nodes' best splits so that this tabulation wouldn't need to be \n",
    "                # performed for each node. But found there was virtually no speed improvement\n",
    "                # to justify the more complicated code required to store and update these \n",
    "                # values during the best split search.\n",
    "                #\n",
    "                # Implementation detail #2: Setting aside a block of memory to \n",
    "                # store the current node's wt class cts and passing a pointer to\n",
    "                # this block to the split search function sped up training by 8%\n",
    "                # compared to passing a ptr to the location of node's wt class cts \n",
    "                # in the self.weighted_class_counts array.\n",
    "                memset(node_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                sum_node_wcc, sum_node_wcc_sqr = 0., 0.\n",
    "                for i in range(n_samples_node):\n",
    "                    row = self.rows[start + i]\n",
    "                    label = y[row]\n",
    "                    wt = self.class_weights[label]\n",
    "                    # Compute the node's proxy gini numerator and denominator while we're at it.\n",
    "                    sum_node_wcc_sqr += wt*(2*node_wcc[label] + wt) # numerator\n",
    "                    sum_node_wcc += wt                              # denominator\n",
    "                    node_wcc[label] += wt\n",
    "                memcpy(&self.weighted_class_counts[node_id*self.n_class], node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                \n",
    "                # Make a leaf if required to do so. \n",
    "                n_classes_node = 0\n",
    "                for c in range(self.n_class):\n",
    "                    wcc = node_wcc[c]\n",
    "                    if wcc > 0: n_classes_node += 1\n",
    "                if n_classes_node == 1:                   \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "\n",
    "                # Otherwise split the node.\n",
    "                else:\n",
    "                    # Initialize stats for best split of node.\n",
    "                    best_split.feat = -1\n",
    "                    best_split.thresh = 0.\n",
    "                    best_split.score = NEG_INF\n",
    "\n",
    "                    # Ensure feats drawn w/out replacement.\n",
    "                    n_drawn_feats = 0\n",
    "                    n_new_consts = 0\n",
    "                    n_total_consts = n_consts\n",
    "                    lb = 0                      # Range in `features` array from which we \n",
    "                    ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "                        \n",
    "                    while n_drawn_feats < self.m:\n",
    "                        n_drawn_feats += 1\n",
    "\n",
    "                        # Breiman & Cutler's original Fortran random forests implementation \n",
    "                        # allows for known constant features to be drawn during a split-search.\n",
    "                        # I follow their example, as I believe that doing so allows individual \n",
    "                        # trees to be less correlated with each other. Since I don't pre-sort\n",
    "                        # features, I would prefer not to have to sort any more features than\n",
    "                        # necessary, and so I've adopted the technique Sklearn uses to track \n",
    "                        # constant features:\n",
    "                        #     https://github.com/scikit-learn/scikit-learn/blob/dbe39454f766ebefc3219f2c1871ac1774316532/sklearn/tree/_splitter.pyx#L310\n",
    "                        # \n",
    "                        # The idea is that feature idxs in `features` are organized into two sections:\n",
    "                        #\n",
    "                        #     [<indices of known constant feats>, <indices of non-constant feats>]\n",
    "                        #\n",
    "                        # As we begin drawing feature indices from this above list, those two sections\n",
    "                        # will each be further sub-divided into two sections:\n",
    "                        # \n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <undrawn non-constant feats>, <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # When we choose a feature that happens to be a known constant, we'll re-locate\n",
    "                        # its idx to the right-end of the first of those four sections. Then we \n",
    "                        # increment the lower bound threshold, `lb`, by one so that we don't re-draw \n",
    "                        # that feature again.\n",
    "                        #\n",
    "                        # Similarly, if we draw a non-constant feature idx, we'll move it to the \n",
    "                        # left-end of the last of the four partitions and reduce the upper bound\n",
    "                        # threshold, `ub`, by one so that the feature idx can't be drawn again\n",
    "                        # during this split-search. \n",
    "                        #\n",
    "                        # One last important detail: sometimes we'll draw a feature that \n",
    "                        # used to be non-constant for ancestor nodes, but will be found to be \n",
    "                        # constant for the current node. When this happens, we relocate its \n",
    "                        # index so that it sits to the right of the known constant feats section.\n",
    "                        # This means our `features` list could have up to five partitions:\n",
    "                        #\n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <newly discovered const feats>, <undrawn non-constant feats>, \n",
    "                        #      <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # Whenever we find a new constant feature, we increment the `n_new_consts`\n",
    "                        # counter by one. We also increment the `n_total_consts` counter by one. \n",
    "                        # During the split-search we have to use `n_total_consts` to keep track of\n",
    "                        # the total number of constant features. n_consts` mustn't be changed\n",
    "                        # because it tells us where the <newly discovered const feats> section\n",
    "                        # of the `features` list begins.\n",
    "\n",
    "                        # One last wrinkle. We subtract the # of newly discovered const feats from  \n",
    "                        # the upper bound before we select an index `i` from the `features` array, \n",
    "                        # and add it back to `i` after `i` has been genereated. This prevents us from \n",
    "                        # re-drawing any of these new const feats again during this split-search.\n",
    "                        dist = uniform_int_distribution[SIZE_t](lb, ub-n_new_consts)\n",
    "                        idx = dist(self.rng)\n",
    "\n",
    "                        # So that we don't draw a known constant feature again this split-search.\n",
    "                        if idx < n_consts:\n",
    "                            features[idx], features[lb] = features[lb], features[idx]\n",
    "                            lb += 1 \n",
    "                            continue\n",
    "\n",
    "                        # So that no new const feats get drawn more than once per split-search.\n",
    "                        idx += n_new_consts\n",
    "\n",
    "                        feat_idx = features[idx]\n",
    "                        \n",
    "                        # Num split points found among training samples for given feat.\n",
    "                        n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                        \n",
    "                        q = n_samples_node/n_unique_vals_feat\n",
    "                        \n",
    "                        # SmallQ Splitting.\n",
    "                        if q < Q_THRESHOLD:\n",
    "                            # Place all samples' feat values into contiguous storage.\n",
    "                            for r in range(n_samples_node):\n",
    "                                # X is a pointer, so have to index into this 2d array in the C way \n",
    "                                # (also keeping in mind that the array is column-major).\n",
    "                                items[r] = X[feat_idx*self.n_samples + self.rows[start + r]]\n",
    "                                \n",
    "                            # Place all unique split points, in ascending order, inside the \n",
    "                            # first <node_n_unique_vals_feat> indices of the items array.\n",
    "                            node_n_unique_vals_feat = sort_unique(items, 0, n_samples_node)\n",
    "\n",
    "                            # Make sure the feature not constant for node's samples.\n",
    "                            if node_n_unique_vals_feat < 2:\n",
    "                                # Move the newly-discovered constant feat to the far right-end\n",
    "                                # of the left half of `features` list holding the known const\n",
    "                                # feats as well as any other const feats newly discovered \n",
    "                                # during this node's split-search.\n",
    "                                features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                                n_new_consts += 1\n",
    "                                n_total_consts += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                # Initialize weighted class counts of right and left children.\n",
    "                                # Right child's counts are initially the same as parent node's.\n",
    "                                memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                                memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "\n",
    "                                # If the feature has an impurity score that's better than the best score \n",
    "                                # found among all other features visited thus far for this node, find_num_split()\n",
    "                                # updates the attributes of the struct containing the node's best split info. \n",
    "                                # \n",
    "                                # But even if a new best score isn't reached, if an impurity score can\n",
    "                                # be calculated at least once during the feature's split search, the\n",
    "                                # following indicator will be toggled off, to indicate that the feature\n",
    "                                # is not constant.\n",
    "                                current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                                find_num_split_smallQ(X, self.rows, items, y, start, n_samples_node, self.n_samples, \n",
    "                                                      self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                      self.class_weights, l_wcc, r_wcc, node_wcc,\n",
    "                                                      &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc,\n",
    "                                                      node_n_unique_vals_feat, split_counts_raw, split_class_counts_wt,\n",
    "                                                      &current_feat_const)\n",
    "                                \n",
    "                        # LargeQ Splitting.\n",
    "                        else:\n",
    "                            memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                            memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                            current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                            find_num_split_largeQ(self.rows, split_point_idxs, unique_vals_feats, y, start, \n",
    "                                                  n_samples_node, self.n_samples, self.n_class, self.min_samples_leaf, \n",
    "                                                  self.min_weight_leaf, self.class_weights, l_wcc, r_wcc, node_wcc, \n",
    "                                                  &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, \n",
    "                                                  self.max_n_unique_feat_vals, split_counts_raw, split_counts_wt, \n",
    "                                                  split_class_counts_wt, &current_feat_const)\n",
    "\n",
    "                        if current_feat_const:\n",
    "                            # The feature may be constant within the search range permitted\n",
    "                            # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                            # the feature is a newly discovered constant.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The feature is non-constant, so we ensure it's not drawn again\n",
    "                            # during this split-search.\n",
    "                            features[idx], features[ub] = features[ub], features[idx]\n",
    "                            ub -= 1 \n",
    "\n",
    "                    # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                    memcpy(&features[0], &constant_features[0], sizeof(SIZE_t)*n_consts)\n",
    "                    memcpy(&constant_features[n_consts], &features[n_consts], sizeof(SIZE_t)*n_new_consts)\n",
    "\n",
    "                    # Make node a leaf if constant for all randomly drawn feats.\n",
    "                    # (# drawn known constant feats + # drawn new constant feats)\n",
    "                    if lb + n_new_consts == n_drawn_feats: \n",
    "                        self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                    else: \n",
    "                        split_pos = make_num_split(self.rows, X, &node_info, &best_split, self.n_samples) \n",
    "\n",
    "                        # Update tree info for node that's getting split.\n",
    "                        l_child_id = self.n_nodes\n",
    "                        r_child_id = l_child_id + 1\n",
    "                        node.l_child = l_child_id\n",
    "                        node.r_child = r_child_id\n",
    "                        node.feat    = best_split.feat\n",
    "                        node.thresh  = best_split.thresh\n",
    "                        node.label   = -1\n",
    "\n",
    "                        # Prepare for the left and right child nodes\n",
    "                        # by increasing tree data memory capacity if\n",
    "                        # necessary.\n",
    "                        if self.n_nodes + 2 > self.mem_capacity:\n",
    "                            # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                            # part of WhozCraig's SO answer at: \n",
    "                            #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                            # Add one after squaring so that the new capacity can\n",
    "                            # contain not only a tree of greater depth, but also\n",
    "                            # the maximum # nodes that that depth could have.\n",
    "                            new_capacity = 2*self.mem_capacity + 1\n",
    "                            self._increase_mem_capacity(new_capacity)\n",
    "                            self.mem_capacity = new_capacity\n",
    "                        \n",
    "                        # Push right child info onto the LIFO stack.\n",
    "                        node_stack.push({\"start\": split_pos, \"end\": end, \"node_id\": r_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "                        # Push left child info onto queue.\n",
    "                        node_stack.push({\"start\": start, \"end\": split_pos, \"node_id\": l_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "\n",
    "                        # And update size of the tree.\n",
    "                        self.n_nodes += 2\n",
    "                        \n",
    "        free(l_wcc)\n",
    "        free(r_wcc)\n",
    "        free(node_wcc)\n",
    "        free(max_wt_classes)\n",
    "    \n",
    "    def fit(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] y,\n",
    "            np.ndarray[SIZE_t, ndim=2, mode=\"fortran\"] split_point_idxs, \n",
    "            np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] n_unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] rows, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] features,\n",
    "            np.ndarray[DTYPE_t, ndim=1, mode=\"c\"] class_weights, SIZE_t n_class): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "            class_weights (1D C-contiguous array of float64): Desired weight for each class. Shape: (`n_class`,).\n",
    "            n_class                                         : Number of classes in training data. \n",
    "        \"\"\"\n",
    "        # Casting the raw data to pointers gives a 17% speed-up compared to getting\n",
    "        # pointer from the ndarray's buffer interface, as recommended by DavidW in \n",
    "        # his SO answer at: https://stackoverflow.com/a/54832269/8628758. e.g.\n",
    "        #     cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        #     cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        # Not worried about unexpected behavior as all ndarrays' contiguousness and\n",
    "        # memory layout enforced prior to this point.\n",
    "        cdef DTYPE_t* X_ptr = <DTYPE_t*> X.data\n",
    "        cdef SIZE_t* y_ptr = <SIZE_t*> y.data\n",
    "        cdef SIZE_t* split_point_idxs_ptr = <SIZE_t*> split_point_idxs.data\n",
    "        cdef DTYPE_t* unique_vals_feats_ptr = <DTYPE_t*> unique_vals_feats.data\n",
    "        self.n_unique_vals_feats = <SIZE_t*> n_unique_vals_feats.data\n",
    "        self.rows = <SIZE_t*> rows.data\n",
    "        self.features = <SIZE_t*> features.data\n",
    "        self.class_weights = <DTYPE_t*> class_weights.data\n",
    "        self.n_class = n_class\n",
    "        self.n_samples = rows.shape[0]\n",
    "        self.n_features = features.shape[0]\n",
    "        cdef random_device rd # Needed when using the C++ mt19937 rng w/out a seed.\n",
    "        \n",
    "        # Get the max cardinality of all numerical feats.\n",
    "        self.max_n_unique_feat_vals = n_unique_vals_feats.max()\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        cdef SIZE_t init_capacity = 15\n",
    "        \n",
    "        cdef SIZE_t i, row, label\n",
    "        cdef DTYPE_t wt\n",
    "        cdef DTYPE_t sum_wts = 0\n",
    "        cdef Node* root_node = NULL\n",
    "        with nogil:\n",
    "            # Allocate memory for the tree.\n",
    "            self._increase_mem_capacity(init_capacity)\n",
    "            self.mem_capacity = init_capacity\n",
    " \n",
    "            # And sum the class weights of all the root node's samples in\n",
    "            # order to know minimum total weight a leaf must have (which\n",
    "            # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "            for i in range(self.n_samples):\n",
    "                row = self.rows[i]\n",
    "                label = y_ptr[row]\n",
    "                wt = self.class_weights[label]\n",
    "                sum_wts += wt\n",
    "            self.min_weight_leaf = self.min_weight_fraction_leaf*sum_wts\n",
    "            \n",
    "            # Initialize the random number generator. Followed example from:\n",
    "            #     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/tests/run/cpp_stl_random.pyx#L16\n",
    "            if self.seed == -1:\n",
    "                self.rng = mt19937(rd()) # If using the random device engine std::random_device.\n",
    "            else:\n",
    "                self.rng = mt19937(self.seed)\n",
    "\n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X_ptr, y_ptr, split_point_idxs_ptr, unique_vals_feats_ptr)\n",
    "    \n",
    "    cdef Node* _next_node(self, SIZE_t nxt) nogil: \n",
    "        return &self.nodes[nxt]\n",
    "    \n",
    "    cdef SIZE_t _get_leaf_idx(self, SIZE_t i, Node* leaf, SIZE_t n, DTYPE_t* X) nogil:\n",
    "        cdef SIZE_t idx\n",
    "        cdef SIZE_t root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[leaf.feat*n + i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else: \n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "    \n",
    "    def predict(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_preds = X.shape[0]\n",
    "        cdef SIZE_t i\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        cdef SIZE_t[::1] preds_view = preds\n",
    "        cdef Node leaf\n",
    "        with nogil:\n",
    "            for i in range(n_preds): \n",
    "                preds_view[i] = self.nodes[self._get_leaf_idx(i, &leaf, n_preds, X_ptr)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions. Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        cdef DTYPE_t[::1] wcc_view = wcc\n",
    "        cdef Node leaf\n",
    "        cdef SIZE_t i, j, idx\n",
    "        with nogil:\n",
    "            for i in range(n_probs):\n",
    "                idx = self._get_leaf_idx(i, &leaf, n_probs, X_ptr)\n",
    "                for j in range(self.n_class):\n",
    "                    wcc_view[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)\n",
    "\n",
    "class DecisionTreeSmallQLargeQCython():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Marvin Wright's SmallQ and LargeQ numerical splitting algorithms:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L173\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights = [], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                         (int): Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C') \n",
    "        if seed is None:\n",
    "            self.seed = -1\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        self._tree = _DecisionTree(self.m, self.min_samples_leaf, self.min_weight_fraction_leaf, self.seed)\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self._tree.size\n",
    "    \n",
    "    @property\n",
    "    def left_children(self): return self._tree.left_children\n",
    "    \n",
    "    @property\n",
    "    def right_children(self): return self._tree.right_children\n",
    "            \n",
    "    @property \n",
    "    def split_features(self): return self._tree.split_features\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): return self._tree.split_thresholds\n",
    "    \n",
    "    @property\n",
    "    def weighted_class_counts(self): return self._tree.weighted_cc\n",
    "    \n",
    "    @property\n",
    "    def labels(self): return self._tree.labels\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "                                                              All rows used if empty.\n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "                                                              All rows used if empty.\n",
    "                                                              \n",
    "        Returns:\n",
    "            DecisionTreeSmallQLargeQCython: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        self.n_class = np.unique(y).size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "            \n",
    "        self._tree.fit(X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, \n",
    "                       self.rows, self.features, self.class_weights, self.n_class)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict_probs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright SmallQ/LargeQ Decision Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeSmallQLargeQCython(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 Âµs Â± 16.1 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.26 Âµs Â± 52.3 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wright's SmallQ/LargeQ splitting runs about 50% slower than LargeQ alone, but is still around 7% faster than Louppe's splitting function. \n",
    "\n",
    "It appears that, at least for the Titanic dataset, using just LargeQ splitting will provide superior speed. However, before I move on to observing whether LargeQ also enjoys a performance advantage on the larger Santander dataset, on a final lark I'd like to see if there's any benefit to combining both Louppe's and Wright's approaches.\n",
    "\n",
    "Let's see what happens when we use Louppe's splitter for SmallQ splits and Wright's LargeQ splitter for all other splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Louppe SmallQ/Wright LargeQ Decision Tree Python Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_THRESHOLD = 0.02\n",
    "\n",
    "def find_num_split_Louppe(rows, items, labels, node_start, n_parent, n_class, \n",
    "                          min_samples_leaf, min_weight_leaf, c_wts, l_wcc, r_wcc,\n",
    "                          parent_wcc, best_split, current_feat, parent_num, parent_den):\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Gilles Louppe's split-finding algorithm:\n",
    "        Page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    \n",
    "    Saves a split's feature idx, threshold, position, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows           (ndarray of int): Indices of all rows in the training set. \n",
    "                                         Shape: (n train samples,).\n",
    "        items      (ndarray of float64): The sorted feature values of the samples in the parent\n",
    "                                         node (beginning at `node_start`). Shape: (n train samples,).\n",
    "        labels         (ndarray of int): All training labels. Shape: (n training samples,).\n",
    "        node_start                (int): Index of the beginning of the parent node in `rows`.\n",
    "        n_parent                  (int): Number of samples in the parent node.\n",
    "        n_class                   (int): Number of unique classes in the training set.\n",
    "        min_samples_leaf          (int): Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf       (float64): Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts      (ndarray of float64): Class weights. Shape: (`n_class`,).\n",
    "        l_wcc      (ndarray of float64): Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc      (ndarray of float64): Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc (ndarray of float64): Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split              (Split): Holds the feature, threshold, position, and impurity\n",
    "                                         score of the parent node's current best split.\n",
    "        current_feat              (int): Column index of feature under investigation.\n",
    "        parent_num            (float64): Numerator of parent node's impurity score.\n",
    "        parent_den            (float64): Denominator of parent node's impurity score.\n",
    "              \n",
    "    Returns: \n",
    "        int: 1 if feature is constant for eligible split-points. 0, otherwise.\n",
    "    \"\"\"\n",
    "    # So that we can iterate across all feature values in the node.\n",
    "    prev_pos, pos = node_start, node_start\n",
    "    node_end = node_start + n_parent\n",
    "    lowest = items[pos]\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    l_num, l_den  = 0., 0.\n",
    "    r_num, r_den, = parent_num, parent_den\n",
    "    \n",
    "    # Whether or not feat is constant within search range permitted\n",
    "    # by min_samples_leaf and min_weight_leaf (0 if no, 1 if yes).\n",
    "    current_feat_const = 1\n",
    "    \n",
    "    # Find the best split and store its score, threshold, position,\n",
    "    # as well as it's children's weighted class counts.\n",
    "    while pos < node_end:\n",
    "        while items[pos] == lowest: # When consecutive items have the same value.\n",
    "            if pos == node_end - 1: # When the final few samples all have the same value.\n",
    "                return current_feat_const\n",
    "            pos+=1\n",
    "        next_lowest = items[pos]\n",
    "        mid = lowest/2. + next_lowest/2. # Split threshold is always the mid-point between two consecutive values.\n",
    "        if mid == next_lowest: mid = lowest\n",
    "            \n",
    "        # Move samples from the left to right child when it's quicker to do so.\n",
    "        if pos-prev_pos > node_end-pos-1:\n",
    "            l_num, l_den = parent_num, parent_den\n",
    "            r_num, r_den = 0., 0.\n",
    "            l_wcc[:] = parent_wcc\n",
    "            r_wcc[:] = 0.\n",
    "            for r in reversed(range(pos, node_end)):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label]\n",
    "                r_num += w*( 2*r_wcc[label] + w); r_den += w\n",
    "                l_num += w*(-2*l_wcc[label] + w); l_den -= w \n",
    "                r_wcc[label] += w; l_wcc[label] -= w\n",
    "        else:\n",
    "            for r in range(prev_pos, pos):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label] \n",
    "                l_num += w*( 2.*l_wcc[label] + w); l_den += w\n",
    "                r_num += w*(-2.*r_wcc[label] + w); r_den -= w\n",
    "                l_wcc[label] += w; r_wcc[label] -= w  \n",
    "                \n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf.\n",
    "        if pos - node_start < min_samples_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif node_end - pos < min_samples_leaf:\n",
    "            return current_feat_const\n",
    "        # l_den and r_den are left and right children's weighted sample sums.\n",
    "        elif l_den < min_weight_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif r_den < min_weight_leaf:\n",
    "            return current_feat_const\n",
    "\n",
    "        current_feat_const = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Only update best split stats if current score beats all\n",
    "            # other best found among all other features already explored \n",
    "            # at the current node.\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "        lowest = next_lowest\n",
    "        prev_pos = pos; pos+=1\n",
    "    return current_feat_const\n",
    "\n",
    "class DecisionTreeLouppeWright():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Louppe's splitter for SmallQ splits and Wright's splitter for LargeQ splitting.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights=[], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try \n",
    "                                                to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit \n",
    "                                                the tree.\n",
    "            class_weights (ndarray of float64): Sample weight to be used for each class. Shape: (`n_class`,).\n",
    "            seed                         (int): Use when reproducibility desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C')\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = np.empty(0, dtype=Node, order='C')\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = np.empty(0, dtype=np.float64, order='C')\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self.n_nodes\n",
    "    \n",
    "    @property \n",
    "    def left_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].l_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def right_children(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].r_child\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_features(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].feat\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): \n",
    "        out = np.empty(self.n_nodes, dtype='float64')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].thresh\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def weighted_cc(self):\n",
    "        out_size = self.n_nodes*self.n_class\n",
    "        out = np.empty(out_size, dtype='float64')\n",
    "        for i in range(out_size):\n",
    "            out[i] = self.weighted_class_counts[i]\n",
    "        out.resize(self.n_nodes, self.n_class)\n",
    "        return out\n",
    "\n",
    "    @property \n",
    "    def labels(self): \n",
    "        out = np.empty(self.n_nodes, dtype='int')\n",
    "        for i in range(self.n_nodes):\n",
    "            out[i] = self.nodes[i].label\n",
    "        return out\n",
    "    \n",
    "    def _increase_mem_capacity(self, new_capacity):\n",
    "        \"\"\"Resize ndarrays that hold tree's nodes and weighted class counts.\n",
    "        \n",
    "        Arguments:\n",
    "            new_capacity (int): Amount of nodes that resized arrays will be able to hold.\n",
    "        \"\"\"\n",
    "        self.nodes.resize(new_capacity, refcheck=False)\n",
    "        self.weighted_class_counts.resize(new_capacity*self.n_class, refcheck=False)\n",
    "    \n",
    "    def _make_leaf(self, node_id, wcc, n_classes_node):\n",
    "        \"\"\"Set and store the class label of a leaf node.\n",
    "        \n",
    "        Break ties at random when multiple classes share the same max weight.\n",
    "        Doing this avoids a bias towards lower classes that would be a possible\n",
    "        consequence of using np.argmax (which is what Sklearn does).\n",
    "        \n",
    "        Arguments:\n",
    "            node_id            (int): Location of node in `self.nodes`.\n",
    "            wcc (ndarray of float64): Node's weighted class counts. Shape: (`self.n_class`,).\n",
    "            n_classes_node     (int): Number of unique class labels found among\n",
    "                                      node's training samples.\n",
    "        \"\"\"\n",
    "        if n_classes_node == 1: \n",
    "            label = max(enumerate(wcc), key=lambda f: f[1])[0]\n",
    "        else:              \n",
    "            label = self._rng.choice(np.argwhere(wcc==np.max(wcc)).flatten())\n",
    "        self.nodes[node_id] = Node(-1, -1, -1, np.nan, label) \n",
    "        \n",
    "    def _grow_tree(self, X, y, split_point_idxs, unique_feat_vals):\n",
    "        \"\"\"Depth-first growth of a decision tree.\n",
    "        \n",
    "        Arguments:\n",
    "            X                     (ndarray of float64): Training samples. Shape: (n samples, n features).\n",
    "            y                         (ndarray of int): Training labels. Shape: (n samples,).\n",
    "            split_point_idxs          (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                        Shape: (n training samples, n features).\n",
    "            unique_vals_feats     (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                        Shape: (max cardinality of all feats, n features).\n",
    "        \"\"\"\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        node_stack = []\n",
    "        \n",
    "        # Stores the weighted class counts of the current node.\n",
    "        node_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        ##############################################################\n",
    "        # For finding the best split.\n",
    "        ##############################################################\n",
    "        l_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        r_wcc = np.empty(self.n_class, dtype=np.float64)\n",
    "        \n",
    "        # For SmallQ splitting using Louppe.\n",
    "        items = np.empty(self.n_samples, dtype=np.float64)\n",
    "        \n",
    "        # 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value.\n",
    "        # For Wright LargeQ splitting.\n",
    "        \n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        split_counts_raw = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        # Weighted sample counts at each split-point.\n",
    "        split_counts_wt = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        # Weighted class counts for each split-point.\n",
    "        split_class_counts_wt = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64)  \n",
    "\n",
    "        # Keeping track of nodes' constant features. \n",
    "        features = self.features.copy()\n",
    "        constant_features = np.empty(self.n_features, dtype=np.intp)\n",
    "        \n",
    "        # Push root node onto the LIFO stack.\n",
    "        node_stack.append(StackEntry(0, self.n_samples, 0, 0, 0))\n",
    "        self.n_nodes = 1\n",
    "        \n",
    "        while len(node_stack) > 0:\n",
    "            node_info = node_stack.pop()\n",
    "            start, end = node_info.start, node_info.end\n",
    "            node_id, parent_id = node_info.node_id, node_info.parent_id\n",
    "            n_consts = node_info.n_const_feats\n",
    "            n_samples_node = end-start\n",
    "            \n",
    "            # Tabulate and store the current node's weighted class counts.\n",
    "            node_wcc[:] = 0.\n",
    "            for i in range(n_samples_node):\n",
    "                row = self.rows[start + i]\n",
    "                label = y[row]\n",
    "                wt = self.class_weights[label]\n",
    "                node_wcc[label] += wt \n",
    "            self.weighted_class_counts[node_id*self.n_class: (node_id + 1)* self.n_class] = node_wcc\n",
    "            \n",
    "            # Make a leaf if required to do so.\n",
    "            n_classes_node, sum_node_wcc, sum_node_wcc_sqr = 0, 0., 0.\n",
    "            for c in range(self.n_class):\n",
    "                wcc = node_wcc[c]\n",
    "                if wcc > 0: n_classes_node += 1\n",
    "                # Compute the current node's proxy gini numerator and denominator while we're at it.\n",
    "                sum_node_wcc_sqr += wcc**2 \n",
    "                sum_node_wcc += wcc \n",
    "            if n_classes_node == 1:                      \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "            \n",
    "            # Or perform a split.\n",
    "            else:\n",
    "                # Initialize stats for best split of node.\n",
    "                best_split = Split(-1, 0., -np.inf)\n",
    "                \n",
    "                # Ensure feats drawn w/out replacement.\n",
    "                n_drawn_feats = 0\n",
    "                n_new_consts = 0\n",
    "                n_total_consts = n_consts\n",
    "                lb = 0                      # Range in `features` array from which we \n",
    "                ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "               \n",
    "                while n_drawn_feats < self.m:\n",
    "                    n_drawn_feats += 1\n",
    "                    idx = self._rng.choice(range(lb, ub-n_new_consts+1))\n",
    "                    \n",
    "                    # So that we don't draw a known constant feature again this split-search.\n",
    "                    if idx < n_consts:\n",
    "                        features[idx], features[lb] = features[lb], features[idx]\n",
    "                        lb += 1 \n",
    "                        continue\n",
    "                        \n",
    "                    # So that no new const feats get drawn more than once per split-search.\n",
    "                    idx += n_new_consts\n",
    "                    \n",
    "                    feat_idx = features[idx]\n",
    "                  \n",
    "                    # Num split points found among training samples for given feat.\n",
    "                    n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                    \n",
    "                    q = n_samples_node/n_unique_vals_feat\n",
    "                    \n",
    "                    # Louppe numerical splitting.\n",
    "                    if q < Q_THRESHOLD:\n",
    "                        # Prepare the rows' feature values for sorting.\n",
    "                        items[start:end] = X[:,feat_idx][self.rows[start:end]]\n",
    "\n",
    "                        # Sort feature values and corresponding sample row indices\n",
    "                        # to prepare for numerical split finding.\n",
    "                        dual_sort(items, self.rows, start, end)\n",
    "\n",
    "                        # Make sure the feature not constant for node's samples.\n",
    "                        if items[start] == items[end-1]:\n",
    "                            # Move the newly-discovered constant feat to the far right-end\n",
    "                            # of the left half of `features` list holding the known const\n",
    "                            # feats as well as any other const feats newly discovered \n",
    "                            # during this node's split-search.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # Initialize weighted class counts of right and left children.\n",
    "                            # Right child's counts are initially the same as parent node's.\n",
    "                            r_wcc[:] = node_wcc\n",
    "                            l_wcc[:] = 0.\n",
    "\n",
    "                            # If the feature has an impurity score that's better than the best score \n",
    "                            # found among all other features visited thus far for this node, find_num_split()\n",
    "                            # updates the attributes of the struct containing the node's best split info. \n",
    "                            # \n",
    "                            # But even if a new best score isn't reached, if an impurity score can\n",
    "                            # be calculated at least once during the feature's split search, the\n",
    "                            # following indicator will be toggled off, to indicate that the feature\n",
    "                            # is not constant (1 = is constant; 0 = not constant).\n",
    "                            current_feat_const = find_num_split_Louppe(self.rows, items, y, start, n_samples_node, self.n_class, \n",
    "                                                                       self.min_samples_leaf, self.min_weight_leaf, self.class_weights, \n",
    "                                                                       l_wcc, r_wcc, node_wcc, best_split, feat_idx, sum_node_wcc_sqr, \n",
    "                                                                       sum_node_wcc)\n",
    "                    # Wright LargeQ numerical splitting.\n",
    "                    else:\n",
    "                        r_wcc[:] = node_wcc\n",
    "                        l_wcc[:] = 0.\n",
    "                        current_feat_const = find_num_split_largeQ(self.rows, split_point_idxs, unique_vals_feats, y, start, n_samples_node, \n",
    "                                                           self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                                           self.class_weights, l_wcc, r_wcc, node_wcc, best_split, feat_idx, \n",
    "                                                           sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, split_counts_raw, \n",
    "                                                           split_counts_wt, split_class_counts_wt)\n",
    "\n",
    "                    if current_feat_const:\n",
    "                        # The feature may be constant within the search range permitted\n",
    "                        # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                        # the feature is a newly discovered constant.\n",
    "                        features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                        n_new_consts += 1\n",
    "                        n_total_consts += 1\n",
    "                        continue\n",
    "                    else:\n",
    "                        # The feature is non-constant, so we ensure it's not drawn again\n",
    "                        # during this split-search.\n",
    "                        features[idx], features[ub] = features[ub], features[idx]\n",
    "                        ub -= 1 \n",
    "                            \n",
    "                # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                features[0:n_consts] = constant_features[0:n_consts]\n",
    "                constant_features[n_consts:n_consts+n_new_consts] = features[n_consts:n_consts+n_new_consts]\n",
    "                \n",
    "                # Make node a leaf if constant for all randomly drawn feats.\n",
    "                # (# drawn known constant feats + # drawn new constant feats)\n",
    "                if lb + n_new_consts == n_drawn_feats: \n",
    "                    self._make_leaf(node_id, node_wcc, n_classes_node)\n",
    "                else: \n",
    "                    split_pos = make_num_split(self.rows, X, node_info, best_split) \n",
    "\n",
    "                    # Update info for node that's getting split.\n",
    "                    l_child_id = self.n_nodes\n",
    "                    r_child_id = l_child_id + 1\n",
    "                    self.nodes[node_id] = Node(l_child_id, r_child_id, best_split.feat, best_split.thresh, -1)\n",
    "\n",
    "                    # Prepare for the left and right child nodes\n",
    "                    # by increasing tree data memory capacity if\n",
    "                    # necessary.\n",
    "                    if self.n_nodes + 2 > self.mem_capacity:\n",
    "                        # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                        # part of WhozCraig's SO answer at: \n",
    "                        #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                        # Add one after squaring so that the new capacity can\n",
    "                        # contain not only a tree of greater depth, but also\n",
    "                        # the maximum # nodes that that depth could have.\n",
    "                        new_capacity = 2*self.mem_capacity + 1\n",
    "                        self._increase_mem_capacity(new_capacity)\n",
    "                        self.mem_capacity = new_capacity\n",
    "                    \n",
    "                    # Push right child info onto the LIFO stack.\n",
    "                    node_stack.append(StackEntry(split_pos, end, r_child_id, node_id, n_total_consts))\n",
    "                    # Push left child info onto queue.\n",
    "                    node_stack.append(StackEntry(start, split_pos, l_child_id, node_id, n_total_consts))\n",
    "\n",
    "                    # And update size of the tree.\n",
    "                    self.n_nodes += 2\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X    (Fortan-style ndarray of float64): Pre-processed training data.\n",
    "            y                     (ndarray of int): Training labels.\n",
    "            split_point_idxs      (ndarray of int): All numerical feature split-point locations for all rows.\n",
    "                                                    Shape: (n training samples, n features).\n",
    "            unique_vals_feats (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                    Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats      (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows                            (list): Indices of the rows to be used for training. \n",
    "                                                    All rows used if empty.\n",
    "            features                        (list): Column indices of training features that will be used.\n",
    "                                                    All features used if empty.    \n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        # Determine # classes found among all training samples.\n",
    "        root_cc = np.unique(y, return_counts=True)[1] \n",
    "        self.n_class = root_cc.size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "\n",
    "        self.n_samples = len(self.rows)\n",
    "        self.n_features = len(self.features)\n",
    "        \n",
    "        # Store the num unique vals for each numerical feat and\n",
    "        # find the maximum cardinality of all features.\n",
    "        self.n_unique_vals_feats = n_unique_vals_feats\n",
    "        self.max_n_unique_feat_vals = self.n_unique_vals_feats.max()\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        init_capacity = 15\n",
    "        \n",
    "         # Allocate tree memory.\n",
    "        self._increase_mem_capacity(init_capacity)\n",
    "        self.mem_capacity = init_capacity\n",
    "        \n",
    "        # And sum the class weights of all the root node's samples in\n",
    "        # order to know minimum total weight a leaf must have (which\n",
    "        # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "        root_wcc = root_cc*self.class_weights\n",
    "        self.min_weight_leaf = self.min_weight_fraction_leaf*root_wcc.sum()\n",
    "        \n",
    "        # Initialize the random number generator.\n",
    "        self._rng = get_random_generator(self.seed)\n",
    "        \n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X, y, split_point_idxs, unique_feat_vals)\n",
    "        return self\n",
    "        \n",
    "    def _next_node(self, nxt): return self.nodes[nxt]\n",
    "       \n",
    "    def _get_leaf_idx(self, i, X):\n",
    "        root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[:,leaf.feat][i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else:\n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        n_preds = X.shape[0]\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        for i in range(n_preds):\n",
    "            preds[i] = self.nodes[self._get_leaf_idx(i, X)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        for i in range(n_probs):\n",
    "            idx = self._get_leaf_idx(i, X)\n",
    "            for j in range(self.n_class):\n",
    "                wcc[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Louppe SmallQ/Wright LargeQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeLouppeWright(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "371"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752808988764045"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 ms Â± 2.87 ms per loop (mean Â± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "527 Âµs Â± 10.2 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Louppe SmallQ/Wright LargeQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "# cython: wraparound=False, boundscheck=False, cdivision=True, initializedcheck=False\n",
    "# distutils: language = c++\n",
    "# distutils: extra_compile_args = -std=c++11\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "np.import_array()\n",
    "ctypedef np.float64_t DTYPE_t\n",
    "ctypedef np.intp_t SIZE_t # Signed, same as ssize_t in C. See MSeifert's SO answer: https://stackoverflow.com/a/46416257/8628758\n",
    "cimport cython\n",
    "from libc.math cimport log as ln\n",
    "from libc.stdlib cimport realloc, free\n",
    "from libc.string cimport memcpy\n",
    "from libc.string cimport memset\n",
    "from libcpp.stack cimport stack\n",
    "\n",
    "# For C++ random number generation.\n",
    "from libc.stdint cimport uint_fast32_t \n",
    "\n",
    "# Swap helper func for sorting.\n",
    "cdef inline void dual_swap(DTYPE_t* items, SIZE_t* rows, SIZE_t i, SIZE_t j) nogil:\n",
    "    items[i], items[j] = items[j], items[i]\n",
    "    rows[i], rows[j] = rows[j], rows[i]\n",
    "\n",
    "# Quicksort helpers\n",
    "\n",
    "cdef inline void dual_med_three(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Find the median-of-three pivot point of the second through final \n",
    "    items of a list of numbers. Once identified, the pivot is moved to \n",
    "    the front of the list. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L78\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t middle = <int>(first + (last - first)/2)\n",
    "    cdef SIZE_t second = first + 1\n",
    "    last -= 1\n",
    "    if items[second] < items[middle]:\n",
    "        if items[middle] < items[last]:\n",
    "            dual_swap(items, rows, first, middle)    \n",
    "        elif items[second] < items[last]:\n",
    "            dual_swap(items, rows, first, last)         \n",
    "        else:                        \n",
    "            dual_swap(items, rows, first, second)\n",
    "    elif items[second] < items[last]:\n",
    "        dual_swap(items, rows, first, second)\n",
    "    elif items[middle] < items[last]:\n",
    "        dual_swap(items, rows, first, last)\n",
    "    else:\n",
    "        dual_swap(items, rows, first, middle)\n",
    "\n",
    "cdef inline SIZE_t dual_partition(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, SIZE_t pivot) nogil:\n",
    "    \"\"\"Group numbers less than the pivot value together on the left and\n",
    "    those that are greater on the right. Find the index that separates\n",
    "    these two groups, which will belong to the first item that is greater\n",
    "    than or equal to the pivot. Borrows from libstdc++ implementation at: \n",
    "        https://github.com/gcc-mirror/gcc/blob/d9375e490072d1aae73a93949aa158fcd2a27018/libstdc%2B%2B-v3/include/bits/stl_algo.h#L1885\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        pivot      : Index holding the median pivot value.\n",
    "        \n",
    "    Returns:\n",
    "        Index of cut point used to partition the items into two smaller sequences.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        while first < last and items[first] < items[pivot]:\n",
    "            first += 1                      # Get index of first item greater than or equal to median-of-three pivot. \n",
    "        last -= 1\n",
    "        while items[pivot] < items[last]:\n",
    "            last -= 1                       # Get index of last item less than or equal to the pivot.\n",
    "        if not (first < last): \n",
    "            return first                    # After swaps are done, return index of first item in right partition.\n",
    "        \n",
    "        dual_swap(items, rows, first, last) # Swap the first item greater than or equal to the pivot with the\n",
    "                                            # last item less than or equal to the pivot. \n",
    "        first += 1\n",
    "\n",
    "cdef inline void dual_insertion_sort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Follows the spirit of the Numpy implementation at: \n",
    "        https://github.com/numpy/numpy/blob/5ffb84c3057a187b01acdeaa628137193df12098/numpy/core/src/npysort/quicksort.cpp#L211\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef SIZE_t i\n",
    "    cdef SIZE_t j\n",
    "    cdef SIZE_t k\n",
    "    cdef DTYPE_t val\n",
    "    for i in range(first+1, last):\n",
    "        j = i\n",
    "        k = i - 1\n",
    "        val = items[i]\n",
    "        row = rows[i]\n",
    "        while (j > first) and val < items[k]:\n",
    "            items[j] = items[k]\n",
    "            rows[j] = rows[k]\n",
    "            j-=1\n",
    "            k-=1\n",
    "        items[j] = val\n",
    "        rows[j] = row\n",
    "\n",
    "# Heapsort\n",
    "\n",
    "cdef inline void dual_sift_down(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n, \n",
    "                                SIZE_t p, SIZE_t c, DTYPE_t val, SIZE_t row) nogil:\n",
    "    \"\"\"Swap a heap item with one of its children if that child's value is \n",
    "    greater than or equal to that parent's value. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L61\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "        p    : Index of the parent.\n",
    "        c    : Index of the parent's first (left) child.\n",
    "        val  : The parent's value.\n",
    "        row  : The parent's training row index.\n",
    "    \"\"\"\n",
    "    while c < n:    # Look at the descendents of current parent, `p`.\n",
    "        if c < n-1 and items[start + c] < items[start + c + 1]: # Find larger of the first and second children.\n",
    "            c += 1\n",
    "        if val < items[start + c]: # If child greater than parent, swap child and parent.\n",
    "            items[start + p] = items[start + c]\n",
    "            rows[start + p] = rows[start + c]\n",
    "            p = c   # Current greater child becomes the parent.\n",
    "            c += c  # Look at this child's child, if it exists.\n",
    "        else:\n",
    "            break \n",
    "    items[start + p] = val\n",
    "    rows[start + p] = row\n",
    "\n",
    "cdef inline void dual_sort_heap(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Sort a binary max heap of numbers. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L77\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t row\n",
    "    while n > 0:\n",
    "        n -= 1\n",
    "        val = items[start + n]\n",
    "        row = rows[start + n]\n",
    "        items[start + n] = items[start]\n",
    "        rows[start + n] = rows[start]\n",
    "        dual_sift_down(items, rows, start, n, 0, 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapify(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Turn a list of items into a binary max heap. From Williams, 1964.\n",
    "    Modeled after Numpy's implementation at:\n",
    "        https://github.com/numpy/numpy/blob/084d05a5d1ef3efe79474b09b42594ee9ef086cb/numpy/core/src/npysort/heapsort.cpp#L59\n",
    "    \n",
    "    Arguments:\n",
    "        items: 1-d array containing numbers.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number.\n",
    "        n    : Quantity of numbers.\n",
    "    \"\"\"\n",
    "    cdef DTYPE_t val\n",
    "    cdef SIZE_t p\n",
    "    cdef SIZE_t last_p = (n-2)//2\n",
    "    for p in range(last_p, -1, -1):\n",
    "        val = items[start + p] # value of last parent\n",
    "        row = rows[start + p]\n",
    "        dual_sift_down(items, rows, start, n, p, 2*p + 1, val, row)\n",
    "\n",
    "cdef inline void dual_heapsort(DTYPE_t* items, SIZE_t* rows, SIZE_t start, int n) nogil:\n",
    "    \"\"\"Applies the heapsort algorithm to sort a list of items from least to greatest. \n",
    "    From Williams, 1964.\n",
    "    Arguments:\n",
    "        items: 1-d array containing the numbers to be sorted.\n",
    "        rows : Row indices of all training samples.\n",
    "        start: Index of the first number to be sorted.\n",
    "        n    : Quantity of numbers to be sorted\n",
    "    \"\"\"\n",
    "    dual_heapify(items, rows, start, n)\n",
    "    dual_sort_heap(items, rows, start, n)\n",
    "    \n",
    "# Introsort \n",
    "\n",
    "cdef void dual_introsort_loop(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last, int depth) nogil:\n",
    "    \"\"\"The recursive heart of the introsort algorithm.\n",
    "    \n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "        depth      : Current recursion depth.\n",
    "    \"\"\"\n",
    "    cdef int MIN_SIZE_THRESH = 16\n",
    "    cdef SIZE_t cut\n",
    "    while last-first > MIN_SIZE_THRESH:\n",
    "        if depth == 0:\n",
    "            dual_heapsort(items, rows, first, last-first)\n",
    "        depth -= 1\n",
    "        dual_med_three(items, rows, first, last)\n",
    "        cut = dual_partition(items, rows, first+1, last, first)\n",
    "        dual_introsort_loop(items, rows, cut, last, depth)\n",
    "        last = cut\n",
    "\n",
    "# Log base-2 helper function. From Sklearn's implementation at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_utils.pyx#L7\n",
    "cdef inline DTYPE_t log2(DTYPE_t x) nogil:\n",
    "    return ln(x) / ln(2.0)\n",
    "\n",
    "cdef void dual_introsort(DTYPE_t* items, SIZE_t* rows, SIZE_t first, SIZE_t last) nogil:\n",
    "    \"\"\"Implementation as described in Musser, 1997. Switches to heapsort\n",
    "    when max recursion depth exceeded. Otherwise uses median-of-three \n",
    "    quicksort (Bentley & McIlroy, 1993) with all the usual optimizations:\n",
    "        - Swap equal elements.\n",
    "        - Only process partitions longer than the minimum size threshold.\n",
    "        - When a new partition is made, recurse on the smaller half and \n",
    "          iterate over the larger half.\n",
    "        - Make a final pass with insertion sort over the entire list.\n",
    "\n",
    "    Arguments:\n",
    "        items      : The numbers to be sorted.\n",
    "        rows       : Row indices of all training samples.\n",
    "        first, last: The range of items to be sorted. \n",
    "    \"\"\"\n",
    "    cdef int max_depth = 2 * <int>log2(last-first)\n",
    "    dual_introsort_loop(items, rows, first, last, max_depth)\n",
    "    dual_insertion_sort(items, rows, first, last)\n",
    "    \n",
    "# For convenient memory reallocation.\n",
    "ctypedef fused realloc_t:\n",
    "    SIZE_t\n",
    "    DTYPE_t\n",
    "    Node\n",
    "\n",
    "cdef inline realloc_t* safe_realloc(realloc_t* ptr, SIZE_t n_items) nogil except *:\n",
    "    # Inspired by Sklearn's safe_realloc() func. However, thankfully\n",
    "    # Cython now no longer requires us to send a pointer to a pointer\n",
    "    # in order to prevent crashes.\n",
    "    cdef realloc_t elem = ptr[0]\n",
    "    cdef SIZE_t n_bytes = n_items * sizeof(elem)\n",
    "    # Make sure we're not trying to allocate too much memory.\n",
    "    if n_bytes/sizeof(elem) != n_items:\n",
    "        with gil:\n",
    "            raise MemoryError(f\"Overflow error: unable to allocate {n_bytes} bytes.\")       \n",
    "    cdef realloc_t* res_ptr = <realloc_t *> realloc(ptr, n_bytes)\n",
    "    with gil:\n",
    "        if not res_ptr: raise MemoryError()\n",
    "    return res_ptr\n",
    "\n",
    "# C++ random number generator. Not yet a part of a Cython release so\n",
    "# pasted in from: \n",
    "#     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/Cython/Includes/libcpp/random.pxd#L1\n",
    "cdef extern from \"<random>\" namespace \"std\" nogil:\n",
    "    cdef cppclass random_device:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        random_device() except +\n",
    "        result_type operator()() except +\n",
    "\n",
    "    cdef cppclass mt19937:\n",
    "        ctypedef uint_fast32_t result_type\n",
    "        mt19937() except +\n",
    "        mt19937(result_type seed) except +\n",
    "        result_type operator()() except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        void discard(size_t z) except +\n",
    "        void seed(result_type seed) except +\n",
    "\n",
    "    cdef cppclass uniform_int_distribution[T]:\n",
    "        ctypedef T result_type\n",
    "        uniform_int_distribution() except +\n",
    "        uniform_int_distribution(T, T) except +\n",
    "        result_type operator()[Generator](Generator&) except +\n",
    "        result_type min() except +\n",
    "        result_type max() except +\n",
    "        \n",
    "# Info for any node that will eventually be split or made into a leaf.\n",
    "# Similar to what Sklearn does at:\n",
    "#     https://github.com/scikit-learn/scikit-learn/blob/a2c4d8b1f4471f52a4fcf1026f495e637a472568/sklearn/tree/_tree.pyx#L126\n",
    "cdef struct StackEntry:\n",
    "    SIZE_t start\n",
    "    SIZE_t end\n",
    "    SIZE_t node_id\n",
    "    SIZE_t parent_id\n",
    "    SIZE_t n_const_feats\n",
    "\n",
    "# To compare node splits.\n",
    "cdef struct Split:\n",
    "    SIZE_t feat\n",
    "    DTYPE_t thresh\n",
    "    DTYPE_t score  \n",
    "\n",
    "# Vital characteristics of a node. Set when it's added to the tree.\n",
    "cdef struct Node:\n",
    "    SIZE_t l_child # idx of left child, -1 if leaf\n",
    "    SIZE_t r_child # idx of right child, -1 if leaf\n",
    "    SIZE_t feat    # col idx of split feature, -1 if leaf\n",
    "    DTYPE_t thresh # double split threshold, NAN if leaf\n",
    "    SIZE_t label   # class label if leaf, -1 if non-leaf.\n",
    "\n",
    "# Louppe numerical splitting.\n",
    "cdef inline void find_num_split(SIZE_t* rows, DTYPE_t* items, SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, \n",
    "                                SIZE_t n_class, SIZE_t min_samples_leaf, DTYPE_t min_weight_leaf, DTYPE_t* c_wts,\n",
    "                                DTYPE_t* l_wcc, DTYPE_t* r_wcc, DTYPE_t* parent_wcc, Split* best_split, \n",
    "                                SIZE_t current_feat, DTYPE_t parent_num, DTYPE_t parent_den, \n",
    "                                bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Gilles Louppe's split-finding algorithm:\n",
    "        Page 31 in Louppe, 2015: https://arxiv.org/pdf/1407.7502.pdf\n",
    "    \n",
    "    Saves a split's feature idx, threshold, position, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows              : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        items             : The sorted feature values of the samples in the parent\n",
    "                            node (beginning at `node_start`). Shape: (n train samples,).\n",
    "        labels            : All training labels. Shape: (n training samples,).\n",
    "        node_start        : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent          : Number of samples in the parent node.\n",
    "        n_class           : Number of unique classes in the training set.\n",
    "        min_samples_leaf  : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf   : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts             : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc             : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc             : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc        : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split        : Holds the feature, threshold, position, and impurity\n",
    "                            score of the parent node's current best split.\n",
    "        current_feat      : Column index of feature under investigation.\n",
    "        parent_num        : Numerator of parent node's impurity score.\n",
    "        parent_den        : Denominator of parent node's impurity score.\n",
    "        current_feat_const: Whether current splitting feature is constant for all eligible split \n",
    "                            thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, w, score\n",
    "    cdef SIZE_t row, label, r\n",
    "    \n",
    "    # To iterate across all the node's samples' feature values.\n",
    "    cdef SIZE_t prev_pos, pos, node_end\n",
    "    cdef DTYPE_t lowest, next_lowest, mid\n",
    "    \n",
    "    prev_pos, pos = node_start, node_start\n",
    "    node_end = node_start + n_parent\n",
    "    lowest = items[pos]\n",
    "    l_num, l_den = 0., 0.\n",
    "    r_num, r_den = parent_num, parent_den\n",
    "    \n",
    "    # Find the best split and store its score, threshold, position,\n",
    "    # as well as it's children's weighted class counts.\n",
    "    while pos < node_end:\n",
    "        while items[pos] == lowest: # When consecutive items have the same value.\n",
    "            if pos == node_end - 1: # When the final few samples all have the same value.\n",
    "                return\n",
    "            pos+=1\n",
    "        next_lowest = items[pos]\n",
    "        mid = lowest/2. + next_lowest/2. # Split threshold is always the mid-point between two consecutive values.\n",
    "        if mid == next_lowest: mid = lowest\n",
    "\n",
    "        # Move samples from the left to right child when it's quicker to do so.\n",
    "        if pos-prev_pos > node_end-pos-1:\n",
    "            l_num, l_den = parent_num, parent_den\n",
    "            r_num, r_den = 0., 0.\n",
    "            memcpy(l_wcc, parent_wcc, n_class*sizeof(DTYPE_t))\n",
    "            memset(r_wcc, 0, n_class*sizeof(DTYPE_t))\n",
    "            for r in reversed(range(pos, node_end)):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label]\n",
    "                r_num += w*( 2*r_wcc[label] + w); r_den += w\n",
    "                l_num += w*(-2*l_wcc[label] + w); l_den -= w \n",
    "                r_wcc[label] += w; l_wcc[label] -= w\n",
    "        else:\n",
    "            for r in range(prev_pos, pos):\n",
    "                row = rows[r]\n",
    "                label = labels[row]; w = c_wts[label] \n",
    "                l_num += w*( 2.*l_wcc[label] + w); l_den += w\n",
    "                r_num += w*(-2.*r_wcc[label] + w); r_den -= w\n",
    "                l_wcc[label] += w; r_wcc[label] -= w\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf.\n",
    "        if pos - node_start < min_samples_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif node_end - pos < min_samples_leaf:\n",
    "            return\n",
    "        # l_den and r_den are left and right children's weighted sample sums.\n",
    "        elif l_den < min_weight_leaf: \n",
    "            lowest = next_lowest\n",
    "            prev_pos = pos; pos+=1 \n",
    "            continue\n",
    "        elif r_den < min_weight_leaf:\n",
    "            return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Only update best split stats if current score beats all\n",
    "            # other best found among all other features already explored \n",
    "            # at the current node.\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "        lowest = next_lowest\n",
    "        prev_pos = pos; pos+=1\n",
    "\n",
    "cdef inline void find_num_split_largeQ(SIZE_t* rows, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats, \n",
    "                                       SIZE_t* labels, SIZE_t node_start, SIZE_t n_parent, SIZE_t n_samples,\n",
    "                                       SIZE_t n_class, SIZE_t min_samples_leaf, DTYPE_t min_weight_leaf, \n",
    "                                       DTYPE_t* c_wts, DTYPE_t* l_wcc, DTYPE_t* r_wcc, DTYPE_t* parent_wcc, \n",
    "                                       Split* best_split, SIZE_t current_feat, DTYPE_t parent_num, \n",
    "                                       DTYPE_t parent_den, SIZE_t n_unique_vals_feat, SIZE_t max_n_unique_vals, \n",
    "                                       SIZE_t* split_counts_raw, DTYPE_t* split_counts_wt, \n",
    "                                       DTYPE_t* split_class_counts_wt, bint* current_feat_const) nogil:\n",
    "    \"\"\"Calculates the impurity score of each eligible split threshold in a \n",
    "    decision tree node that belongs to a single numerical feature.\n",
    "\n",
    "    Uses Marvin Wright's LargeQ splitting algorithm:\n",
    "        https://github.com/imbs-hl/ranger/blob/5f71872d7b552fd2cf652daab92416f52976df86/src/TreeClassification.cpp#L316\n",
    "    \n",
    "    Saves a split's feature idx, threshold, and impurity score if the\n",
    "    score is a new best for the node.\n",
    "    \n",
    "    Arguments:\n",
    "        rows                   : Indices of all rows in the training set. Shape: (n train samples,).\n",
    "        split_point_idxs       : All numerical feature split-point locations for all rows.\n",
    "                                 Shape: (n training samples, n features).\n",
    "        unique_vals_feats      : Columns contain sorted unique values for all features. \n",
    "                                 Shape: (max cardinality of all feats, n features).\n",
    "                                 node (beginning index 0). Shape: (n train samples,).\n",
    "        labels                 : All training labels. Shape: (n training samples,).\n",
    "        node_start             : Index of the beginning of the parent node in `rows`.\n",
    "        n_parent               : Number of samples in the parent node.\n",
    "        n_samples              : Number of samples in the training data.\n",
    "        n_class                : Number of unique classes in the training set.\n",
    "        min_samples_leaf       : Any leaf will have no fewer than this many samples.\n",
    "        min_weight_leaf        : Total weight of any leaf's samples will be at least this much.\n",
    "        c_wts                  : Class weights. Shape: (`n_class`,).\n",
    "        l_wcc                  : Left child's weight class counts. Shape: (`n_class`,).\n",
    "        r_wcc                  : Right child's weight class counts. Shape: (`n_class`,).\n",
    "        parent_wcc             : Parent node's weight class counts. Shape: (`n_class`,).\n",
    "        best_split             : Holds the feature, threshold and impurity\n",
    "                                 score of the parent node's current best split.\n",
    "        current_feat           : Column index of feature under investigation.\n",
    "        parent_num             : Numerator of parent node's impurity score.\n",
    "        parent_den             : Denominator of parent node's impurity score.\n",
    "        n_unique_vals_feat     : Number of unique values for one feature found among \n",
    "                                 all training samples.\n",
    "        max_n_unique_vals      : Maximum cardinality of all features in dataset.\n",
    "        split_counts_raw       : Stores sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_counts_wt        : Stores weighted sample counts found at each split point of\n",
    "                                 a given feature in a given node. \n",
    "                                 Shape: (<max cardinality of all numerical feats in dataset>,).\n",
    "        split_class_counts_wt  : Stores weighted class counts of each class at each\n",
    "                                 split point of a given feature in a given node. Shape:\n",
    "                                 (<max cardinality of all numerical feats in dataset> x `n_class`,)\n",
    "        current_feat_const     : Whether current splitting feature is constant for all eligible split \n",
    "                                 thresholds in the current node. 1 if yes, 0 otherwise.\n",
    "    \"\"\"\n",
    "    # Variables used while tabulating raw and weighted sample counts, \n",
    "    # as well as weighted class counts at all unique split points.\n",
    "    cdef SIZE_t row, label, split_point_idx\n",
    "    \n",
    "    # Make sure node's samples aren't all constant for feature.\n",
    "    cdef SIZE_t n_splits_node = 0\n",
    "    \n",
    "    # Variables to track progress during the split search.\n",
    "    cdef SIZE_t n_left, n_right, i, j, k\n",
    "    \n",
    "    # Variables used to calculate proxy gini scores.\n",
    "    cdef DTYPE_t l_num, l_den, r_num, r_den, wt, score, mid\n",
    "    \n",
    "    # Tabulate sample counts, weighted counts, and weighted class counts at \n",
    "    # each split point. Values at split points not belonging to node's rows\n",
    "    # will remain zero.\n",
    "    memset(split_counts_raw, 0, sizeof(SIZE_t)*n_unique_vals_feat)\n",
    "    memset(split_counts_wt, 0, sizeof(DTYPE_t)*n_unique_vals_feat)\n",
    "    memset(split_class_counts_wt, 0, sizeof(SIZE_t)*n_unique_vals_feat*n_class)\n",
    "    for i in range(n_parent):\n",
    "        row = rows[node_start + i]\n",
    "        label = labels[row]\n",
    "        wt = c_wts[label]\n",
    "        split_point_idx = split_point_idxs[n_samples*current_feat + row]\n",
    "        split_counts_raw[split_point_idx] += 1\n",
    "        split_counts_wt[split_point_idx] += wt\n",
    "        split_class_counts_wt[split_point_idx*n_class + label] += wt\n",
    "        \n",
    "    # If feat is constant for the node.\n",
    "    for i in range(n_unique_vals_feat):\n",
    "        if split_counts_raw[i] > 0: n_splits_node += 1\n",
    "    if n_splits_node < 2: return\n",
    "    \n",
    "    # To keep track of num samples in left child.\n",
    "    n_left = 0    \n",
    "    # Left child's proxy gini score denominator.\n",
    "    l_den = 0.\n",
    "    \n",
    "    # Search for the threshold of the best split.\n",
    "    for i in range(n_unique_vals_feat - 1):\n",
    "        if split_counts_raw[i] == 0: continue # Move to next split-point if no samples at this one.\n",
    "        \n",
    "        n_left += split_counts_raw[i]\n",
    "        n_right = n_parent - n_left\n",
    "        if n_right == 0: return # Make sure to stop search when right child empty.\n",
    "        \n",
    "        # Calculate denominators of proxy gini scores.\n",
    "        l_den += split_counts_wt[i]\n",
    "        r_den = parent_den - l_den\n",
    "\n",
    "        # Calculate numerators of proxy gini scores.\n",
    "        l_num, r_num = 0., 0. \n",
    "        for j in range(n_class):\n",
    "            # Can't do the on-line proxy gini update algorithm cause we\n",
    "            # move all samples from a given class over to the left side \n",
    "            # before updating the calculation.\n",
    "            wt = split_class_counts_wt[i*n_class + j]\n",
    "            l_wcc[j] += wt\n",
    "            r_wcc[j] -= wt\n",
    "            l_num += l_wcc[j]*l_wcc[j]\n",
    "            r_num += r_wcc[j]*r_wcc[j]\n",
    "\n",
    "        # Only investigate split-points that satisfy min_samples_leaf and min_weight_leaf\n",
    "        if n_left < min_samples_leaf: continue\n",
    "        elif n_right < min_samples_leaf: return\n",
    "        elif l_den < min_weight_leaf: continue\n",
    "        elif r_den < min_weight_leaf: return\n",
    "\n",
    "        current_feat_const[0] = 0 # If we can compute a score, current feat not constant.\n",
    "        score = (l_num/l_den) + (r_num/r_den) # Proxy gini score.\n",
    "        if score > best_split.score: \n",
    "            # Find raw feature value of sample(s) at next-closest split-point.\n",
    "            k = i+1\n",
    "            while split_counts_raw[k] == 0: k+=1\n",
    "            # Split threshold is always the mid-point between two consecutive values.\n",
    "            mid = (unique_vals_feats[current_feat*max_n_unique_vals + i]/2. + \n",
    "                   unique_vals_feats[current_feat*max_n_unique_vals + k]/2.) \n",
    "            if mid == unique_vals_feats[current_feat*max_n_unique_vals + k]: \n",
    "                mid = unique_vals_feats[current_feat*max_n_unique_vals + i]\n",
    "            best_split.score, best_split.thresh, best_split.feat = score, mid, current_feat\n",
    "\n",
    "cdef inline SIZE_t make_num_split(SIZE_t* rows, DTYPE_t* X, StackEntry* node_info, Split* best_split, \n",
    "                                SIZE_t n_samples) nogil:\n",
    "    cdef SIZE_t p, p_end\n",
    "    p, p_end = node_info.start, node_info.end\n",
    "    while p < p_end:\n",
    "        if X[best_split.feat*n_samples + rows[p]] <= best_split.thresh: p+=1\n",
    "        else: p_end-=1; rows[p], rows[p_end] = rows[p_end], rows[p] \n",
    "    return p\n",
    "\n",
    "# Necessary constants.\n",
    "cdef DTYPE_t NEG_INF = -np.inf\n",
    "cdef DTYPE_t NAN = np.nan\n",
    "cdef DTYPE_t Q_THRESHOLD = 0.02\n",
    "            \n",
    "cdef class _DecisionTree:\n",
    "    # Class attributes.\n",
    "    cdef SIZE_t seed\n",
    "    cdef mt19937 rng\n",
    "    cdef SIZE_t mem_capacity\n",
    "    cdef SIZE_t n_samples\n",
    "    cdef SIZE_t n_features\n",
    "    cdef SIZE_t n_class\n",
    "    cdef SIZE_t m\n",
    "    cdef SIZE_t min_samples_leaf, \n",
    "    cdef DTYPE_t min_weight_fraction_leaf\n",
    "    cdef DTYPE_t min_weight_leaf\n",
    "    cdef SIZE_t n_nodes\n",
    "    cdef SIZE_t max_n_unique_feat_vals\n",
    "    cdef SIZE_t* n_unique_vals_feats\n",
    "    cdef SIZE_t* rows\n",
    "    cdef SIZE_t* features\n",
    "    cdef DTYPE_t* class_weights\n",
    "    cdef Node* nodes\n",
    "    cdef DTYPE_t* weighted_class_counts\n",
    "    def __cinit__(self, SIZE_t m, SIZE_t min_samples_leaf, DTYPE_t min_weight_fraction_leaf, SIZE_t seed): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                       : Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf        : Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf: Total weight of any leaf's samples must comprise this portion \n",
    "                                      of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                    : A seed for the C++ mt19937 32bit int random generator. \n",
    "                                      Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m, self.min_samples_leaf = m, min_samples_leaf\n",
    "        self.min_weight_fraction_leaf = min_weight_fraction_leaf\n",
    "        self.seed = seed\n",
    "        \n",
    "        # The Decision Tree data structure: a 1-d array of nodes. Index of \n",
    "        # each node in this array is its \"node id.\" Root node's id is 0.\n",
    "        # Each `Node` object in the array contains that node's:\n",
    "        #     - left child node id\n",
    "        #     - right child node id\n",
    "        #     - split feature column index\n",
    "        #     - numerical split threshold\n",
    "        #     - class label\n",
    "        self.nodes = NULL\n",
    "        \n",
    "        # Tree nodes' weighted class counts. Will ultimately be a \n",
    "        # 1-d array of length: n_nodes * n_class.\n",
    "        self.weighted_class_counts = NULL \n",
    "        \n",
    "    def __dealloc__(self):\n",
    "        free(self.nodes)\n",
    "        free(self.weighted_class_counts)\n",
    "        \n",
    "    property size:\n",
    "        def __get__(self):\n",
    "            return self.n_nodes\n",
    "    \n",
    "    property left_children:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].l_child\n",
    "            return out\n",
    "\n",
    "    property right_children:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].r_child\n",
    "            return out\n",
    "        \n",
    "    property split_features: \n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].feat\n",
    "            return out\n",
    "        \n",
    "    property split_thresholds:\n",
    "        def __get__(self):\n",
    "            out = np.empty(self.n_nodes, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].thresh\n",
    "            return out\n",
    "        \n",
    "    property weighted_cc:\n",
    "        def __get__(self):\n",
    "            cdef SIZE_t out_size = self.n_nodes*self.n_class\n",
    "            out = np.empty(out_size, dtype='float64')\n",
    "            cdef DTYPE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(out_size):\n",
    "                    out_view[i] = self.weighted_class_counts[i]\n",
    "            out.resize(self.n_nodes, self.n_class)\n",
    "            return out\n",
    "    \n",
    "    property labels:\n",
    "        def __get__(self): \n",
    "            out = np.empty(self.n_nodes, dtype='int')\n",
    "            cdef SIZE_t[::1] out_view = out\n",
    "            cdef SIZE_t i\n",
    "            with nogil:\n",
    "                for i in range(self.n_nodes):\n",
    "                    out_view[i] = self.nodes[i].label\n",
    "            return out\n",
    "    \n",
    "    cdef void _increase_mem_capacity(self, SIZE_t new_capacity) nogil:\n",
    "        self.nodes = safe_realloc(self.nodes, new_capacity)\n",
    "        self.weighted_class_counts = safe_realloc(self.weighted_class_counts, self.n_class*new_capacity)\n",
    "    \n",
    "    cdef void _make_leaf(self, Node* leaf_node, SIZE_t* y, SIZE_t node_start, SIZE_t node_id, \n",
    "                         SIZE_t n_classes_node, SIZE_t* max_wt_classes) nogil:\n",
    "        # Class with largest wcc becomes leaf node's label. Break ties with a random choice.\n",
    "        cdef SIZE_t label\n",
    "        cdef DTYPE_t max_wt = 0.\n",
    "        cdef SIZE_t lb = 0\n",
    "        cdef SIZE_t ub = -1\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t i, j\n",
    "        # If all node's samples have the same class.\n",
    "        if n_classes_node == 1:\n",
    "            label = y[self.rows[node_start]]\n",
    "        else:\n",
    "            # Otherwise find label with max weighted class count for the node.\n",
    "            for i in range(self.n_class):\n",
    "                max_wt = max(max_wt, self.weighted_class_counts[node_id*self.n_class + i])\n",
    "            # See if multiple classes share this max count.\n",
    "            for i in range(self.n_class):\n",
    "                if self.weighted_class_counts[node_id*self.n_class + i] == max_wt:\n",
    "                    ub += 1\n",
    "                    max_wt_classes[ub] = i\n",
    "            # If so, randomly choose leaf's label from among those classes.\n",
    "            if ub > 0:\n",
    "                dist = uniform_int_distribution[SIZE_t](lb, ub) # Choose an int w/in range lb, ub, inclusive.\n",
    "                j = dist(self.rng)\n",
    "                label = max_wt_classes[j]\n",
    "            else:\n",
    "                label = max_wt_classes[lb]\n",
    "        leaf_node.l_child = -1\n",
    "        leaf_node.r_child = -1    \n",
    "        leaf_node.feat = -1  \n",
    "        leaf_node.thresh = NAN\n",
    "        leaf_node.label = label \n",
    "\n",
    "    cdef _grow_tree(self, DTYPE_t* X, SIZE_t* y, SIZE_t* split_point_idxs, DTYPE_t* unique_vals_feats):\n",
    "        # LIFO stack holding all nodes still to be investigated.\n",
    "        cdef stack[StackEntry] node_stack\n",
    "\n",
    "        #####################################################################\n",
    "        # Variables containing info of the node currently being investigated.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t start, end, node_id, parent_id, n_consts, n_samples_node\n",
    "        cdef DTYPE_t* node_wcc = NULL\n",
    "        cdef StackEntry node_info\n",
    "        cdef Node* node = NULL\n",
    "        \n",
    "        # Holds child node info if the current node gets split.\n",
    "        cdef SIZE_t l_child_id, r_child_id\n",
    "        cdef Node* l_child_node = NULL\n",
    "        cdef Node* r_child_node = NULL\n",
    "        \n",
    "        #####################################################################\n",
    "        # For finding the best split.\n",
    "        #####################################################################\n",
    "        cdef Split best_split\n",
    "        cdef DTYPE_t* l_wcc = NULL\n",
    "        cdef DTYPE_t* r_wcc = NULL\n",
    "        cdef DTYPE_t sum_node_wcc_sqr, sum_node_wcc # Parent node's proxy Gini score num and den.\n",
    "        cdef SIZE_t split_pos\n",
    "        \n",
    "        # Indicates a feature has been discovered to be constant during a\n",
    "        # split search within the search range permitted by min_samples_leaf \n",
    "        # and min_weight_leaf.\n",
    "        cdef bint current_feat_const \n",
    "        \n",
    "        # Determines whether to use SmallQ or LargeQ splitting. \n",
    "        cdef DTYPE_t q\n",
    "        cdef SIZE_t n_unique_vals_feat\n",
    "\n",
    "        # Using Louppe splitting for SmallQ. A C-contiguous array of doubles to \n",
    "        # hold feature values of a given node's samples. Using Numpy to allocate \n",
    "        # memory to longer vectors is often faster than using realloc().\n",
    "        cdef DTYPE_t[::1] items_buffer = np.empty(self.n_samples, dtype=np.float64)\n",
    "        cdef DTYPE_t* items = &items_buffer[0]\n",
    "        cdef SIZE_t r\n",
    "\n",
    "        # Three 1-d arrays containing raw and weighted sample counts, as\n",
    "        # well as weighted class counts for each unique raw feature value.\n",
    "        # Used for both SmallQ and LargeQ splitting, except for split_counts_wt,\n",
    "        # which is just used for LargeQ.\n",
    "        \n",
    "        # Raw, non-weighted, sample counts at each split-point.\n",
    "        cdef SIZE_t[::1] split_counts_raw_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.intp) \n",
    "        cdef SIZE_t* split_counts_raw = &split_counts_raw_buffer[0]\n",
    "        # Weighted sample counts at each split-point.\n",
    "        cdef DTYPE_t[::1] split_counts_wt_buffer = np.empty(self.max_n_unique_feat_vals, dtype=np.float64)\n",
    "        cdef DTYPE_t* split_counts_wt = &split_counts_wt_buffer[0]\n",
    "        # Weighted class counts for each split-point.\n",
    "        cdef DTYPE_t[::1] split_class_counts_wt_buffer = np.empty(self.n_class*self.max_n_unique_feat_vals, dtype=np.float64) \n",
    "        cdef DTYPE_t* split_class_counts_wt = &split_class_counts_wt_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For random feature selection (w/out replacement) and keeping track \n",
    "        # of nodes' constant features. \n",
    "        #####################################################################\n",
    "        cdef uniform_int_distribution[SIZE_t] dist\n",
    "        cdef SIZE_t lb, ub, idx, feat_idx, n_drawn_feats, n_new_consts, n_total_consts\n",
    "        cdef SIZE_t[::1] features_buffer = np.empty(self.n_features, dtype=np.intp) \n",
    "        cdef SIZE_t* features = &features_buffer[0]\n",
    "        cdef SIZE_t[::1] constant_features_buffer = np.empty(self.n_features, dtype=np.intp)\n",
    "        cdef SIZE_t* constant_features = &constant_features_buffer[0]\n",
    "        \n",
    "        #####################################################################\n",
    "        # For determining whether node should be a leaf.\n",
    "        #####################################################################\n",
    "        cdef SIZE_t i, c, cc, n_classes_node, row, label\n",
    "        cdef DTYPE_t wcc, wt\n",
    "        # Stores classes that share a leaf's max class wt. When two or more \n",
    "        # present, leaf label randomly chosen from these classes\n",
    "        cdef SIZE_t* max_wt_classes = NULL\n",
    "        \n",
    "        with nogil:\n",
    "            # Allocate memory to pointers.\n",
    "            l_wcc = safe_realloc(l_wcc, self.n_class)\n",
    "            r_wcc = safe_realloc(r_wcc, self.n_class)\n",
    "            node_wcc = safe_realloc(node_wcc, self.n_class)\n",
    "            max_wt_classes = safe_realloc(max_wt_classes, self.n_class*sizeof(SIZE_t))\n",
    "            # Fill with feature column indices so we can track constant feats.\n",
    "            memcpy(features, self.features, self.n_features* sizeof(SIZE_t))\n",
    "            \n",
    "            # Push root node onto the LIFO stack.\n",
    "            node_stack.push({\"start\": 0, \"end\": self.n_samples, \"node_id\": 0, \n",
    "                             \"parent_id\": 0, \"n_const_feats\": 0})\n",
    "            self.n_nodes = 1\n",
    "            while not node_stack.empty():\n",
    "                node_info = node_stack.top()\n",
    "                node_stack.pop()\n",
    "                start, end = node_info.start, node_info.end\n",
    "                node_id, parent_id = node_info.node_id, node_info.parent_id # TODO: `parent_id` unused; is it necessary?\n",
    "                n_consts = node_info.n_const_feats\n",
    "                n_samples_node = end-start\n",
    "                node = &self.nodes[node_id]\n",
    "                \n",
    "                # Tabulate the current node's weighted class counts.\n",
    "                #\n",
    "                # Implementation detail #1: I tried storing the l and r child wt class cts\n",
    "                # of nodes' best splits so that this tabulation wouldn't need to be \n",
    "                # performed for each node. But found there was virtually no speed improvement\n",
    "                # to justify the more complicated code required to store and update these \n",
    "                # values during the best split search.\n",
    "                #\n",
    "                # Implementation detail #2: Setting aside a block of memory to \n",
    "                # store the current node's wt class cts and passing a pointer to\n",
    "                # this block to the split search function sped up training by 8%\n",
    "                # compared to passing a ptr to the location of node's wt class cts \n",
    "                # in the self.weighted_class_counts array.\n",
    "                memset(node_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                sum_node_wcc, sum_node_wcc_sqr = 0., 0.\n",
    "                for i in range(n_samples_node):\n",
    "                    row = self.rows[start + i]\n",
    "                    label = y[row]\n",
    "                    wt = self.class_weights[label]\n",
    "                    # Compute the node's proxy gini numerator and denominator while we're at it.\n",
    "                    sum_node_wcc_sqr += wt*(2*node_wcc[label] + wt) # numerator\n",
    "                    sum_node_wcc += wt                              # denominator\n",
    "                    node_wcc[label] += wt\n",
    "                memcpy(&self.weighted_class_counts[node_id*self.n_class], node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                \n",
    "                # Make a leaf if required to do so. \n",
    "                n_classes_node = 0\n",
    "                for c in range(self.n_class):\n",
    "                    wcc = node_wcc[c]\n",
    "                    if wcc > 0: n_classes_node += 1\n",
    "                if n_classes_node == 1:                   \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif n_samples_node < 2*self.min_samples_leaf:  \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                elif sum_node_wcc < 2.*self.min_weight_leaf: \n",
    "                    self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "\n",
    "                # Otherwise split the node.\n",
    "                else:\n",
    "                    # Initialize stats for best split of node.\n",
    "                    best_split.feat = -1\n",
    "                    best_split.thresh = 0.\n",
    "                    best_split.score = NEG_INF\n",
    "\n",
    "                    # Ensure feats drawn w/out replacement.\n",
    "                    n_drawn_feats = 0\n",
    "                    n_new_consts = 0\n",
    "                    n_total_consts = n_consts\n",
    "                    lb = 0                      # Range in `features` array from which we \n",
    "                    ub = self.n_features - 1    # randomly select a feature's column index. \n",
    "                        \n",
    "                    while n_drawn_feats < self.m:\n",
    "                        n_drawn_feats += 1\n",
    "\n",
    "                        # Breiman & Cutler's original Fortran random forests implementation \n",
    "                        # allows for known constant features to be drawn during a split-search.\n",
    "                        # I follow their example, as I believe that doing so allows individual \n",
    "                        # trees to be less correlated with each other. Since I don't pre-sort\n",
    "                        # features, I would prefer not to have to sort any more features than\n",
    "                        # necessary, and so I've adopted the technique Sklearn uses to track \n",
    "                        # constant features:\n",
    "                        #     https://github.com/scikit-learn/scikit-learn/blob/dbe39454f766ebefc3219f2c1871ac1774316532/sklearn/tree/_splitter.pyx#L310\n",
    "                        # \n",
    "                        # The idea is that feature idxs in `features` are organized into two sections:\n",
    "                        #\n",
    "                        #     [<indices of known constant feats>, <indices of non-constant feats>]\n",
    "                        #\n",
    "                        # As we begin drawing feature indices from this above list, those two sections\n",
    "                        # will each be further sub-divided into two sections:\n",
    "                        # \n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <undrawn non-constant feats>, <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # When we choose a feature that happens to be a known constant, we'll re-locate\n",
    "                        # its idx to the right-end of the first of those four sections. Then we \n",
    "                        # increment the lower bound threshold, `lb`, by one so that we don't re-draw \n",
    "                        # that feature again.\n",
    "                        #\n",
    "                        # Similarly, if we draw a non-constant feature idx, we'll move it to the \n",
    "                        # left-end of the last of the four partitions and reduce the upper bound\n",
    "                        # threshold, `ub`, by one so that the feature idx can't be drawn again\n",
    "                        # during this split-search. \n",
    "                        #\n",
    "                        # One last important detail: sometimes we'll draw a feature that \n",
    "                        # used to be non-constant for ancestor nodes, but will be found to be \n",
    "                        # constant for the current node. When this happens, we relocate its \n",
    "                        # index so that it sits to the right of the known constant feats section.\n",
    "                        # This means our `features` list could have up to five partitions:\n",
    "                        #\n",
    "                        #     [<drawn known constant feats>, <undrawn known constant feats>, \n",
    "                        #      <newly discovered const feats>, <undrawn non-constant feats>, \n",
    "                        #      <drawn non-constant feats>]\n",
    "                        #\n",
    "                        # Whenever we find a new constant feature, we increment the `n_new_consts`\n",
    "                        # counter by one. We also increment the `n_total_consts` counter by one. \n",
    "                        # During the split-search we have to use `n_total_consts` to keep track of\n",
    "                        # the total number of constant features. n_consts` mustn't be changed\n",
    "                        # because it tells us where the <newly discovered const feats> section\n",
    "                        # of the `features` list begins.\n",
    "\n",
    "                        # One last wrinkle. We subtract the # of newly discovered const feats from  \n",
    "                        # the upper bound before we select an index `i` from the `features` array, \n",
    "                        # and add it back to `i` after `i` has been genereated. This prevents us from \n",
    "                        # re-drawing any of these new const feats again during this split-search.\n",
    "                        dist = uniform_int_distribution[SIZE_t](lb, ub-n_new_consts)\n",
    "                        idx = dist(self.rng)\n",
    "\n",
    "                        # So that we don't draw a known constant feature again this split-search.\n",
    "                        if idx < n_consts:\n",
    "                            features[idx], features[lb] = features[lb], features[idx]\n",
    "                            lb += 1 \n",
    "                            continue\n",
    "\n",
    "                        # So that no new const feats get drawn more than once per split-search.\n",
    "                        idx += n_new_consts\n",
    "\n",
    "                        feat_idx = features[idx]\n",
    "                        \n",
    "                        # Num split points found among training samples for given feat.\n",
    "                        n_unique_vals_feat = self.n_unique_vals_feats[feat_idx]\n",
    "                        \n",
    "                        q = n_samples_node/n_unique_vals_feat\n",
    "                        \n",
    "                        # SmallQ Splitting.\n",
    "                        if q < Q_THRESHOLD:\n",
    "                            # Prepare the rows' feature values for sorting.\n",
    "                            for r in range(start, end):\n",
    "                                # X is a pointer, so have to index into this 2d array in the C way \n",
    "                                # (also keeping in mind that the array is column-major).\n",
    "                                items[r] = X[feat_idx*self.n_samples + self.rows[r]]\n",
    "\n",
    "                            # Sort feature values and corresponding sample row indices\n",
    "                            # to prepare for numerical split finding.\n",
    "                            dual_introsort(items, self.rows, start, end)\n",
    "\n",
    "                            # Make sure the feature not constant for node's samples.\n",
    "                            if items[start] == items[end-1]:\n",
    "                                # Move the newly-discovered constant feat to the far right-end\n",
    "                                # of the left half of `features` list holding the known const\n",
    "                                # feats as well as any other const feats newly discovered \n",
    "                                # during this node's split-search.\n",
    "                                features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                                n_new_consts += 1\n",
    "                                n_total_consts += 1\n",
    "                                continue\n",
    "                            else:\n",
    "                                # Initialize weighted class counts of right and left children.\n",
    "                                # Right child's counts are initially the same as parent node's.\n",
    "                                memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                                memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "\n",
    "                                # If the feature has an impurity score that's better than the best score \n",
    "                                # found among all other features visited thus far for this node, find_num_split()\n",
    "                                # updates the attributes of the struct containing the node's best split info. \n",
    "                                # \n",
    "                                # But even if a new best score isn't reached, if an impurity score can\n",
    "                                # be calculated at least once during the feature's split search, the\n",
    "                                # following indicator will be toggled off, to indicate that the feature\n",
    "                                # is not constant.\n",
    "                                current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                                find_num_split(self.rows, items, y, start, n_samples_node, \n",
    "                                               self.n_class, self.min_samples_leaf, self.min_weight_leaf, \n",
    "                                               self.class_weights, l_wcc, r_wcc, node_wcc,\n",
    "                                               &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc,\n",
    "                                               &current_feat_const)\n",
    "                                \n",
    "                        # LargeQ Splitting.\n",
    "                        else:\n",
    "                            memcpy(r_wcc, node_wcc, self.n_class*sizeof(DTYPE_t))\n",
    "                            memset(l_wcc, 0, self.n_class*sizeof(DTYPE_t))\n",
    "                            current_feat_const = 1 # 1 = is constant; 0 = not constant\n",
    "                            find_num_split_largeQ(self.rows, split_point_idxs, unique_vals_feats, y, start, \n",
    "                                                  n_samples_node, self.n_samples, self.n_class, self.min_samples_leaf, \n",
    "                                                  self.min_weight_leaf, self.class_weights, l_wcc, r_wcc, node_wcc, \n",
    "                                                  &best_split, feat_idx, sum_node_wcc_sqr, sum_node_wcc, n_unique_vals_feat, \n",
    "                                                  self.max_n_unique_feat_vals, split_counts_raw, split_counts_wt, \n",
    "                                                  split_class_counts_wt, &current_feat_const)\n",
    "\n",
    "                        if current_feat_const:\n",
    "                            # The feature may be constant within the search range permitted\n",
    "                            # by self.min_samples_leaf and self.min_weight_leaf. If so, \n",
    "                            # the feature is a newly discovered constant.\n",
    "                            features[idx], features[n_total_consts] = features[n_total_consts], features[idx]\n",
    "                            n_new_consts += 1\n",
    "                            n_total_consts += 1\n",
    "                            continue\n",
    "                        else:\n",
    "                            # The feature is non-constant, so we ensure it's not drawn again\n",
    "                            # during this split-search.\n",
    "                            features[idx], features[ub] = features[ub], features[idx]\n",
    "                            ub -= 1 \n",
    "\n",
    "                    # To ensure that the constant features info is accurate for sibling or child nodes.\n",
    "                    memcpy(&features[0], &constant_features[0], sizeof(SIZE_t)*n_consts)\n",
    "                    memcpy(&constant_features[n_consts], &features[n_consts], sizeof(SIZE_t)*n_new_consts)\n",
    "\n",
    "                    # Make node a leaf if constant for all randomly drawn feats.\n",
    "                    # (# drawn known constant feats + # drawn new constant feats)\n",
    "                    if lb + n_new_consts == n_drawn_feats: \n",
    "                        self._make_leaf(node, y, start, node_id, n_classes_node, max_wt_classes)\n",
    "                    else: \n",
    "                        split_pos = make_num_split(self.rows, X, &node_info, &best_split, self.n_samples) \n",
    "\n",
    "                        # Update tree info for node that's getting split.\n",
    "                        l_child_id = self.n_nodes\n",
    "                        r_child_id = l_child_id + 1\n",
    "                        node.l_child = l_child_id\n",
    "                        node.r_child = r_child_id\n",
    "                        node.feat    = best_split.feat\n",
    "                        node.thresh  = best_split.thresh\n",
    "                        node.label   = -1\n",
    "\n",
    "                        # Prepare for the left and right child nodes\n",
    "                        # by increasing tree data memory capacity if\n",
    "                        # necessary.\n",
    "                        if self.n_nodes + 2 > self.mem_capacity:\n",
    "                            # Expand memory capacity geometrically. See \"geometric growth\" \n",
    "                            # part of WhozCraig's SO answer at: \n",
    "                            #     https://stackoverflow.com/a/51665863/8628758.\n",
    "                            # Add one after squaring so that the new capacity can\n",
    "                            # contain not only a tree of greater depth, but also\n",
    "                            # the maximum # nodes that that depth could have.\n",
    "                            new_capacity = 2*self.mem_capacity + 1\n",
    "                            self._increase_mem_capacity(new_capacity)\n",
    "                            self.mem_capacity = new_capacity\n",
    "                        \n",
    "                        # Push right child info onto the LIFO stack.\n",
    "                        node_stack.push({\"start\": split_pos, \"end\": end, \"node_id\": r_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "                        # Push left child info onto queue.\n",
    "                        node_stack.push({\"start\": start, \"end\": split_pos, \"node_id\": l_child_id, \n",
    "                                         \"parent_id\": node_id, \"n_const_feats\": n_total_consts})\n",
    "\n",
    "                        # And update size of the tree.\n",
    "                        self.n_nodes += 2\n",
    "                        \n",
    "        free(l_wcc)\n",
    "        free(r_wcc)\n",
    "        free(node_wcc)\n",
    "        free(max_wt_classes)\n",
    "    \n",
    "    def fit(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] y,\n",
    "            np.ndarray[SIZE_t, ndim=2, mode=\"fortran\"] split_point_idxs, \n",
    "            np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] n_unique_vals_feats,\n",
    "            np.ndarray[SIZE_t, ndim=1, mode=\"c\"] rows, np.ndarray[SIZE_t, ndim=1, mode=\"c\"] features,\n",
    "            np.ndarray[DTYPE_t, ndim=1, mode=\"c\"] class_weights, SIZE_t n_class): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "            class_weights (1D C-contiguous array of float64): Desired weight for each class. Shape: (`n_class`,).\n",
    "            n_class                                         : Number of classes in training data. \n",
    "        \"\"\"\n",
    "        # Casting the raw data to pointers gives a 17% speed-up compared to getting\n",
    "        # pointer from the ndarray's buffer interface, as recommended by DavidW in \n",
    "        # his SO answer at: https://stackoverflow.com/a/54832269/8628758. e.g.\n",
    "        #     cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        #     cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        # Not worried about unexpected behavior as all ndarrays' contiguousness and\n",
    "        # memory layout enforced prior to this point.\n",
    "        cdef DTYPE_t* X_ptr = <DTYPE_t*> X.data\n",
    "        cdef SIZE_t* y_ptr = <SIZE_t*> y.data\n",
    "        cdef SIZE_t* split_point_idxs_ptr = <SIZE_t*> split_point_idxs.data\n",
    "        cdef DTYPE_t* unique_vals_feats_ptr = <DTYPE_t*> unique_vals_feats.data\n",
    "        self.n_unique_vals_feats = <SIZE_t*> n_unique_vals_feats.data\n",
    "        self.rows = <SIZE_t*> rows.data\n",
    "        self.features = <SIZE_t*> features.data\n",
    "        self.class_weights = <DTYPE_t*> class_weights.data\n",
    "        self.n_class = n_class\n",
    "        self.n_samples = rows.shape[0]\n",
    "        self.n_features = features.shape[0]\n",
    "        cdef random_device rd # Needed when using the C++ mt19937 rng w/out a seed.\n",
    "        \n",
    "        # Get the max cardinality of all numerical feats.\n",
    "        self.max_n_unique_feat_vals = n_unique_vals_feats.max()\n",
    "        \n",
    "        # Why initialize tree memory to hold 15 nodes? For a given \n",
    "        # depth, d >= 1, a tree will have a maximum of d^2 - 1 nodes. \n",
    "        # i.e. at d=1 a tree only has its root node. When d = 2, the \n",
    "        # tree has 3 nodes. If d=3, a tree will have 2^3 - 1 = 7 nodes, \n",
    "        # etc. 15 is the max # of nodes a tree of depth=4 could have. \n",
    "        cdef SIZE_t init_capacity = 15\n",
    "        \n",
    "        cdef SIZE_t i, row, label\n",
    "        cdef DTYPE_t wt\n",
    "        cdef DTYPE_t sum_wts = 0\n",
    "        cdef Node* root_node = NULL\n",
    "        with nogil:\n",
    "            # Allocate memory for the tree.\n",
    "            self._increase_mem_capacity(init_capacity)\n",
    "            self.mem_capacity = init_capacity\n",
    " \n",
    "            # And sum the class weights of all the root node's samples in\n",
    "            # order to know minimum total weight a leaf must have (which\n",
    "            # we must know when regularizing by min_weight_fraction_leaf.)\n",
    "            for i in range(self.n_samples):\n",
    "                row = self.rows[i]\n",
    "                label = y_ptr[row]\n",
    "                wt = self.class_weights[label]\n",
    "                sum_wts += wt\n",
    "            self.min_weight_leaf = self.min_weight_fraction_leaf*sum_wts\n",
    "            \n",
    "            # Initialize the random number generator. Followed example from:\n",
    "            #     https://github.com/cython/cython/blob/9341e73aceface39dd7b48bf46b3f376cde33296/tests/run/cpp_stl_random.pyx#L16\n",
    "            if self.seed == -1:\n",
    "                self.rng = mt19937(rd()) # If using the random device engine std::random_device.\n",
    "            else:\n",
    "                self.rng = mt19937(self.seed)\n",
    "\n",
    "        # Initiate tree building.\n",
    "        self._grow_tree(X_ptr, y_ptr, split_point_idxs_ptr, unique_vals_feats_ptr)\n",
    "    \n",
    "    cdef Node* _next_node(self, SIZE_t nxt) nogil: \n",
    "        return &self.nodes[nxt]\n",
    "    \n",
    "    cdef SIZE_t _get_leaf_idx(self, SIZE_t i, Node* leaf, SIZE_t n, DTYPE_t* X) nogil:\n",
    "        cdef SIZE_t idx\n",
    "        cdef SIZE_t root_idx = 0\n",
    "        leaf = self._next_node(root_idx)\n",
    "        while leaf.label == -1:\n",
    "            if X[leaf.feat*n + i] <= leaf.thresh:\n",
    "                idx = leaf.l_child\n",
    "                leaf = self._next_node(idx)\n",
    "            else: \n",
    "                idx = leaf.r_child\n",
    "                leaf = self._next_node(idx)\n",
    "        return idx\n",
    "    \n",
    "    def predict(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate class predictions for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of int: Class predictions. Shape: (`X.size`,).\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_preds = X.shape[0]\n",
    "        cdef SIZE_t i\n",
    "        preds = np.empty(n_preds, dtype=np.intp)\n",
    "        cdef SIZE_t[::1] preds_view = preds\n",
    "        cdef Node leaf\n",
    "        with nogil:\n",
    "            for i in range(n_preds): \n",
    "                preds_view[i] = self.nodes[self._get_leaf_idx(i, &leaf, n_preds, X_ptr)].label\n",
    "        return preds\n",
    "    \n",
    "    def predict_probs(self, np.ndarray[DTYPE_t, ndim=2, mode=\"fortran\"] X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D Fortran-contiguous ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions. Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        cdef DTYPE_t[::1,:] X_buffer = X\n",
    "        cdef DTYPE_t* X_ptr = &X_buffer[0,0]\n",
    "        cdef SIZE_t n_probs = X.shape[0]\n",
    "        wcc = np.empty(n_probs*self.n_class, dtype=np.float64)\n",
    "        cdef DTYPE_t[::1] wcc_view = wcc\n",
    "        cdef Node leaf\n",
    "        cdef SIZE_t i, j, idx\n",
    "        with nogil:\n",
    "            for i in range(n_probs):\n",
    "                idx = self._get_leaf_idx(i, &leaf, n_probs, X_ptr)\n",
    "                for j in range(self.n_class):\n",
    "                    wcc_view[i*self.n_class + j] = self.weighted_class_counts[idx*self.n_class + j]\n",
    "        wcc.resize(n_probs, self.n_class)\n",
    "        sums = np.sum(wcc, axis=1)[:,None]\n",
    "        return np.divide(wcc, sums)\n",
    "\n",
    "class DecisionTreeLouppeWrightCython():\n",
    "    \"\"\"Fit a decision tree classifier using a depth-first tree \n",
    "    growth algorithm. \n",
    "    \n",
    "    Uses Louppe's splitter for SmallQ splits and Wright's splitter for LargeQ splitting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, m, min_samples_leaf=1, min_weight_fraction_leaf=0., class_weights = [], seed=None): \n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            m                            (int): Number of candidate features randomly selected to try to split each node.\n",
    "            min_samples_leaf             (int): Any leaf will have no fewer than this many samples.\n",
    "            min_weight_fraction_leaf (float64): Total weight of any leaf's samples must comprise this portion \n",
    "                                                of the sum of weights of *all* training samples used to fit the tree.\n",
    "            seed                         (int): Use when reproducibility is desired.\n",
    "        \"\"\"\n",
    "        self.m = m\n",
    "        self.min_samples_leaf, self.min_weight_fraction_leaf = min_samples_leaf, min_weight_fraction_leaf\n",
    "        self.class_weights = np.array(class_weights, dtype=np.float64, order='C') \n",
    "        if seed is None:\n",
    "            self.seed = -1\n",
    "        else:\n",
    "            self.seed = seed\n",
    "        self._tree = _DecisionTree(self.m, self.min_samples_leaf, self.min_weight_fraction_leaf, self.seed)\n",
    "        \n",
    "    @property\n",
    "    def size(self): return self._tree.size\n",
    "    \n",
    "    @property\n",
    "    def left_children(self): return self._tree.left_children\n",
    "    \n",
    "    @property\n",
    "    def right_children(self): return self._tree.right_children\n",
    "            \n",
    "    @property \n",
    "    def split_features(self): return self._tree.split_features\n",
    "\n",
    "    @property \n",
    "    def split_thresholds(self): return self._tree.split_thresholds\n",
    "    \n",
    "    @property\n",
    "    def weighted_class_counts(self): return self._tree.weighted_cc\n",
    "    \n",
    "    @property\n",
    "    def labels(self): return self._tree.labels\n",
    "    \n",
    "    def fit(self, X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, rows=[], features=[]): \n",
    "        \"\"\"Fit a decision tree classifier model.\n",
    "        \n",
    "        Arguments:\n",
    "            X       (2D Fortran-contiguous array of float64): Pre-processed training data.\n",
    "            y                 (1D C-contiguous array of int): Training labels.\n",
    "            split_point_idxs                (ndarray of int): All numerical feature split-point locations for \n",
    "                                                              all rows. Shape: (n training samples, n features).\n",
    "            unique_vals_feats           (ndarray of float64): Columns contain sorted unique values for all features.\n",
    "                                                              Shape: (max cardinality of all feats, n features).\n",
    "            n_unique_vals_feats                (ndarray int): Cardinality of each feature. Shape: (n features,).\n",
    "            rows              (1D C-contiguous array of int): Indices of the rows to be used for training. \n",
    "                                                              All rows used if empty.\n",
    "            feats             (1D C-contiguous array of int): Column indices of training features.\n",
    "                                                              All rows used if empty.\n",
    "                                                              \n",
    "        Returns:\n",
    "            DecisionTreeLouppeWrightCython: A decision tree object.\n",
    "        \"\"\"\n",
    "        if len(rows) > 0:\n",
    "            self.rows = np.array(rows, dtype='int', order='C')\n",
    "        else:\n",
    "            self.rows = np.arange(0, X.shape[0], 1)\n",
    "            \n",
    "        if len(features) > 0:\n",
    "            self.features = np.array(features, dtype='int', order='C')\n",
    "        else:\n",
    "            self.features = np.arange(0, X.shape[1], 1)\n",
    "        \n",
    "        self.n_class = np.unique(y).size\n",
    "        if len(self.class_weights) == 0: \n",
    "            self.class_weights.resize(self.n_class, refcheck=False)\n",
    "            self.class_weights[:] = 1.\n",
    "            \n",
    "        self._tree.fit(X, y, split_point_idxs, unique_feat_vals, n_unique_vals_feats, \n",
    "                       self.rows, self.features, self.class_weights, self.n_class)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict(X)\n",
    "    \n",
    "    def predict_probs(self, X):\n",
    "        \"\"\"Generate prediction probabilities for one or more test inputs.\n",
    "        \n",
    "        Arguments:\n",
    "            X (2D ndarray of float64): Pre-processed test samples.\n",
    "            \n",
    "        Returns:\n",
    "            ndarray of float64: Class probability predictions.\n",
    "                                Shape: (`X.size`, `self.n_class`)\n",
    "        \"\"\"\n",
    "        return self._tree.predict_probs(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Louppe SmallQ/Wright LargeQ Tree's Speed on the Titanic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4\n",
    "dt = DecisionTreeLouppeWrightCython(m, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "393"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8202247191011236"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = dt.predict(xVal_proc)\n",
    "accuracy(preds, yVal_titanic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "346 Âµs Â± 5.15 Âµs per loop (mean Â± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.4 Âµs Â± 103 ns per loop (mean Â± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There you have it, using Louppe for SmallQ splitting speeds things up compared to a SmallQ/LargeQ hybrid that uses Wright's SmallQ algorithm. However, the winner for the Titanic dataset is still a tree that only makes splits using Wright's LargeQ algorithm:\n",
    "\n",
    "|Splitting Algorithm|Speed on Titanic Dataset|\n",
    "|---|---|\n",
    "|Louppe|538 Âµs|\n",
    "|Wright SmallQ|1.03 ms|\n",
    "|Wright LargeQ|315 Âµs|\n",
    "|Wright SmallQ/LargeQ|500 Âµs|\n",
    "|Louppe SmallQ/Wright LargeQ|346 Âµs| \n",
    "\n",
    "Before I call it a day, I'd like to see if the above gaps in relative performance hold true when these splitting algorithms are faced with a much larger dataset. Kaggle's Santander Customer Satisfaction competition [dataset](https://www.kaggle.com/competitions/santander-customer-satisfaction/data) fits the bill nicely, given that it clocks in at 369 numerical features and over 60K rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the Santander Competition's Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "santander_dir = Path.home()/'data'/'santander_cust_sat'\n",
    "\n",
    "def get_santander_data():\n",
    "    \"\"\"Download and place the kaggle Santander train and test csv files into Pandas dataframes.\"\"\"\n",
    "    santander_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_csv_path, test_csv_path = santander_dir/'train.csv', santander_dir/'test.csv'\n",
    "    if not train_csv_path.exists():\n",
    "        # Visit https://github.com/Kaggle/kaggle-api for more info on \n",
    "        # how to install the kaggle API and generate a kaggle.json key.\n",
    "        !kaggle competitions download -c santander-customer-satisfaction --path \"$santander_dir\"\n",
    "        with ZipFile(santander_dir/'santander-customer-satisfaction.zip', 'r') as z: z.extractall(santander_dir) \n",
    "    santander_train_df = pd.read_csv(train_csv_path); santander_train_df.drop('ID', axis=1, inplace=True)\n",
    "    santander_test_df  = pd.read_csv(test_csv_path)\n",
    "    return santander_train_df, santander_test_df\n",
    "    \n",
    "santander_train_df, _ = get_santander_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "xTrain_santander, yTrain_santander, xVal_santander, yVal_santander = train_val_split(santander_train_df, 369)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60816, 369)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain_santander.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to calculate an ROC AUC score\n",
    "This competition's evaluation metric was the area under the ROC curve. This is because a good model needs to be able to find all the unsatisfied customers without incorrectly labelling too many satisfied customers as \"unsatisfied.\" ROC will do a better job of punishing a model for committing false-positive errors than a metric like accuracy would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_roc_auc(probs, labels, pos_label=1):\n",
    "    \"\"\"Calculate area under the receiver operating characteristic curve for a 2-class prediction task.\n",
    "    \n",
    "    To write this function, I first studied the logic spread across four sklearn files:\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/8479a74af207d857da4188b75375ce9d24c7ef90/sklearn/metrics/_ranking.py#L324\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/8479a74af207d857da4188b75375ce9d24c7ef90/sklearn/metrics/_ranking.py#L827\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/8479a74af207d857da4188b75375ce9d24c7ef90/sklearn/metrics/_ranking.py#L653\n",
    "        https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/metrics/_ranking.py#L41\n",
    "    \n",
    "   And then I picked out the necessary steps and condensed into a concise function that \n",
    "   generates an ROC curve and calculates its area.\n",
    "    \n",
    "    Arguments:\n",
    "        probs (Two-column Numpy array): Prediction probabilities of both classes for n samples. \n",
    "        labels          (list of ints): Ground-truth labels.\n",
    "        pos_label                (int): Class label (either 0 or 1) of the positive class.\n",
    "        \n",
    "    Returns:\n",
    "        The area under the ROC curve.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = len(labels); row_idxs = np.array(list(range(n)))\n",
    "    pos_probs = np.array(probs[:,pos_label])                    # Use pred probs from positive class.\n",
    "    dual_sort(pos_probs, row_idxs, 0, n)                        # Sort pred probs and corresponding  \n",
    "    pos_probs, row_idxs = np.flip(pos_probs), np.flip(row_idxs) # row indices in decreasing order.\n",
    "    pos_labels = (np.array(labels)==pos_label)\n",
    "    pos_labels = pos_labels[row_idxs]                           # Keep order consistent with the now-sorted pred probs.\n",
    "    threshold_idxs = np.where(np.diff(pos_probs))[0]            # We only care about unique pred probs.\n",
    "    threshold_idxs = np.append(threshold_idxs, n-1)             # Include idx of final sample.\n",
    "    TP = np.cumsum(pos_labels)[threshold_idxs]                  # Count all true positives under each unique pred prob.\n",
    "    FP = 1 + threshold_idxs - TP\n",
    "    TP, FP = np.append(0, TP), np.append(0, FP)                 # Make sure ROC curve will contain origin coord.\n",
    "    TPR, FPR = TP/TP[-1], FP/FP[-1]\n",
    "    AUC = np.trapz(TPR, FPR)                                    # TPRs are on y-axis, FPRs on x-axis.\n",
    "    return AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xTrain_proc, yTrain_proc, nan_fillers, \n",
    " _, split_point_idxs, unique_vals_feats, \n",
    " n_unique_vals_feats) = preprocess_train(xTrain_santander, yTrain_santander, largeQ=True)\n",
    "xVal_proc = preprocess_test(xVal_santander, nan_fillers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Louppe Tree's Speed on the Santander Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aside: Regularizing with `min_samples_leaf`\n",
    "I've found that when using a single decision tree, I get beter results on the Santander satisfaction data by regularizing my model using the `min_samples_leaf` parameter. This forces the growth of shallower tree models that generalize a bit better to unseen samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "dt = DecisionTreeLouppeCython(m, min_samples_leaf=5, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986164474454197"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_probs(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 ms Â± 1.89 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7 ms Â± 811 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_probs(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright SmallQ Tree's Speed on the Santander Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "dt = DecisionTreeSmallQCython(m, min_samples_leaf=5, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986164474454197"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_probs(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874 ms Â± 40.5 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.35 ms Â± 904 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_probs(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright LargeQ Tree's Speed on the Santander Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "dt = DecisionTreeLargeQCython(m, min_samples_leaf=5, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986164474454197"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_probs(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 ms Â± 2.35 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61 ms Â± 639 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_probs(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Wright SmallQ/LargeQ Tree's Speed on the Santander Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "dt = DecisionTreeSmallQLargeQCython(m, min_samples_leaf=5, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986164474454197"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_probs(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139 ms Â± 1.98 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.25 ms Â± 290 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_probs(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cython Louppe SmallQ/Wright LargeQ Tree's Speed on the Santander Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 10\n",
    "dt = DecisionTreeLouppeWrightCython(m, min_samples_leaf=5, seed=42)\n",
    "dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6986164474454197"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_probs(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129 ms Â± 1.33 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc, split_point_idxs, unique_vals_feats, n_unique_vals_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.09 ms Â± 581 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_probs(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion: The Louppe/Wright hybrid is fastest, but usually Louppe alone will be best\n",
    "Recall that on the Titanic dataset, Wright LargeQ splitting was the fastest, closely followed by the Louppe SmallQ/Wright LargeQ hybrid. Wright's SmallQ/LargeQ hybrid and then Louppe's splitter took third and fourth place, respectively, with fairly similar speeds. Finally, the Wright SmallQ splitting brought up the rear clocking in nearly twice as slow as Louppe's splitter, and three times as slow as the best-forming Wright LargeQ splitter.\n",
    "\n",
    "|Splitting Algorithm|Speed on Santander Dataset|\n",
    "|---|---|\n",
    "|Louppe|137 ms|\n",
    "|Wright SmallQ|874 ms|\n",
    "|Wright LargeQ|138 ms|\n",
    "|Wright SmallQ/LargeQ|139 ms|\n",
    "|Louppe SmallQ/Wright LargeQ|129 ms| \n",
    "\n",
    "However, on the much larger Santander dataset, with the exception of Wright's SmallQ splitter, the splitting algorithms' relative speeds are virtually identical. The Louppe/Wright hybrid is now best, but I don't believe that its marginal 5% speed improvement over it's closest rival (Louppe) is a large enough carrot to compensate for the cost, both in time and memory, necessary to pre-sort the data for LargeQ splitting.\n",
    "\n",
    "Louppe splitting doesn't require any pre-sorting, and its implementation is far more straightforward than either of the two hybrid SmallQ/LargeQ approaches I experimented with. Therefore, it's my belief that the Louppe splitting algorithm is the best for most random forest implementations and the practitioners who will use those them.\n",
    "\n",
    "#### More research necessary to determine whether Louppe will *always* be sufficient\n",
    "There could conceivably be scenarios where pre-sorting/LargeQ splitting is preferable to Louppe's just-in-time sorting method. As I hypothesized at this notebook's outset, I could imagine this being the case if one were trying to fit a decision tree to billions of rows of training data that consists of low-cardinality numerical features. The costs of relocating potentially billions of floats to a contiguous memory buffer for each candidate feature, for each node, may place sole use Louppe's method at a significant disadvantage to LargeQ or a LargeQ hybrid strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sklearn Library Tree's Speed on the Santander Data\n",
    "To verify that the Cython code I've written is well-tuned and performant, let's compare the speed of the Sklearn library's [`DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html), which also uses a Cython implementation of Louppe's splitter, with the performance of my own Cython implementation of Louppe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(criterion='gini', splitter='best', min_samples_leaf=5, \n",
    "                            max_features=10, random_state=79)\n",
    "dt.fit(xTrain_proc, yTrain_proc);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1049"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.tree_.node_count # Number of nodes in the sklearn decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6786673008005639"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = dt.predict_proba(xVal_proc)\n",
    "binary_roc_auc(probs, yVal_santander)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128 ms Â± 4.2 ms per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 100 dt.fit(xTrain_proc, yTrain_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.3 ms Â± 652 Âµs per loop (mean Â± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit dt.predict_proba(xVal_proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### My Cython Louppe implementation is about as fast as Sklearn during training\n",
    "Gratifyingly, my Cython implementation runs roughly as fast as Sklearn's. I should emphasize that despite choosing random seeds such that both Sklearn's and my own trees grew to the same size (1,049 nodes), these trees are not identical. The crucial implication of this is that any comparison of my and Sklearn's relative speeds will never be perfectly apples-to-apples.\n",
    "\n",
    "The first reason for this is that our two tree-growing implementations use different random number generator implementations. Sklearn employs a [home-grown](https://github.com/scikit-learn/scikit-learn/blob/f9fd3b535af47986d20ad1ad06be35de10221cc2/sklearn/utils/_random.pxd#L24) version of George Marsaglia's [Xorshift RNG algorithm](http://www.jstatsoft.org/v08/i14/paper), while I rely on the C++ standard library's [version](https://cplusplus.com/reference/random/mersenne_twister_engine/) of Makoto Matsumoto's and Takuji Nishimura's [Mersenne Twister algorithm](https://en.wikipedia.org/wiki/Mersenne_Twister). \n",
    "\n",
    "But even if my and Sklearn's decision tree implementations used identical RNG implementations, it'd still be impossible to compare their respective speeds under identical conditions. In other words, it wouldn't be possible to force each algorithm to grow the same decision tree in the same way (i.e. drawing and sorting the same features for each node's best split search). This is because it is highly likely that for any given node split, my and Sklearn's RNGs will *not* be called the same number of times. There are two reasons for this:\n",
    "1. Sklearn [will draw beyond](https://github.com/scikit-learn/scikit-learn/blob/213d21fe719ce5778726203893c78251b8af34fa/sklearn/tree/_splitter.pyx#L305) `m` number of features if the first `m` features that were drawn are constant. Conversely, my implementation would turn such a node into a leaf upon drawing `m` constant features.\n",
    "2. When setting a node as a leaf, my implementation will randomly choose the node's label on occasions where more than one class label have identical weighted class counts among the leaf node's samples, and this random choice happens during model training. Sklearn, on the other hand, does not set leaf labels during training, and instead defers this task to inference time. \n",
    "\n",
    "In other words, at a given tree node, my and Sklearn's implementations will almost certainly choose different batches of candidate splitting features. If one implementation got unlucky and happened to more frequently pick features that took longer to sort, that implementation would take longer to train, even if it were coded to be as performant as the other implementation.\n",
    "\n",
    "#### My Cython is way faster during inference\n",
    "At test-time, however, it is possible to make an apples-apples comparison of the inference speeds of my and Sklearn's implementations. My Cython code takes all of about 4ms to run inference on all samples in the validation set while Sklearn is over 200% slower, taking nearly 15ms to do the same task. \n",
    "\n",
    "I believe this discrepancy in performance is explained by the fact that for each test input, my code only has to travel through the tree's node structure to the appropriate leaf and then grab that leaf's label, which had already been determined at training. Sklearn, on the other hand, has to [execute a call](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_classes.py#L432) to `np.argmax` inside a pure Python for-loop that iterates over each test sample. To be fair, I believe the for-loop is necesssary because Sklearn [supports multi-label](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) classification problems, which my implementation does not.\n",
    "\n",
    "#### Aside: A quibble regarding bias that Sklearn may introduce at test-time\n",
    "When its tree's `predict()` function is called and Sklearn does finally determine leaf labels, [it uses](https://github.com/scikit-learn/scikit-learn/blob/95d4f0841d57e8b5f6b2a570312e9d832e69debc/sklearn/tree/_classes.py#L434) `np.argmax` to select the class label with the maximum weighted class count. Using `np.argmax` in this way introduces a possible bias toward leaves being given lower class labels, as Numpy's argmax function [only returns](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html) the index *of the first* element that contains the max value of an array's elements. If ever a leaf's samples's class labels are equally weighted, Sklearn will always set the leaf's class label to the lower class. e.g. class `0` instead of class `1`.\n",
    "\n",
    "It would be a pity to train a decision tree model that tends to predict lower classes solely due to a quirk of `np.argmax`. For this reason, my implementation breaks ties by randomly choosing the label class from all classes that share the maximum weighted sample sum of a given leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing My Cython Louppe Implementation for Memory Leaks\n",
    "This final sanity check ensures that my implementation properly frees all allocated memory. I used the code from [this gist](https://gist.github.com/raghavrv/c5a147220509d872e3627830967dff1b) created by Sklearn contributor [Venkat Rajagopalan](https://gist.github.com/raghavrv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              types |   # objects |   total size\n",
      "=================================================== | =========== | ============\n",
      "                          pandas.core.series.Series |        1141 |    643.16 MB\n",
      "                                               list |       19822 |      1.64 MB\n",
      "                                                str |       19064 |      1.36 MB\n",
      "                                               dict |        3473 |    368.22 KB\n",
      "                                                int |        5431 |    148.59 KB\n",
      "                                      numpy.ndarray |        1151 |    119.83 KB\n",
      "  pandas.core.internals.managers.SingleBlockManager |        1141 |    106.97 KB\n",
      "                                              tuple |        2148 |    106.15 KB\n",
      "              pandas._libs.internals.BlockPlacement |        1141 |     80.23 KB\n",
      "                                              slice |        1141 |     62.40 KB\n",
      "              pandas.core.internals.blocks.IntBlock |         789 |     55.48 KB\n",
      "            pandas.core.internals.blocks.FloatBlock |         341 |     23.98 KB\n",
      "                    pandas._libs.index.ObjectEngine |           5 |     21.02 KB\n",
      "                                               code |          -1 |      6.91 KB\n",
      "                                               type |          13 |      6.08 KB\n",
      "\n",
      "\n",
      "Initial memory : 3292MB   ==================================================\n",
      "\n",
      "\n",
      "                                         types |   # objects |   total size\n",
      "============================================== | =========== | ============\n",
      "                                          list |           8 |    640     B\n",
      "                                           str |           8 |    632     B\n",
      "                                          code |           0 |    454     B\n",
      "                                           int |           9 |    252     B\n",
      "  sklearn.tree._classes.DecisionTreeClassifier |          -1 |    -48     B\n",
      "                                          dict |          -1 |   -232     B\n",
      "\n",
      "\n",
      "After iteration 0 : 3297MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "   code |           0 |    439     B\n",
      "   list |          -1 |    -80     B\n",
      "    str |          -1 |    -93     B\n",
      "\n",
      "\n",
      "After iteration 1 : 3298MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "   code |           0 |    117     B\n",
      "\n",
      "\n",
      "After iteration 2 : 3298MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "   code |           0 |    677     B\n",
      "\n",
      "\n",
      "After iteration 3 : 3294MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "\n",
      "\n",
      "After iteration 4 : 3300MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "\n",
      "\n",
      "After iteration 5 : 3302MB   ==================================================\n",
      "\n",
      "\n",
      "  types |   # objects |   total size\n",
      "======= | =========== | ============\n",
      "   code |           0 |    183     B\n",
      "\n",
      "\n",
      "After iteration 6 : 3303MB   ==================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adapted from raghavrv's wonderful gist at: https://gist.github.com/raghavrv/c5a147220509d872e3627830967dff1b\n",
    "\n",
    "import os, time, gc, psutil\n",
    "from pympler import tracker\n",
    "import numpy as np\n",
    "\n",
    "tracker.memory_tracker = tracker.SummaryTracker()\n",
    "def get_mem():\n",
    "    return \"{:.0f}MB\".format(p.memory_info().rss / 1e6)\n",
    "\n",
    "p = psutil.Process()\n",
    "\n",
    "def sleep_1s_and_print_mem(title):\n",
    "    time.sleep(1)\n",
    "    tracker.memory_tracker.print_diff()\n",
    "    print(\"\\n\\n\" + title + \" : \" + get_mem() + \"   \" + \"=\" * 50 + \"\\n\\n\")\n",
    "\n",
    "sleep_1s_and_print_mem(\"Initial memory\")\n",
    "\n",
    "for i in range(7):\n",
    "    dt = DecisionTreeLouppeCython(m, min_samples_leaf=5, seed=47).fit(xTrain_proc, yTrain_proc)\n",
    "    del dt\n",
    "    gc.collect()\n",
    "\n",
    "    sleep_1s_and_print_mem(\"After iteration %d\" % i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
